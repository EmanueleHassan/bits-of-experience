<br>
<br>

<p>
So far in the <i>ANOVA</i> series we have encountered a simple one-way
analysis of variance test. This was based on the idea of comparing the
standardized sum of squares of the treatment effect and the one of the
stochastic component. We did this with the <i>omnibus</i> F-test testing
the null of no-effect of the treatments.
</p>

<p>
What happens however when we have a rather specific question, such as
the when we are concerned about the effect of a specific
treatment/factor and we question whether this is significant?
</p>

<!-- TEASER_END -->

<br>

<div id="outline-container-org892b93d" class="outline-2">
<h2 id="org892b93d">Setting up Hypothesis</h2>
<div class="outline-text-2" id="text-org892b93d">
<p>
In order to address our specific question we must set up an
hypothesis. In this sense, we can formulate a vector containing our
hypothesis.
</p>

<p>
For instance assuming a dataset containing a factor with three levels
such as the one encountered in the previous post. We might be
interested in comparing the mean of two of the levels of the factor.
We might then set up the hypothesis
</p>

\begin{align*}
  \vec{c} = \begin{pmatrix} 1 \\ -1 \\ 0\end{pmatrix} ; & \hspace{10mm} \vec{\alpha} = \begin{pmatrix} \alpha_1 \\ \alpha_2 \\ \alpha_3 \end{pmatrix} \\ 

  \\

  H_0: \vec{c}^\intercal * \vec{\alpha} &= 0  \\

  H_A: \vec{c}^\intercal * \vec{\alpha} &\neq 0 
\end{align*}

<br>

<p>
In the above case, we expressed the Null of equal treatment effects
for the two first levels. In an analogous way we can formulate any
other linear hypothesis.
</p>

<p>
Notice that the above is called a contrast and we usually provide a
side-constraint such that we ensure that the contrast above is testing
the difference among the treatments rather than the overall level of
the response. 
</p>

<p>
\[\sum^{m}_{i=1} c_i = 0\]
</p>

<p>
Notice however that if we are interested in the testing the difference
in levels of the treatment groups a Null as:
</p>

<p>
\[\vec{c} = \begin{pmatrix} 1 \\ 2 \\ 0 \end{pmatrix}\]
</p>

<p>
might make completely sense.
</p>

<br>
</div>
</div>


<div id="outline-container-org8c5032d" class="outline-2">
<h2 id="org8c5032d">Analysis of Variance and F-tests</h2>
<div class="outline-text-2" id="text-org8c5032d">
<p>
It is possible to demonstrate that every contrast is associated with
a sum of squares:
</p>

<p>
\[ SS_c = \frac{(\sum^{m}_{i=1} c_i 1/n_i \sum^{n_i}_{j=1} y_{ij})^2}{\sum^{m}_{i=1} \frac{c_i^2}{n_i}}  \]
</p>


<p>
having <i>one degree of freedom</i>. Notice, that above the nominator
represents the squared treatment group mean, weighted by the contrast
weight assigned to each group. The nominator is then weighted by being
multiplied by the number of observations in each group normalized by
the contrast weight.
</p>

<p>
It is then possible to see that under the null hypothesis it holds
</p>

<p>
\[\frac{MS_c}{MS_E} = \frac{SS_c}{MS_E} \mathtt{\sim} F_{1,N-m}\]
</p>
</div>
</div>


<div id="outline-container-org10d6bbe" class="outline-2">
<h2 id="org10d6bbe">Orthogonal, Non-redundant Contrasts</h2>
<div class="outline-text-2" id="text-org10d6bbe">
<p>
A particularly interesting family of contrasts are orthogonal
contrasts. These are non redundant contrasts. 
</p>

<p>
Two contrasts \(c\) and \(c^{*}\) are orthogonal when it holds:
</p>

<p>
\[ \sum_{i = 1}^{m} \frac{c_i * c_i^{*}}{n_i} = 0\]
</p>


<p>
Then no information contained in one contrast is present in a second
and contrasts are therefore non-redundant. To see that consider the
toy example:
</p>

<p>
\[ \psi_1  =   \begin{pmatrix} 1 \\ 1 \\ 0\end{pmatrix}  \hspace{10mm}   \psi_2  = \begin{pmatrix} 0 \\ 0 \\ 1\end{pmatrix} \hspace{10mm}  \psi_3  = \begin{pmatrix} 1 \\ 0 \\ 1\end{pmatrix}  \]
</p>

<p>
Then, &psi;<sub>3</sub> is a redundant contrast and having it or not does not add
any info to the experimenter.
</p>

<p>
What is important now is that given our <i>m</i> treatments, we can always
find <i>m-1</i> orthogonal treatments; this because one dimension is
already occupied by the global mean, where we constrain \(c = (1,
...., 1)^\intercal\). Given those, we individual sum of squares for
individual contrasts are independent by definition and the total
<i>treatment sum of squares</i> can be rewritten as the sum of the
individual contrasts.
</p>

<p>
\[ SS_{c_1} + .... + SS_{c_m} = SS_{Trt} \] 
</p>

<p>
<b>This means that posing the right <i>m-1</i> questions about the treatment
you get all the information that is involved with the it.</b> 
</p>

<br>
</div>
</div>

<div id="outline-container-org3777984" class="outline-2">
<h2 id="org3777984">Testing Multiple Contrasts</h2>
<div class="outline-text-2" id="text-org3777984">
<p>
While using contrasts is a neat tool to test a set of linear
hypothesis one should be careful, when repeatedly testing multiple
contrasts hypothesis.
</p>

<p>
For instance, when testing <i>j</i> independent Null \(H_{0,j}\) with
confidence &alpha;, the probability of rejecting at least one Null when
<i>all of the Null are correct</i> is
</p>

<p>
\[ 1 - (1-a)^j\]
</p>

<p>
It is straightforward to see that such probability tends to one, no
matter how high the confidence as j gets large.
</p>

<p>
This requires a way to holistically handle all of the different
Nulls. This is the topic of the chapter.
</p>

<p>
In this sense, it is of use looking at the <i>family-wise error
rate</i>. This expresses the /probability of rejecting at least one of
the true H<sub>0</sub> [V] /:  
</p>

<p>
\[ FWER = P(V \geq 1). \]
</p>

<p>
Given the above definition we say that <b>a procedure controls the
family-wise error rate in the strong sense</b> at level &alpha; iff:
</p>

<p>
\[ FWER = P(V \geq 1) \leq \alpha. \]
</p>

<br>
</div>

<div id="outline-container-orgf2c45c5" class="outline-3">
<h3 id="orgf2c45c5">FWER and FDR</h3>
<div class="outline-text-3" id="text-orgf2c45c5">
<p>
An important by product results that naturally emerges is that when
controlling the family-wise error rate in the strong sense, we
automatically bound the <i>false discovery rate (FDR)</i>  defined as:
</p>

<p>
\[ FDR = \frac{number\ of\ rejected\ Null\ where\ the\ Null\ was
correct}{total\ number\ of\ rejected\ Null} = fraction\ of\ wrongly\ rejected\
Null\]
</p>

<p>
This concretely means that when bounding the FWER to &alpha; we
automatically bound the FDR to &alpha;.
</p>
</div>
</div>

<div id="outline-container-orge4cee3a" class="outline-3">
<h3 id="orge4cee3a">FWER and Confidence Intervals</h3>
<div class="outline-text-3" id="text-orge4cee3a">
<p>
We can extend the concept of FWER to confidence intervals. This should
then express the probability that <i>all intervals</i> cover the
corresponding true parameter value is (1 - &alpha;). This means that
when looking at all the confidence intervals simultaneously we should
should get a probability of (1-&alpha;) of capturing the true
parameter. The concept is defined as <b>simultaneous confidence
intervals</b>. 
</p>

<p>
Notice, that in the above cases, we typically start with “individual”
p-values that we modify (or adjust) such that the appropriate overall
error rate (like FWER) is being controlled. Interpretation of an
individual p-value is as you learned it in your introductory course
(“the probability to observe an event as extreme as …”). The modified
p-values should be interpreted as the smallest overall error rate such
that we can reject the corresponding null hypothesis. 
</p>
</div>
</div>

<div id="outline-container-orgf9fac76" class="outline-3">
<h3 id="orgf9fac76">Bonferroni</h3>
<div class="outline-text-3" id="text-orgf9fac76">
<p>
This is a rather generic but very conservative approach and it
obviates to compute the exact FWER and Simultaneous Confidence
Interval introduced above.
</p>

<p>
It consists of using the rather restrictive <b>individual</b> significance
level \(a^{*} = \frac{a}{m}\). This will control the <i>family-wise error
rate</i> in the strong sense albeit being quite restrictive. 
</p>

<p>
Moreover, the confidence intervals based on the adjusted significance
level are <i>simultaneous</i>.
</p>
</div>
</div>

<div id="outline-container-org1863ac9" class="outline-3">
<h3 id="org1863ac9">Bonferroni-Holm</h3>
<div class="outline-text-3" id="text-org1863ac9">
<p>
This is generally <i>less conservative</i> and uniformly more powerful than
Bonferroni. 
</p>

<p>
It works as follows:
</p>

<ol class="org-ol">
<li>Sort p-values from the smallest to the largest: $p<sub>(1)</sub> &le;
&#x2026; &le; p<sub>(j)</sub> $</li>

<li>for i = 1,2 &#x2026;.: Reject the Null if \(p_{(i)} \leq \frac{a}{m-i+1}\)</li>

<li>stop when reaching the first non-significant p-value.</li>
</ol>

<p>
Notice therefore that under Bonferroni-Holm the higher p-values are
compared with lower confidence levels. This has therefore a relaxing
effect in comparison to the standard Bonferroni test.
</p>
</div>
</div>

<div id="outline-container-org71b7c51" class="outline-3">
<h3 id="org71b7c51">Scheffé</h3>
<div class="outline-text-3" id="text-org71b7c51">
<p>
The Scheffé procedure controls for the search over <i>any possible
contrast</i>. This means that we can try out as many contrasts as we like
and still get honest p-values. This comes, however at the expense of
low power. That is of a <i>low probability of rejecting when the
alternative is right</i>.  Scheffé contains therefore, the issue of a
Type I error at the expense of a higher probability of committing Type
II errors.
</p>

<p>
The Scheffé procedure works as follows:
</p>

<ol class="org-ol">
<li>Calculate the F-ratio as if ordinary contrast</li>

<li>use the distribution $ (m-1) F<sub>m-1, N-m</sub>$ instead of \(F_{m-1,
   N-m}\) to calculate p-values.</li>
</ol>
</div>
</div>


<div id="outline-container-orge85f92f" class="outline-3">
<h3 id="orge85f92f">Tukey Honest Significance Difference</h3>
<div class="outline-text-3" id="text-orge85f92f">
<p>
A special case for a multiple testing problem is when comparing
between all possible pairs of treatments. 
</p>

<p>
Overall, there are \(frac{m* (m-1)}{2}\) treatment pairs that we can
compare. When testing all such different pairs we could proceed by
calculating the pairwise F-statistics and applying <i>Bonferroni-Holm</i>
to obtain the p-values.
</p>

<p>
A more <i>powerful</i> approach in such case consists in applying the
<i>Tukey Honest Significant Difference</i>. This is custom tailored for
this test design and the details were omit in the class script.
</p>
</div>
</div>


<div id="outline-container-org08f0b39" class="outline-3">
<h3 id="org08f0b39">Multiple Comparison with a Control (MCC)</h3>
<div class="outline-text-3" id="text-org08f0b39">
<p>
This is in a similar spirit as the Tukey Honest Significance
difference above. It is a high <i>power</i> test for comparing the all the
treatment groups with an overall control group.
</p>

<p>
The corresponding test yielding the best p-values in such a case is
called <i>Dunnett procedure</i> and might be further inspected inc ase of
interest.
</p>
</div>


<div id="outline-container-org616dd92" class="outline-4">
<h4 id="org616dd92">Literature</h4>
<div class="outline-text-4" id="text-org616dd92">
<p>
<a href="https://stat.ethz.ch/lectures/as19/anova.php#course_materials">Applied Anaysis of Variance - ETH Lecture Notes - Autumn 2019</a>
</p>
</div>
</div>
</div>
</div>
