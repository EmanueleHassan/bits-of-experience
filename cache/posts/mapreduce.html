<br>
<br>

<p>
This post go briefly over the key idea of using MapReduce as a way to
parallelize operations over multiple distributed machines. 
</p>

<p>
This was in fact the first tool that was developed for parallelizing
computation over multiple machines and not simply to use multiple
machines for the storage layer.
</p>

<!-- TEASER_END -->

<br>

<div id="outline-container-orge1abf7a" class="outline-2">
<h2 id="orge1abf7a">The logical Layer</h2>
<div class="outline-text-2" id="text-orge1abf7a">
<p>
The basic idea of MapReduce is to take the data from the storage
layer (be it in on block-storage, HDFS, the local file system) and
independently from the storage format (i.e. be it in tree form,
tabular form etc.) process it in a distributed parallel fashion.
</p>

<p>
In order to do that <code>MapReduce</code> leverages on the idea of dividing
the input data into several chuncks. And operate on these different
chuncks independently. The name of <code>MapReduce</code> come then in this
sense by the fact that the way we operate on the data is by
<code>Mapping</code> the input chuncks data to some processed data given some
function. And finally by aggregating, i.e. <code>Reducing</code> such
distributed computations. 
</p>

<p>
In the best case scenario we would therefore observe a situation
close to the following one
</p>

<br>
<br>

<img width="100%" height="100%" src="../../images/Bildschirmfoto_2020-05-23_um_15.47.17.png" class="center">

<br>
<br>

<p>
I.e. in the ideal situation we would simply have to rely on the
input chuncks to perform our <code>MapReduce</code> job as all of the necessary
information is contained in there.
</p>

<p>
This is ideal in the way that all of the data live over the same
machine and no big networking bandwidth and coordination issue arise
to transport the necessary data from the distributed network of
machine to the CPUs where the necessary computation is performed.
</p>

<p>
However, this is not always the case and possible as we might well
run into cases where we need additional information such as when we
desire to output some sorted result or in the case we might want to
sum over all of the data. In such case we indeed run into the case
where we would have to communicate over the entire distributed
network and transport the bits of information across.
</p>

<p>
The general MapReduce situation looks therefore rather as follows: 
</p>

<br>
<br>

<img width="100%" height="100%" src="../../images/Bildschirmfoto_2020-05-23_um_15.57.05.png" class="center">

<br>
<br>

<p>
A final thing to notice is that the input data of MapReduce must be
of a format through which the <code>mapping</code> and <code>reducing</code> component of
the operations makes sense overall. In this sense while you might
well operate on any data format stored (be it tabular, tree etc.)
when feeding the data you must transform them into a <code>key-value</code>
form. In such a way it will then be possible to apply the mapping
function on each and every key-value tuple and finally to
<code>aggregate</code> over the keys.
</p>

<p>
However, important to notice is here that the key-value pairs do not
have to share the same form after each mapping function but they
rather have to respect simply the <code>key-value</code> structure where at
each step a key with the corresponding value is identifiable.
</p>

<br>
<br>

<img width="100%" height="100%" src="../../images/Bildschirmfoto_2020-05-23_um_16.16.47.png" class="center">

<br>
<br>
</div>
</div>


<div id="outline-container-org8a44824" class="outline-2">
<h2 id="org8a44824">Internal Operations of MapReduce Job</h2>
<div class="outline-text-2" id="text-org8a44824">
<p>
Important is to understand what goes on under the hood when you
launch a Hadoop MapReduce job.
</p>

<p>
While the user simply specifies the <code>map</code> and the <code>reduce</code> component
that is necessary to perform the job, behind the scenes <code>Hadoop</code>
takes care of three different tasks.
</p>

<p>
It first <code>splits</code> the stored key-values in logical 128MB blocks -
the usual HDFS block size where the data live -. Such splits -
i.e. the underlying HDFS blocks - should store as homogeneous keys
as possible.
</p>

<p>
The map acts then on the logical splits above. After the <code>map</code>
occurs it sorts the data by keys and partition them over all of the
distributed machines. The <code>reducer</code> is then applied.
</p>

<p>
Important is here to understand that if the map resulted in same
homogeneous keys on different machines these represent already
partitioned keys on which the <code>reducer</code> is applied.
</p>

<br>
<br>

<img width="100%" height="100%" src="../../images/Bildschirmfoto_2020-05-23_um_16.48.09.png" class="center">

<br>
<br>

<br>
</div>
</div>

<div id="outline-container-org9637abc" class="outline-2">
<h2 id="org9637abc">The physical Layer / Architecture</h2>
<div class="outline-text-2" id="text-org9637abc">
<p>
MapReduce acted originall right on top of <code>HDFS</code>. It leveraged
therefore the existing <b>master-slave</b> architecture for assigning the
parallel computation jobs across machines.
</p>

<p>
Recall that in HDFS you observe a <code>Namenode</code> acting as Master -
i.e. assigning the replication and tasks across of the distributed
network and keeping the state of the latter - and various <code>DataNodes</code>
acting as Slaves - i.e. storing the data and sending state information
to the master -.
</p>

<p>
Hadoop MapReduce adds another Master, the <code>JobTracker</code> being
responsible for assigning and distributing the tasks among the slaves,
i.e. the <code>TaskTracker</code>, executing the computations.
</p>

<p>
Often the Master processes of the <code>JobTracker</code> and the <code>NameNode</code> live
on the very same machine as well as the <code>TaskTracker</code> and <code>DataNodes</code>
processes.
</p>

<br>
<br>

<img width="100%" height="100%" src="../../images/Bildschirmfoto_2020-05-24_um_09.39.14.png" class="center">

<br>
<br>

<p>
Notice, also that the benefit of having the <code>TaskTracker</code> on the same
physical machine of the <code>DataNode</code> is that it might well be possible
to <b>shortcircuit</b> the shuffling as if data are well
distributed/splitted by key in the network it might well be that for
many most common operations you can act locally on the data of the
<code>DataNode</code> for the <code>map</code> component. 
</p>


<p>
Important, is then to understand that it is the job of the
<code>JobTracker</code> to assign the different <code>map</code> and <code>reduce</code> <b>tasks</b> to the
different <code>TaskTrackers</code>.
</p>

<p>
The process looks then as follows
</p>

<br>
<br>

<img width="100%" height="100%" src="../../images/mapreduce.svg" class="center">

<br>
<br>

<p>
Three important remarks:
</p>

<ul class="org-ul">
<li>It is in fact the job of the <code>JobTracker</code> to assign the different
<code>Map</code> jobs to the different <code>TaskTrackers</code> in a way that the
<b>shortcircuiting</b> is maximized.</li>

<li>This first version of MapReduce was highly inefficient as resources
were sitting idle as resources are being assigned at the beginning
of MapReduce. Slots reserved to the Reducer will therefore be idle
in the mapping phase, while slots reserved to the Mapper will be
idle in the Reduce phase.</li>

<li>An HTTP server, be it an <b>apache</b> or <b>ngnix</b>, is available on each
<code>TaskTracker</code> node. In the reducer phase a process on the
TraskTracker is then responsible for getting the necessary keys for
each reduce job via HTTP connection over the different TaskTracker
servers. This is where the expensive shuffling takes place.</li>
</ul>


<br>
</div>
</div>

<div id="outline-container-orgbc923d2" class="outline-2">
<h2 id="orgbc923d2">On Shuffling Optimization</h2>
<div class="outline-text-2" id="text-orgbc923d2">
<p>
It is clear that shuffling is expensive. The coordination game is
expensive, the network bandwidth is used and the waiting time for each
job increase.
</p>

<p>
A desire in MapReduce is therefore to reduce the shuffling of the data
to the bare minimum.
</p>

<p>
In order to do that MapReduce often works with a <b>Combiner</b> phase that
acts in the middle of the <b>mapping</b> and <b>reducer</b> phase.
</p>

<p>
The key idea here is that if it is possible to compress the data
through the reduce function before the shuffling phase, then of course
you would have synthesize the amount of data while keeping the
information and the sheer size of the data to be transmitted over HTTP
is highly reduced.
</p>

<p>
Two conditions must be fulfilled so that the <b>combine</b> operation above
is possible via the reduce function
</p>

<ul class="org-ul">
<li>key-values for reduce input and reduce output must be identical.</li>

<li>the commutative and associative laws must hold, i.e. it does not
matter which operation is done first and in which order you
aggregate the operations.</li>
</ul>
</div>
</div>

<div id="outline-container-orgaa89b3d" class="outline-2">
<h2 id="orgaa89b3d">Literature</h2>
<div class="outline-text-2" id="text-orgaa89b3d">
<p>
<a href="https://www.systems.ethz.ch/courses/spring2020/bigdataforeng/material">Big Data for Engineers - ETH course</a>
</p>
</div>
</div>
