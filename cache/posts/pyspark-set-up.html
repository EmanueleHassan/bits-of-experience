<div class="HTML" id="orgf35fbdd">
<p>
&lt;br&gt;
&lt;br&gt;
</p>

</div>

<p>
This post aims at providing an overview of the necessary steps
required in order to leverage Apache Spark through the Python API and
the PySpark module.
</p>

<p>
Despite the information is vastly reported over the internet my usual
<i>procedere</i> when I deal with new tools and software is to write a
short piece of note when learning new software that is intended as an
overview over the tool and serve as a beginner cheat sheet.
</p>

<!-- TEASER_END -->

<div id="outline-container-orgb2e1af8" class="outline-3">
<h3 id="orgb2e1af8">Disclaimer</h3>
<div class="outline-text-3" id="text-orgb2e1af8">
<p>
Ok, before starting let me underline that this post falls under the
category <i>learn by posting</i>.  As such it may contain a flawed
theoretical understanding about the subject and some inaccuracies.
</p>

<p>
You are can read more about this kind of posts on the home-page under
<a href="https://marcohassan.github.io/bits-of-experience/pages/bits-of-experience-a-readable-view-on-my-study-adventures/">this link</a>.
</p>
</div>
</div>

<div id="outline-container-org5804632" class="outline-2">
<h2 id="org5804632">PySpark - Why?</h2>
<div class="outline-text-2" id="text-org5804632">
<p>
During the last decades we observed a explosion of laptop and mobiles
usage. They entered the houses of the vast majority of people in
developed countries and together with that the <i>internet</i> business
boomed.  Together with that a huge amount of data started to be
collected to the point where the data collected became so big they
could not be saved over a single hardware.7
</p>

<p>
This tremendous amount of data became <i>Big Data</i>, which is simply a
huge amount of data that requires special technology and software to
be treated as it can poorly be handled through standard hardware on
local machines.
</p>

<p>
In the early 2000nds Google published a white paper proposing a
distributed file system architecture allowing to save and operate on
vast amount of data by distributing data across multiple hard drives
and saving replicas of them to handle system failures. The idea was
soon embraced by both the academic and the industry soon becoming the
norm.  The <i>hadoop distributed file system</i>, a Java implementation of
google distributed open source implementation was developed and is by
now an institution in the market.
</p>

<p>
With the industry shifting to <i>hdfs</i> its limits soon became visible.
First of all Mapreduce operates through data batch processing and
iterative Mapreduce jobs. What this practically means is that while
the relation between consequent jobs is known to the user, it is not
the framework and therefore there is no possibility of optimization of
the operations for the Mapreduce framework. Moreover, Mapreduce offers
no possibility of saving intermediate data in memory in the case a job
is small enough to be sure that system failures are unlikely and no
need for data replication across a distributed file system is needed.
</p>

<p>
Here is where <i>Spark</i> comes to play. Spark offers the possibility to
store data in the memory, effectively eliminating intermediate disk
persistence and thus improving completion time. 
</p>

<p>
Apart from real-time and batch processing, Apache Spark supports
interactive queries and iterative algorithms also. Apache Spark has
its own cluster manager, where it can host its application. It
leverages Apache Hadoop for both storage and processing. It uses HDFS
(Hadoop Distributed File system) for storage and it can run Spark
applications on YARN as well.
</p>

<p>
In simple terms if you want to use large computing clusters but do not
want to parallelize your programs Apache Spark kicks in.
</p>
</div>
</div>

<div id="outline-container-org6f09a5c" class="outline-2">
<h2 id="org6f09a5c">Where do data come from?</h2>
<div class="outline-text-2" id="text-org6f09a5c">
<p>
There are esentially two options. The first one is to connect hardware
coupled with the data directly/physically to the worker nodes. This
solution is called <i>JBOD</i> or <i>direcly attached storage</i> approach. In
order then to treat the different connected hardwares as a single file
system you need then to integrate a software component, namely HDFS.
</p>

<p>
A second approach consists in leveraging external data with a special
type of networking connection allowing a very performing I/O
stream. This through <i>switching fabric</i>.
</p>
</div>
</div>

<div id="outline-container-org4bddc25" class="outline-2">
<h2 id="org4bddc25">Software Download</h2>
<div class="outline-text-2" id="text-org4bddc25">
<p>
Notice all of the code below was executed on Ubuntu Linux
OS. 
</p>

<p>
For MacOS the <a href="https://www.tutorialkart.com/apache-spark/how-to-install-spark-on-mac-os/">usage of homebrew</a> is recommended.
</p>
</div>

<div id="outline-container-orgf9063a6" class="outline-3">
<h3 id="orgf9063a6">Java Installation</h3>
<div class="outline-text-3" id="text-orgf9063a6">
<p>
Pyspark is an API leveraging a Java Virtual Machine for connecting and
operating on Spark data frames. As such the proper installation of
Java is necessary for the smooth operation of Pyspark.
</p>

<p>
<b>Install Java</b>
</p>

<pre class="example" id="org608e471">
sudo apt install openjdk-8-jre-headless 
</pre>

<p>
<i>Notice:</i> You used a different Javascript and runtime
environment to the one mentioned online. Consider that if having
integration issues at a later stage and rather install this Java SE
runtime environment <a href="https://docs.oracle.com/en/java/javase/12/install/installation-jdk-linux-platforms.html#GUID-ADC9C14A-5F51-4C32-802C-9639A947317F">SE</a>.
</p>
</div>
</div>

<div id="outline-container-orgcfcdc60" class="outline-3">
<h3 id="orgcfcdc60">Spark installation</h3>
<div class="outline-text-3" id="text-orgcfcdc60">
<p>
Download the compressed spark software under <a href="https://www.apache.org/dyn/closer.lua/spark/spark-2.4.3/spark-2.4.3-bin-hadoop2.7.tgz">this link.</a>
</p>

<p>
Unzip it in your repository of choice.
</p>


<pre class="example" id="org9029a62">
$ tar -xvf ~/Scaricati/spark-2.4.3-bin-hadoop2.7.tgz
</pre>

<p>
Set Spark in your path and in the Py4j API path.
</p>

<pre class="example" id="org6a1aa4a">
$ export SPARK_HOME=&lt;unziped file location&gt;/spark-2.1.0-bin-hadoop2.7
$ export PATH=$PATH:/home/hadoop/spark-2.1.0-bin-hadoop2.7/bin
$ export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$PYTHONPATH
$ export PATH=$SPARK_HOME/python:$PATH
</pre>

<p>
After the above installation you should be able to run spark directly
on a jupyter notebook.
</p>
</div>
</div>

<div id="outline-container-org13572ba" class="outline-3">
<h3 id="org13572ba">PySpark on Jupyter Notebook and Emacs Integration</h3>
<div class="outline-text-3" id="text-org13572ba">
<p>
In order to start and connect to pyspark on your local machine it will
then just be necessary to start an ipython kernel by running
</p>

<pre class="example" id="org7eb0d2b">
$ ipython kernel
</pre>

<p>
This will prompt the name of a <i>.json</i> file that contains the
information (port, connection protocol etc.) to connect to the kernel
in a similar way to the following message:
</p>

<blockquote>
<p>
To connect another client to this kernel, use:
    &#x2013;existing kernel-5060.json
</p>
</blockquote>

<p>
You can then specify your kernel.json as your session argument in
<code>ob-ipython</code> and connect to the running kernel. At this point after
installing findspark via
</p>

<pre class="example" id="org4ea316e">
$ pip3 install --user findspark
</pre>

<p>
you are good to import your pyspark module and leverage the Pyspark
API on your favourite editor: <i>emacs</i>
</p>

<div class="highlight"><pre><span></span>import findspark
findspark.init()
import pyspark
import random
sc = pyspark.SparkContext(appName=&quot;Pi&quot;)
num_samples = 100000000
def inside(p):     
  x, y = random.random(), random.random()
  return x*x + y*y &lt; 1
count = sc.parallelize(range(0, num_samples)).filter(inside).count()
pi = 4 * count / num_samples
print (pi)
sc.stop()
</pre></div>

<pre class="example">
3.14109952
</pre>


<p>
In the case you will want to leverage a more powerful machine and run
your pyspark application on a server you might follow the logic
explained in the following post keeping the structure outlined in the
post. <a href="https://necromuralist.github.io/posts/programming/remote-jupyter-sessions-with-ob-ipython/">Connect to remote ipython kernel.</a>
</p>
</div>
</div>
</div>
