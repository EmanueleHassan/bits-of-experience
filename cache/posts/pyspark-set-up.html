<p>
This post aims at providing an overview of the necessary steps
required in order to leverage Apache Spark through the Python API and
the PySpark module.
</p>

<p>
Despite the information is vastly reported over the internet my usual
<i>procedere</i> when I deal with new tools and software is to write a
short piece of note when learning new software that is intended as an
overview over the tool and serve as a beginner cheat sheet.
</p>

<!-- TEASER_END -->

<div id="outline-container-sec-1" class="outline-2">
<h2 id="sec-1">Disclaimer</h2>
<div class="outline-text-2" id="text-1">
<p>
Ok, before starting let me underline that this post falls under the
category <i>learn by posting</i>.  As such it may contain a flawed
theoretical understanding about the subject and some inaccuracies.
</p>

<p>
You are can read more about this kind of posts on the home-page under
<a href="https://marcohassan.github.io/bits-of-experience/pages/bits-of-experience-a-readable-view-on-my-study-adventures/">this link</a>.
</p>
</div>
</div>

<div id="outline-container-sec-2" class="outline-2">
<h2 id="sec-2">PySpark - Why?</h2>
<div class="outline-text-2" id="text-2">
<p>
During the last decades we observed a explosion of laptop and mobiles
usage. They entered the houses of the vast majority of people in
developed countries and together with that the <i>internet</i> business
boomed.  Together with that a huge amount of data started to be
collected to the point where the data collected became so big they
could not be saved over a single hardware.7
</p>

<p>
This tremendous amount of data became <i>Big Data</i>, which is simply a
huge amount of data that requires special technology and software to
be treated as it can poorly be handled through standard hardware on
local machines.
</p>

<p>
In the early 2000nds Google published a white paper proposing a
distributed file system architecture allowing to save and operate on
vast amount of data by distributing data across multiple hard drives
and saving replicas of them to handle system failures. The idea was
soon embraced by both the academic and the industry soon becoming the
norm.  The <i>hadoop distributed file system</i>, a Java implementation of
google distributed open source implementation was developed and is by
now an institution in the market.
</p>

<p>
With the industry shifting to <i>hdfs</i> its limits soon became visible.
First of all Mapreduce operates through data batch processing and
iterative Mapreduce jobs. What this practically means is that while
the relation between consequent jobs is known to the user, it is not
the framework and therefore there is no possibility of optimization of
the operations for the Mapreduce framework. Moreover, Mapreduce offers
no possibility of saving intermediate data in memory in the case a job
is small enough to be sure that system failures are unlikely and no
need for data replication across a distributed file system is needed.
</p>

<p>
Here is where <i>Spark</i> comes to play. Spark offers the possibility to
store data in the memory, effectively eliminating intermediate disk
persistence and thus improving completion time. 
</p>

<p>
Apart from real-time and batch processing, Apache Spark supports
interactive queries and iterative algorithms also. Apache Spark has
its own cluster manager, where it can host its application. It
leverages Apache Hadoop for both storage and processing. It uses HDFS
(Hadoop Distributed File system) for storage and it can run Spark
applications on YARN as well.
</p>
</div>
</div>

<div id="outline-container-sec-3" class="outline-2">
<h2 id="sec-3">Software Download</h2>
<div class="outline-text-2" id="text-3">
<p>
Notice all of the code below was executed on Ubuntu Linux
OS. Adjustments might be required for MacOS environments.
</p>
</div>

<div id="outline-container-sec-3-1" class="outline-3">
<h3 id="sec-3-1">Java Installation</h3>
<div class="outline-text-3" id="text-3-1">
<p>
Pyspark is an API leveraging a Java Virtual Machine for connecting and
operating on Spark data frames. As such the proper installation of
Java is necessary for the smooth operation of Pyspark.
</p>

<p>
Install Java
</p>

<div class="highlight"><pre><span></span>sudo apt install openjdk-8-jre-headless 
</pre></div>

<p>
<i>Notice:</i> You used a different Javascript and runtime
environment to the one mentioned online. Consider that if having
integration issues at a later stage and rather install this Java SE
runtime environment <a href="https://docs.oracle.com/en/java/javase/12/install/installation-jdk-linux-platforms.html#GUID-ADC9C14A-5F51-4C32-802C-9639A947317F">SE</a>.
</p>
</div>
</div>
</div>
