<style>
img {
display: block;
margin-top: 60px;
margin-bottom: 60px;
margin-left: auto;
margin-right: auto;
width: 70%;
height: 100%;
class: center;
}
</style>


<p>
Here are some Notes about the topic of my Master Thesis - Bayesian
Networks.
</p>

<p>
Note that most of these Notes are based on <i>Probabilistic Graphical
Models - Principles and Techniques</i> (<a href="https://www.amazon.de/Probabilistic-Graphical-Models-Principles-Computation/dp/0262013193">Koller and Friedman</a>).
</p>

<!-- TEASER_END -->


<div id="outline-container-org77556c1" class="outline-2">
<h2 id="org77556c1">Bayesian Networks</h2>
<div class="outline-text-2" id="text-org77556c1">
<p>
The general outlook.
</p>

<p>
So recall that in general you have three elements in Bayesian
Networks:
</p>

<ul class="org-ul">
<li>Representation

<ul class="org-ul">
<li><p>
how do you represent the joint probability of the events as a
network (i.e. as a graph data structure)? Can such structure
represent the joint in a compact way due to the conditional
independence relations?
</p>

<p>
<b>Note 1:</b> that such compact formulation is one of the key benefits of
Bayesian Networks as it really gives the possibility of shrinking
the amount of parameters needed to describe the full joint
probability leveraging the independence structure among the RVs.
</p>

<p>
<b>Note 2:</b> this formulation is <i>transparent</i>, i.e. highly
understandable also to non-AI experts. It is so to say highly
explainable and in this new buzz of <i>explainable AI</i> a solid
option.
</p></li>
</ul></li>

<li>Inference

<ul class="org-ul">
<li>given some information about some Parent variables, how can I
infer/compute the distribution of the children in the Network?</li>
</ul></li>

<li>Learning

<ul class="org-ul">
<li>given some observed data, how can I use such information to
construct / (infer) / learn the  <i>structure</i> of the network?</li>

<li>given some observed data, how can I learn the <i>parameters</i> of the
network? I.e. how can I use the information content of the data to
derive some plausible parameterization of the network.</li>
</ul></li>
</ul>


<p>
So these are the main tasks you have to deal with in Bayesian
Networks. Basically you can do all of the three in a very simple way,
which is from a theoretical standpoint very concrete and
straightforward or you can start to consider all the aspects of the
problem going quickly towards more complex situations.
</p>
</div>

<div id="outline-container-org954c6cf" class="outline-3">
<h3 id="org954c6cf">Representation</h3>
<div class="outline-text-3" id="text-org954c6cf">
<br>
<br>

<style>
.bg-svg {
  width: 40%;
  background-image: url(../../images/bayesNet1.svg);
  background-size: cover;
  height: 0;
  padding: 0; /* reset */
  padding-bottom: 92%;
  border: thin dotted darkgrey;
  float:right;
  margin-left: 5%;
}
.content p{
    display: block;
    margin: 2px 0 0 0;
}
</style>

<div style="width: 100%">
   <div style= "width: 70%; margin-left = 5%;">
      <div class="bg-svg">
   </div>
   <p>

   <br/>
   <br/>

   As mentioned bayesian networks allow us to express the joint
   through less parameters.

   <br/>
   <br/>

   The idea is that you factorize the joint as a product of the
   conditionals and given the parameterization of the conditionals you
   fully specify the joint. Given the independence structures the
   number of factorization of conditional terms is limited and the
   overall necessary parameters to specify the joint small.

   <br/>
   <br/>
   
   For instance if a Variable D is fully determined by its parents B,
   C in this graph:

   <br/>   
   <br/>
   
   Then you might well understand that given B, C you do not need
   P(D | A, B, C) parameters as P(D | B, C) suffices.
  </p>
  <br style="clear: both;" />
</div>

<br>
<br>
<br>
<br>   

<p>
A concrete example is the following:
</p>

<img src="../../images/Bildschirmfoto_2021-02-15_um_13.21.25.png">

<p>
Notice there that instead of needing 2 (Diff) * 2 (Int) * 3
(Grade) * 2 (Sat) * 2 (Let) = 48 parameters to describe the joint
you simply need 2 + 2 + 12 + 6 = 22.
</p>

<p>
Given this understanding it is immediate to see that Bayesian
Networks are defined as above, i.e. as a graph data structure to
which <i>local probabilities</i> are applied. In the specific each RV in
the graph is associated with <i>conditional probability distributions
(CPD)</i> that specify the distribution given each possible joint
assignment of values to its parents. And the graph structure
together with the CPD specifies the Bayesian Network.
</p>

<p>
A <b>second</b> representation/ definition of Bayesian Networks is to
define it via a <i>global probability P</i> together with the independence
relations determined by the graph.
</p>

<p>
To determine independence relations in graphs you can use standard
logic where the argument is essentially the following:
</p>

<blockquote>
<p>
Our intuition tells us that the parents of a variable “shield” it
from probabilistic influence that is causal in nature. In other
words, once I know the value of the parents, no information
relating directly or indirectly to its parents or other ancestors
can influence my beliefs about it. However, information about its
descendants can change my beliefs about it, via an evidential
reasoning process. (Koller and Friedman)
</p>
</blockquote>

<p>
Such that you would have the following <i>local independence
structures</i>:
</p>

<p>
\[ For each variable X_i : (X_i \perp NonDescendants X_i | Parents
   X_i) \]
</p>

<p>
Notice that such set of independence is called an I-map for a
probability distribution <i>P</i>. You then say that a graph <i>G</i> is an
I-map for <i>P</i> if it satisfies the I-map relations specified <i>I(P)</i>.
</p>

<p>
And you would ultimately have the following definition:
</p>

<img src="../../images/Bildschirmfoto_2021-02-15_um_14.50.38.png">

<p>
So that you basically take here the opposite direction, from a
joint distribution <i>P</i> and the local independence structure you
have a fully specified Bayesian Network.
</p>

<p>
<b>Note</b> that you can go from one representation to the other and the
BN is defined <b>if and only if</b> you can from one to the other.
</p>
</div>

<div id="outline-container-orgac9f551" class="outline-4">
<h4 id="orgac9f551">On Graph Dependencies and D-separation</h4>
<div class="outline-text-4" id="text-orgac9f551">
<p>
Given the above discussion and the fact that it is possible to
determine the BN given a joint density and a Graph structure, the
question now is on how to extract the conditional independence
structures implied by a graph, i.e. to extract the I-map
relations.
</p>

<p>
In order to do that a simple algorithm exists the <i>d-separation
algorithm</i>.
</p>

<p>
The idea here is the following. You know that for three nodes X,
Y, Z there exists a dependence structure between X and Y if one of
the following conditions <b>hold</b>:
</p>

<img src="../../images/Bildschirmfoto_2021-02-15_um_15.18.31.png">

<p>
This is quite intuitive.
</p>

<p>
It follows now that we can quickly assess whether two variables
are generally conditionally independent by making reasonings
leveraging the active trails as above.
</p>

<p>
I.e. for two variables to be <b>dependent</b> there must be an active
trail as defined by the conditions above.
</p>

<p>
Notice that for instance in the student BN you can investigate
the conditional independence between SAT and Difficulty as
follows:
</p>

<img src="../../images/Bildschirmfoto_2021-02-15_um_15.25.30.png">


<p>
Generally it holds:
</p>

<img src="../../images/Bildschirmfoto_2021-02-15_um_15.26.42.png">

<p>
You can then find in the book an algorithm for checking
d-separation, if interested at any point in time. Notice that
there is are also reasonings about <i>completeness</i> and <i>soundness</i>
of d-separation. I.e. how well that covers and fully specifies
independence structures of <i>P</i>.
</p>

<p>
I write in here the definition of <i>completeness</i> and <i>soundness</i>
should it be of interest at any point at a later stage:
</p>

<p>
<i>Soundness</i>:
</p>

<p>
If two nodes X and Y are d-separated given some Z, then we are
guaranteed that they are, conditionally independent given Z.
</p>

<p>
<i>Completeness</i>:
</p>

<p>
D-separation is complete if it detects all of the possible
independencies. I.e. if two variables X and Y are independent
given Z, then they are d-separated.
</p>

<p>
Formally:
</p>

<img src="../../images/Bildschirmfoto_2021-02-19_um_09.32.33.png">
</div>
</div>

<div id="outline-container-org9c5c27d" class="outline-4">
<h4 id="org9c5c27d">On CPD</h4>
<div class="outline-text-4" id="text-org9c5c27d">
<p>
So far we discussed the possibility of representing the
high-dimensional joint distribution into a product of
lower-dimensional CPDs or factors, i.e. a product of local
probabilities models.
</p>

<p>
In this section we explore more into the detail the possibility of
representing such CPDs.
</p>
</div>


<ul class="org-ul">
<li><a id="org75bda20"></a>Tabular CPD<br />
<div class="outline-text-5" id="text-org75bda20">
<p>
This is the most basics form of CPD. It works for spaces composed
solely of <b>discrete</b> valued RV.
</p>

<p>
It consists in expressing the \(P(X | PA_X)\) as a table that
contains the joint probability of \(X \and PA_X\).
</p>

<p>
This is essentially what was given in the example above.
</p>

<p>
<b>Note:</b> it is important to realize that the number of joint
probabilities that you have to express is given by
</p>

<p>
\[|Val(PA_X)| * |Val(X)|\]
</p>

<p>
I.e. it grows <i>exponentially</i> in the number of parents. This is a
serious problem in many settings. You can also not ask an expert
to express all such CPDs. He will loose patient at some point.
</p>

<p>
So the idea is to find a mechanism to express each and every
\(P(X | PA_X)\) for each X and \(PA_X\) but without doing the
exercise explicitly.
</p>

<p>
I.e. you should find a <i>functional formula CPD = f(X, PA<sub>X</sub>)</i> such
that you can leverage some structures represented by the
functional formula and do not have to express all of the
probabilities individually.
</p>

<p>
You can then read in the book some forms of such deterministic
CPDs. The general idea is quite simple. There might be
deterministic structures that naturally arise due to the
structure of the modeled phenomena.
</p>

<p>
Moreover for deterministic networks you might have the notion of
<code>context specific independence</code>. Here the idea is that given some
particular configuration \(X \cup Y \cup Z\) you might have
independence of X and Y given Z in this particular configuration.
</p>
</div>
</li>


<li><a id="org68f071d"></a>Context Specific CPDs for non-deterministic dependecies<br />
<div class="outline-text-5" id="text-org68f071d">
<p>
Structure in CPDs might not just arise in the case of
context-specific CPDs.
</p>

<p>
The idea is that often there is some structure such that for
certain realizations a RV X given some partial assignment to some
subset of parents $ U &sub; PA<sub>X</sub>$ the probability is fully
specified and does not depend on the remaining parents.
</p>

<p>
Two ways to capture such structure is through Tree-CPDs and
rule-based CPDs.
</p>
</div>

<ul class="org-ul">
<li><a id="orgd2fbe6e"></a>Tree-CPDs<br />
<div class="outline-text-6" id="text-orgd2fbe6e">
<p>
This is a very intuitive structure for every human. In fact
trees are used continuously. There is a natural tendencies for
such structures in engineering so nothing new. You saw them 100s
time.
</p>

<p>
However, what is interesting is the example. In fact it is easy
to see that by leveraging the tree structure, i.e. the context
specific structure and the resulting independencies you can
highly reduce the total number of parameters.
</p>

<p>
To understand that think of the following example:
</p>

<img src="../../images/Bildschirmfoto_2021-02-19_um_12.01.18.png" class="center">

<p>
It is immediate then to see that the above highly reduces the
number of parameters.
</p>

<img src="../../images/Bildschirmfoto_2021-02-19_um_12.03.41.png" class="center">


<p>
Notice that when we talk we say that the Tree-CPDs represent the
network context specific information. This is immediate to see
as you do not in fact consider the full structure of the network, but
you already factor out some of the independencies.
</p>

<p>
To see that consider, the following case where you would have
two recommendation letters and are applying for a job. You have
to choose among the two. Then you can represent the case in the
following ways:
</p>

<img src="../../images/Bildschirmfoto_2021-02-20_um_19.08.25.png" class="center">

<p>
It is clear that on the left you work at the network structure
not leveraging context specific information while on (b) you
already start to pack that in.
</p>
</div>
</li>

<li><a id="orgbd1039b"></a>Rule-based<br />
<div class="outline-text-6" id="text-orgbd1039b">
<p>
Another possibility to pack information of the network structure
by leveraging context specific information is via <code>rule-CPDs</code>.
</p>

<p>
They are defined in the following way:
</p>

<img src="../../images/Bildschirmfoto_2021-02-20_um_19.31.28.png" class="center">


<img src="../../images/Bildschirmfoto_2021-02-20_um_19.50.22.png" class="center">

<p>
It follows immediately that it basically consists in sets joint
co-occurrences of RV and assigns probabilities to such cases.
</p>

<p>
With it you can then basically express all sorts of CPDs
structures that are based on some partitioning.
</p>

<p>
It is in fact immediate to see that tree-CPDs can be easily
expressed via rule-based CPDs but the converse is not true.      
</p>
</div>
</li>
</ul>
</li>

<li><a id="orgd38388f"></a>Independence of Causal Influence<br />
<div class="outline-text-5" id="text-orgd38388f">
<p>
Here the idea is the case where you have a set of variables X<sub>i</sub>
influencing Y, such that X<sub>i</sub> can influence Y in an arbitrary
way. I.e. you assume that X<sub>i</sub> can interact with each other in
complex ways making the <b>effect of each combination unrelated to
any other possible combination</b>.
</p>

<p>
Two such models that fulfill such characteristics are
</p>

<ul class="org-ul">
<li>the noisy-or model</li>

<li>the generalized linear models.</li>
</ul>
</div>

<ul class="org-ul">
<li><a id="orga0cb5db"></a>Noisy Or Model<br />
<div class="outline-text-6" id="text-orga0cb5db">
<p>
This is a very simple model. If an event occurs then you have no
100% guarantee that the usual reaction will occur. That is there
is some noise in the model and some side reaction might happen.
</p>

<p>
Think for instance at working hard at work. Then with 90% you
might have a successful project. However, due to some random
factor, say sudden cut of budget or company restructuring, your
project might fail. This is the <i>noisy part</i> and the noisy or
model.
</p>

<p>
This is the general setting. It is then possible to express such
a noisy model through a graphical representation.
</p>

<p>
Think of the following:
</p>

<img src="../../images/bayesNet2.png"  style = "width: 40% !important;">

<p>
It follows then that W<sub>1</sub> expresses the probability of the noisy
factor taking place - i.e. budget restriction. Such that
&lambda;<sub>W</sub> = P(W<sub>1</sub> | W) = 0.9. Where W = work hard and W<sub>1</sub> = normal condition.
Notice now, the case where independently on your hard work the
team mate hard work also affects the result. Then you could be
in a situation as the following
</p>


<div  style ="height: 40%; width: 50%; margin:0 auto;">
   <img src="../../images/bayesNet3.png">
</div>

<p>
Again also the TW hard work induces a probability of success of
95%, i.e. &lambda;_<sub>TW</sub> = P(TW<sub>1</sub> | TW) = 95%, and there is a 5%
prob of failure due to restructuring and budget cut.
</p>

<p>
This is essentially the Noisy-or model. You have a deterministic
or relation influencing the project success - i.e. either your
work or your team members work. You have noise, i.e. despite the
factors you might have project failures due to some
unpredictable conditions - noise. Overall the probability of
success is given by products of lambdas. I.e. if both team work
and individual work multiply both lambdas. If just one, then
take the respective lambda etc.
</p>

<p>
More formally such model is defined as:
</p>

<img src="../../images/Bildschirmfoto_2021-02-21_um_09.48.03.png" class="center">

<p>
Notice that the <i>leak probability</i> was not discussed that
far. It consists of the probability of project success even in
the case that no hard work - for neither myself nor the team
members was put in the project.
</p>

<p>
<b>Note</b> that in such a models the parameters would be represented
by the estimation of the <i>different lambdas</i>.
</p>
</div>
</li>

<li><a id="org25e1961"></a>Generalized Linear Models<br />
<div class="outline-text-6" id="text-org25e1961">
<p>
These are networks where the interaction among the variables is
represented by generalized linear models you saw a couple of
times in your studies.
</p>

<p>
Recall that in generalized linear models you would have a linear
model
</p>

<p>
\[ f(X_1, ..., X_p) = \sum_{i}^{p} w_i * X_i  \]
</p>

<p>
That would represent the load that the parents sets on the
system. Where the load of each individual variable might be
higher or lower and is therefore weighted.
</p>

<p>
Then basically you would transform such a load into a
probability by applying a sensible transformation that could
well reflect the system work. I.e. a very wide used example is
the S-shaped structure that can be modeled via logit or probit
models.
</p>

<p>
You can also start to make inference on what happens
if&#x2026; cases. For instance in the book it is discussed on how, in
the case of a binary model, the log-odd probability changes
w.r.t. a change in one of the independent binary RV. This gives
you an idea of some possible structures and relations that could
occur in such models so that if representative of some real
world situation you can leverage on this.
</p>

<p>
<b>Note</b> that here once the transformation is defined the only
parameters left are the weights/loads entering the linear part
of the model. You should therefore specify these under this
setting.
</p>
</div>
</li>
</ul>
</li>


<li><a id="org36bc0f2"></a>Continuous Variables<br />
<div class="outline-text-5" id="text-org36bc0f2">
<p>
These are not discussed here. Have to move on. The idea is always
the same. You now have some continuous variables, say Y and
X. You would then have for instance a relation governed by a
normal distribution where \(Y \sim N( \beta * X, \sigma^2)\).
</p>

<p>
That would actually be the case when
</p>

<p>
\[ Y = \beta_0 + \sum_{i = 1}^{P} \beta_i X_i + \epsilon \]
</p>

<p>
where &epsilon; is gaussian N(0, &sigma;<sup>2</sup>). So again the usual
stuff.
</p>
</div>
</li>

<li><a id="org4b02539"></a>Hybrid Models<br />
<div class="outline-text-5" id="text-org4b02539">
<p>
Here the basic idea is that you have a network where you have a
mixture of continuous and discrete variables affecting other
variables.
</p>

<p>
Then one possibility to model such hybrid situation is the
following
</p>


<img src="../../images/Bildschirmfoto_2021-02-21_um_10.34.49.png" class="center">

<p>
Notice that such CLG model induces a mixture on the continuous
parents Y. Moreover it does not allow to have <i>discrete
children</i>. Notice moreover that the number of parameters here is
exponential in the number of discrete variables.
</p>

<p>
Another possibility to model hybrid models is via threshold
models, where you would easily go from continuous parents to
discrete children.
</p>

<p>
Notice that these are just very basic possibilities and the idea -
both here and in the book I guess - is to start to make you reason
about how to model such situations. The possibilities are however
uncountable and therefore it is up to you then on a project to
spend some time at the beginning to engineer the entire model and
decide on the setting.
</p>
</div>
</li>
</ul>
</div>


<div id="outline-container-org14eef9f" class="outline-4">
<h4 id="org14eef9f">On Conditional Bayesian Networks</h4>
<div class="outline-text-4" id="text-org14eef9f">
<p>
Recall that no matter the CPD definition resulting from the
network structure before jumping straight into the modeling of the
CPDs for the entire network it might well make sense to consider
to reduce the problem.
</p>

<p>
In some case you might have a general problem that could be split
into submodules. Each submodules would then be generally defined -
say exhaustive - over the entire network if <b>conditioning</b> on some
elements <code>X</code> and upon some output <code>Y</code> it's entire dependency with
the network would be sufficiently specified. All of the other
elements of the sub-module would be <b>encapsulated</b> in between.
</p>

<p>
An example could be for instance the one of expressing the
failures for a PC.
</p>

<p>
Then you might well start with determining the CPDs for each
component given the parents over the entire network. On the other
hand you might consider to decompose the problem, leveraging
<i>conditional Bayesian Network</i>.
</p>


<p>
Consider for instance the <i>hard drive</i>. Although the hard drive
has a rich internal state, the only aspects of its state that
influence objects outside the hard drive are whether it is working
properly and whether it is full. The Temperature input of the hard
drive in a computer is outside the probabilistic model and will be
mapped to the Temperature parent of the Hard-Drive variable in the
computer model.
</p>

<p>
You might then use the following Conditional CPDs to express the
system:
</p>

<img src="../../images/Bildschirmfoto_2021-02-21_um_12.19.59.png" class="center">

<p>
More formally than what previously described, albeit a bit clumsy
ad definition in my opinion:
</p>


<img src="../../images/Bildschirmfoto_2021-02-21_um_12.21.17.png" class="center">
</div>
</div>


<div id="outline-container-org21cf937" class="outline-4">
<h4 id="org21cf937"><span class="todo TODO">TODO</span> Template Based Representations</h4>
<div class="outline-text-4" id="text-org21cf937">
<p>
Skipped and not even read to this stage. Here also temporal dependent models.
</p>
</div>
</div>


<div id="outline-container-org911f16f" class="outline-4">
<h4 id="org911f16f"><span class="todo TODO">TODO</span> Gaussian Network Models</h4>
<div class="outline-text-4" id="text-org911f16f">
<p>
Skipped and not even read to this stage.
</p>
</div>
</div>

<div id="outline-container-orgbd1fbb4" class="outline-4">
<h4 id="orgbd1fbb4"><span class="todo TODO">TODO</span> Exponential Families</h4>
<div class="outline-text-4" id="text-orgbd1fbb4">
<p>
Skipped and not even read to this stage. I guess it is simply the
generalization of gaussian Network Models to the different
exponential family distributions.
</p>
</div>
</div>
</div>


<div id="outline-container-orgcce25cf" class="outline-3">
<h3 id="orgcce25cf">Inference</h3>
<div class="outline-text-3" id="text-orgcce25cf">
<p>
An important exercise for inference is to query
distributions. I.e. as said the task is to compute the probability
of the occurrence of some RV given some evidence <i>E</i>, i.e. a subset
of RVs that is observed.
</p>

<p>
So in general the task is to determine:
</p>

<p>
\[ P (Y | E = e) \]
</p>

<p>
where <code>Y = query variable</code> and <code>E = evidence</code>.
</p>

<p>
Given such definition of probability queries it is possible to
introduce the <b>first type</b> of query: <i>MAP queries</i>.
</p>

<p>
\[ MAP (W| e) = \operatorname*{argmax}_w P (w,e)\]
</p>

<p>
where W = all non-observed RV.
</p>

<blockquote>
<p>
I.e. in MAP queries you are interested in finding the most likely
joint assignment of the non-observed variables given the evidence.
</p>

<p>
If you perform MAP queries for a single RV Y then you are basically
computing a probability query for all of the possible realizations
y and selecting the most probable one.
</p>

<p>
Notice that the joint prob. maximizing the likelihood might well
differ from the individual RV maximizing realization.
</p>
</blockquote>


<p>
A <b>second type of query</b> is: <i>Marginal MAP Query</i>:
</p>

<p>
The idea of this is well explained in the book via example.
</p>

<p>
Imagine you have a class of disease. You want to find the most
likely disease given your evidence. Assume that you observe a
subset of symptoms E = e. You want to find the MAP assignment of
the disease Y.
</p>

<p>
The issue is now that you have non-observed symptoms: Z.
</p>

<p>
If you now have a disease that has just a small number of
associated symptoms with high probability, and you observe such
symptoms, then your MAP query will likely select this realization
as most likely.
</p>

<p>
In reality there might well be a more likely realization - i.e. a
different RV that is associated with a lot of symptoms with small
probability. The result is that when taking that into account and
therefore considering the possible influence of non-observed
symptoms the conclusion might be well different.
</p>

<p>
For this it makes sense to consider <i>marginal MAP</i> that tries in
fact to adjust for the presence of the other <b>non-observed RVs
influencing the outcome</b>.
</p>

<p>
\[ marginal MAP (Y | e) = \operatorname*{argmax}_Y  \sum_{Z}{P (Y,
   Z | e)} \]
</p>
</div>

<div id="outline-container-orga023e84" class="outline-4">
<h4 id="orga023e84"><span class="todo TODO">TODO</span> Exact Inference</h4>
</div>

<div id="outline-container-orge61d3aa" class="outline-4">
<h4 id="orge61d3aa"><span class="todo TODO">TODO</span> Inference as Optimization</h4>
</div>

<div id="outline-container-org86cefe7" class="outline-4">
<h4 id="org86cefe7"><span class="todo TODO">TODO</span> Particle Based Approximate Inference</h4>
<div class="outline-text-4" id="text-org86cefe7">
<p>
These are essentially the methods you saw in stochastic simulation
course. 
</p>
</div>
</div>

<div id="outline-container-org2caf076" class="outline-4">
<h4 id="org2caf076"><span class="todo TODO">TODO</span> Map Inference</h4>
</div>

<div id="outline-container-org8abb673" class="outline-4">
<h4 id="org8abb673"><span class="todo TODO">TODO</span> Inference in Hybrid Models and Temporal Models</h4>
</div>
</div>


<div id="outline-container-orgeeac1d3" class="outline-3">
<h3 id="orgeeac1d3">Learning</h3>
<div class="outline-text-3" id="text-orgeeac1d3">
<p>
I will do now some brief notes on Learning. This will likely be the
matter of my Thesis.
</p>

<p>
It makes sense therefore to focus now on this, given the little
time I have now and as I have to push a bit in order to set things
correctly into the pipeline.
</p>

<p>
Recall that the idea of Learning, is to learn, either (i) the network
structure, or (ii) the parameters of the model or (iii) both, from
the data.
</p>

<p>
In some domains, the amount of knowledge required is just too large
or the expert’s time is too valuable to ask one to set up and
construct all of the network.  In others, there are simply no
experts who have sufficient understanding of the domain.  In many
domains, the properties of the distribution change from one
application site to another or over time, and we cannot expect an
expert to sit and redesign the network every few weeks.
</p>

<p>
For all of these reasons learning model parameters and structure
from the data is particularly important.
</p>

<p>
Formally, we have a distribution P<sup>*</sup> that is induced by a network
M<sup>*</sup> = (K<sup>*</sup>, &theta;<sup>*</sup>). Given a dataset D = (d[1], &#x2026;, d[m]) of M
samples of P<sup>*</sup>. Notice that such data samples are i.i.d. P<sup>*</sup>
distributed. Then given a some model family \(\tilde{M}\) that
defines a probability \(P_{\tilde{M}}\) (or \(\tilde{P}\) when
\(\tilde{M}\) is clear). I.e. we may want to learn only model
parameters for a fixed structure, or some or all of the structure
of the model.
</p>
</div>

<div id="outline-container-org5b3b0aa" class="outline-4">
<h4 id="org5b3b0aa">Goals of Learning</h4>
<div class="outline-text-4" id="text-org5b3b0aa">
<p>
Notice that we want to construct a \(\tilde{M}\) that precisely
represents the distribution P<sup>*</sup>.
</p>

<p>
Because of the limited amount of data and the fact that we might
possibly have to estimate a very high-dimensional distribution it
is clear that in practice we must select an \(\tilde{M}\) that is
just a <b>best</b> approximation of M<sup>*</sup>.
</p>

<p>
To define what a <b>best</b> approximation is, we have to specify the
goals of learning such that we can quantify how well a
distribution approximates.
</p>
</div>


<ul class="org-ul">
<li><a id="orga1434fd"></a>On a Precise Density Estimation<br />
<div class="outline-text-5" id="text-orga1434fd">
<p>
It is clear that if the goal of setting up a bayesian network is
the one of performing <i>inference</i>, then you might want to
<b>estimate the density at best</b> such that your inference will be the
most precise as possible.
</p>

<p>
I.e. you try to construct a model \(\tilde{M}\) such that
\(\tilde{P}\) is "close" to the generating distribution P<sup>*</sup>.
</p>

<p>
In order to measure how close the two densities lie to each other
you can use the <i>relative entropy distance</i>:
</p>

<img src="../../images/Bildschirmfoto_2021-02-22_um_12.01.01.png" class="center" style = "width: 30% !important;">

<p>
Notice however that in the above you implicitly assume that P<sup>*</sup>
is known. Obviously this is not the case in many practical cases
and it is in fact what we aim to achieve.
</p>

<p>
A solution for this is the following:
</p>

<img src="../../images/Bildschirmfoto_2021-02-22_um_12.21.43.png" class="center">

<p>
Continuing the sentence in the above, the -H<sub>p</sub> term is the
negative entropy above and the second is the <i>expected
log-likelihood</i>. It is immediate to see that the second term is
higher, the higher the probability that \(\tilde{M}\) gives to
points, sampled from the true distribution. 
</p>

<p>
As a consequence of that, it holds however that the
log-likelihood as a metric for comparing one learned model to
another, we cannot evaluate a particular \(\tilde{M}\) in how close
it is to the unknown optimum as we have lost in the above the
baseline $E<sub>P<sup>*</sup>(&xi;)</sub>(log (P<sup>*</sup>(&xi;)) - i.e. the first term
that we ignore as not depending on \(\tilde{P}\).
</p>

<p>
Notice moreover that in our discussion we will be interested in
the <i>likelihood</i> of the data given the model M - i.e. on \(l(D :
     M)\). (recall this notation).
</p>

<p>
Another option for comparing how well a model fits a distribution
is through the notion of <i>loss functions</i> \(loss(\xi : M)\). This
measures the loss a model \(M\) makes on a particular data sample,
i.e. on an instance &xi;.
</p>

<p>
Assume that you take loss function is expressed as the <i>negative
log-likelihood</i>, i.e. \(loss(\xi : M) = -
     \sum^{M}_{m=1}log(P(\xi[m]) : M)\).
</p>

<p>
Then it holds for the <i>expected loss</i>:
</p>

<img src="../../images/Bildschirmfoto_2021-02-22_um_12.54.30.png" class="center">
</div>
</li>


<li><a id="orged16bf0"></a>Specific Prediction Tasks<br />
<div class="outline-text-5" id="text-orged16bf0">
<p>
Notice that when assuming that you want to learn the model to
perform probabilistic inference, you implicitly state that your
aim is to make conclusions on the overall distribution <i>P<sup>*</sup></i>.
</p>

<p>
I.e. in such a case you are interested in evaluating the
probability of a full instance &xi;, i.e. the probability of an
occurrence/sample over/of the entire network.
</p>

<p>
In contrast to this setting in many situations we might be
interested in answering a whole range of queries of the form
P(Y | X).
</p>

<p>
For instance in a classification task we might be interested in
selecting an Y given X. We can then work in such a case with a
MAP assignment to Y, i.e.
</p>

<p>
\[h_{\tilde{P}} = \operatorname*{argmax}_y \tilde{P} (y | x)\]
</p>

<p>
We might then act similarly for other cases.
</p>

<p>
We might even use classification errors such as the standard <code>0/1
     loss</code>.
</p>

<p>
Another option is to focus on the general extent to which our
learned model is able to predict data generated from the
distribution.
</p>

<img src="../../images/Bildschirmfoto_2021-02-22_um_14.20.47.png" class="center" style = "width: 30% !important;">

<p>
Notice that it is immediate to see that if we <i>negate</i> the above
we immediately obtain a loss function to compute an empirical
estimate by taking the average relative to a data set.
</p>
</div>
</li>

<li><a id="org7d12f79"></a>Knowledge Discovery<br />
<div class="outline-text-5" id="text-org7d12f79">
<p>
This is another possible goal in comparison to probabilistic
inference. Here the idea is that you want to understand important
properties of the domain by observing P<sup>*</sup>.
</p>

<p>
I.e. what are the direct and indirect dependencies, what
characterizes the nature of the dependencies and so forth.
</p>

<p>
Of course, simpler statistical methods can be used to explore
the data, for example, by highlighting the most significant
correlations between pairs of variables. However, a learned
network model can provide parameters that have direct causal
interpretation and can also reveal much finer structure, for
example, by distinguishing between direct and indirect
dependencies, both of which lead to correlations in the
resulting distribution.
</p>

<p>
Notice that such a task requires a very different approach in
comparison to the prediction task.
</p>

<p>
In this setting, we really do care about reconstructing the
<b>correct model</b> \(M^*\). While before we could well have distorted
reconstructed model \(\tilde{M}\) as long as we would induce a
<b>distribution</b> similar to the one induced by \(M^*\).
</p>

<p>
So in this task we are not interested in some metric stating the
difference in the distributions defined by the models but rather
as a measure of success we should take directly something
representing the distance between \(\tilde{M}\) and \(M^*\).
</p>

<p>
This is however <b>not always achievable</b>. Even, with a large
amounts of data, the true model might not be
<i>identifiable</i>. Recall in fact that for instance the <code>network
     structure</code> itself \(K^*\), might not be well identifiable due to
the I-map discussion of the representation chapter. The best we
can achieve in this sense is to recover an I-equivalent
structure.
</p>

<p>
Such problems are exacerbated when data is limited. It might be
difficult to detect the correlation of two nodes that are in fact
related in the true model and distinguish it from some spurious
correlation in the data. Note that such a limit is less prominent
in a <b>density estimation task</b>. The reasoning is that as if the
correlation does not appear in the data than it is likely to be a
weak one.
</p>

<p>
The relatively high probability of making model identification
errors can be significant if the goal is to discover the correct
structure of the underlying distribution. So here it is important
to make some <b>confidence</b> statement about the inferred
relationship.
</p>

<p>
Thus, in a knowledge discovery application, it is far more
critical to assess the confidence in a prediction, taking into
account the extent to which it can be identified given the
available data and the number of hypotheses that would give rise
to similar observed behavior. On how to deal with it will be
analyzed in the next sections.
</p>
</div>
</li>

<li><a id="org4e4722b"></a>On Learning as an Optimization Task<br />
<div class="outline-text-5" id="text-org4e4722b">
<p>
Notice that in the above sections we defined some numerical
criterion to define the extent to which the distributions are
comparable to each other.
</p>

<p>
Given such numerical measures that we wish to mini- or maximize,
it follows immediately that learning can be generally seen as an
<i>optimization</i> exercise.
</p>

<p>
We have in fact a <i>hypothesis space</i>, that is a set of candidate
models and an objective function that we aim to optimize. So the
learning task essentially amounts to find a high-scoring model
within our model class.
</p>

<p>
In the next section we go a bit deeper and analyze the
ramifications of choosing one objective function over the other.
</p>
</div>


<ul class="org-ul">
<li><a id="org47daed8"></a>Empirical Risk and Overfitting<br />
<div class="outline-text-6" id="text-org47daed8">
<p>
As said one of the objective functions we might have is the
expected loss.
</p>

<p>
I.e. we are interested in minimizing \(E_{\xi \sim
      P^*}[loss(\xi : M)]\).
</p>

<p>
Notice, that as mentioned before as we do not know the
distribution P<sup>*</sup> we work with the empirical distribution
\<sup>P<sub>D</sub></sup>, this is given for an event <i>A</i> as follows:
</p>

<p>
\[\^{P_D}(A) =  \frac{1}{M} \sum_m 1_{\xi{m} \in A} \]
</p>

<p>
i.e. the empirical distribution is the count of instances that
are elements of the event <i>A</i> over all of the instances sampled
M. It is therefore the usual frequency.
</p>

<p>
Notice that as the number of training samples grows the
empirical distribution approaches the true distribution. This
due to the LLN.
</p>

<p>
So as you have not have no knowledge about the true P<sup>*</sup>, you use
\<sup>P<sub>D</sub></sup> as your true distribution and compute the empirical loss
over it.
</p>

<p>
However, note that there are important limits in such
approach. The dimension of Bayesian Networks distribution
increases exponentially in the number of nodes - i.e. especially
when nodes have multiple parents.
</p>

<p>
Consider for instance the following:
</p>

<img src="../../images/Bildschirmfoto_2021-02-26_um_11.03.43.png" class="center">

<p>
Moreover, recall when using the empirical distribution the usual
issue of overfitting. I.e. it might be easy to get very high
accuracy given a possibly large number of parameters. Notice
however that the empirical dist does not have to be 100%
representative for the true distribution as discussed above. So
recall that.
</p>


<p>
Recall the standard <b>bias-variance trade off</b> in this sense. We
are in the following <i>standard statistical dilemma</i>.
</p>

<img src="../../images/Bildschirmfoto_2021-02-26_um_11.21.58.png" class="center">

<p>
So generally we must take care <b>not to allow a too rich class of
possible models</b>.
</p>
</div>
</li>
</ul>
</li>
</ul>
</div>
</div>
</div>
