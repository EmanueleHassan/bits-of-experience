<p>
This posts contain a brief overview of the deployment options for
Python applications. It will give a brief overview on the different
web servers types and the necessity for utilizing a WSGI server when
deploying a Python application.
</p>

<p>
Note that to better make sense of this you should also make sense a
little bit of the different front-end &amp; back-end communication
architectures. A good overview of this is given at <a href="https://build.vsupalov.com/how-backend-and-frontend-communicate/">this link</a>.
</p>

<!-- TEASER_END -->

<div id="outline-container-org4384cad" class="outline-2">
<h2 id="org4384cad">Server types</h2>
<div class="outline-text-2" id="text-org4384cad">
<p>
When deploying an application an important choice is where to run
it. In general there is a ongoing fight among <code>NGINX</code> and <code>APACHE</code>
servers. These are two kind of web-servers that handle your
application and the users requests.
</p>

<p>
On top of it there is the possibility/need to run a WSGI server. This
is in fact an added layer of architecture being responsible for the
communication between the web-server and your python runtime. It is an
interface that was especially developed for running Python
applications and was is necessary as traditional webservers have no
means to run Python applications.
</p>

<br>

<p>
<img src="/images/Bildschirmfoto 2020-03-31 um 17.56.54.png" alt="img-url:/images/Bildschirmfoto 2020-03-31 um 17.56.54.png"/>
</p>

<br>

<p>
Summing up you have in general the following schema:
</p>

<p>
The web-sever, is the server that users connect to. This server works
out how different types of requests should be handled. For instance,
requests to static files (css, images, javascript etc) that are
storted on the server might just be processed directly by Apache or
NGINX, since it does a fantastic (and fast) job of sending static
assets back to the client.
</p>

<p>
Other requests, such as "get me a list of all blog posts" get passed
onto the WSGI server (Gunicorn, uWSGI etc) who in turn do the
required work and send the response to Apache/nginx, who send it back
to the client.
</p>

<br>
</div>
</div>

<div id="outline-container-org33482d1" class="outline-2">
<h2 id="org33482d1">NGINX vs Apache</h2>
<div class="outline-text-2" id="text-org33482d1">
<p>
/Disclaimer: the content of this section is primarily an adjusted
copy-paste of the first source cited in the literature. That article
is great and recommended to anyone interested in the subject./
</p>

<p>
Let's say that  Apache was the gold-standard web-server of the
90s. Recently NGINX was developed to handle the limitations of
Apache; mainly its issue with concurrency.
</p>

<p>
The main issue with Apache is that it was architected to <i>spawn a copy
of itself for each new connection</i>, which was not suitable for
nonlinear scalability of a website. This had the benefit of turning
Apache into a general purpose web server having many different
features, because of the variety of third-party extensions.  Moreover,
one of the greatest benefit is the universal applicability to
practically any kind of web application development. However, the
downside to having such a rich and universal combination of tools in a
single piece of software is less scalability because of increased CPU
and memory usage per connection.
</p>

<p>
Aimed at solving the concurrency problems of Apache, <b>nginx</b> was
written with a different architecture in mind—one which is much more
suitable for nonlinear scalability in both the number of simultaneous
connections and requests per second. nginx is event-based, so it /does
not follow Apache's style of spawning new processes or threads for
each web page request/. The end result is that even as load increases,
memory and CPU usage remain manageable. nginx can now deliver tens of
thousands of concurrent connections on a server with typical hardware.
</p>
</div>


<div id="outline-container-orgf7626ec" class="outline-3">
<h3 id="orgf7626ec">Apache Issue in Greater Detail</h3>
<div class="outline-text-3" id="text-orgf7626ec">
<p>
Traditional process- or thread-based models of handling concurrent
connections involve handling each connection with a separate process
or thread, and blocking on network or input/output
operations. Depending on the application, it can be very inefficient
in terms of memory and CPU consumption. Spawning a separate process or
thread requires preparation of a new runtime environment, /including
allocation of heap and stack memory, and the creation of a new
execution context/. Additional CPU time is also spent creating these
items, which can eventually lead to poor performance due to thread
thrashing on excessive context switching. All of these complications
manifest themselves in older web server architectures like Apache's.
</p>

<blockquote>
<p>
To illustrate the problem, imagine a simple Apache-based web server
which produces a relatively short 100 KB response—a web page with text
or an image. It can be merely a fraction of a second to generate or
retrieve this page, but it takes 10 seconds to transmit it to a client
with a bandwidth of 80 kbps (10 KB/s). Essentially, the web server
would relatively quickly pull 100 KB of content, and then it would be
busy for 10 seconds slowly sending this content to the client before
freeing its connection. Now imagine that you have 1,000 simultaneously
connected clients who have requested similar content. If only 1 MB of
additional memory is allocated per client, it would result in 1000 MB
(about 1 GB) of extra memory devoted to serving just 1000 clients 100
KB of content. In reality, <b>a typical web server based on Apache
commonly allocates more than 1 MB of additional memory per connection</b>,
and regrettably tens of kbps is still often the effective speed of
mobile communications. Although the situation with sending content to
a slow client might be, to some extent, improved by increasing the
size of operating system kernel socket buffers, it's not a general
solution to the problem and can have undesirable side effects.
</p>

<p>
With persistent connections the problem of handling concurrency is
even more pronounced, because to avoid latency associated with
establishing new HTTP connections, clients would stay connected, and
for each connected client there's a certain amount of memory allocated
by the web server.
</p>

<p>
Thus, the web server should be able to scale nonlinearly with the
growing number of simultaneous connections and requests per second.
</p>
</blockquote>

<br>
</div>
</div>
</div>

<div id="outline-container-orgc7c1326" class="outline-2">
<h2 id="orgc7c1326">Final word</h2>
<div class="outline-text-2" id="text-orgc7c1326">
<p>
Given the brief content above it should come as no suprise why you
should choose the nginx as a benchmark when deploying your
applications especially if you expect a high number of concurrent
connections.
</p>

<p>
Many important architectural principles of NGINX were omitted from the
post that is intended just introduce a bit the juice of the talk about
web servers. When deploying many more considerations come into play
and you might want to consider to read the entire first article cited
in the literature here below.
</p>

<br>
</div>
</div>

<div id="outline-container-org5e6de91" class="outline-2">
<h2 id="org5e6de91">Literature</h2>
<div class="outline-text-2" id="text-org5e6de91">
<p>
<a href="https://www.aosabook.org/en/nginx.html">On NGINX vs Apache</a>
</p>

<p>
<a href="https://stackoverflow.com/questions/14187233/rails-what-is-the-use-of-web-servers-apache-nginx-passenger">On WSGI 1</a>
</p>

<p>
<a href="https://www.fullstackpython.com/wsgi-servers.html">On WSGI 2</a>
</p>
</div>
</div>
