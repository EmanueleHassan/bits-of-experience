<br>
<br>

<p>
A strong orchestrator tool that operates above the container level and
allows to manage a cluster to handle containers.
</p>

<p>
It is essentially the tool set that lets you manage containers.
</p>

<p>
Enterprises can use it to manage the life cycle of containerized apps
in a cluster of nodes, which is a collection of worker machines such
as virtual machines (VMs) or physical machines.
</p>

<p>
In general kubernetes try to leverage clusters in order to avoid
having a single point of failure.
</p>

<!-- TEASER_END -->

<p>
Note that in the world of Kubernetes the terminology of the cluster is
slightly different to the one of Big Data.
</p>

<p>
While the virtual machine controlling other VMs is called master VM,
the other are not called <i>workers</i> but rather <i>nodes</i>.
</p>

<div id="outline-container-org306f017" class="outline-2">
<h2 id="org306f017">Theoretical Framework</h2>
<div class="outline-text-2" id="text-org306f017">
</div>
<div id="outline-container-org4b5b620" class="outline-3">
<h3 id="org4b5b620">Architecture</h3>
<div class="outline-text-3" id="text-org4b5b620">
<p>
The kubernetes architecture is rather simple. It consists of three major components:
</p>

<ul class="org-ul">
<li><b>Docker running on a subnet:</b> this is the service used to run
encapsulated container applications. Important is however to notice
that the Docker service must run on a subnet <i>(available to each
node server</i>.</li>

<li><b>Kubelet Service</b>: this communicates with the master components and
receive commands and work.</li>

<li><b>Kube-Proxy Service</b>: this forwards requests to the correct containers,
balances the load and makes sure that the isolated networking
environment is predictable and accessible.</li>
</ul>

<p>
In a more structured way we can say that there are multiple <i>Pods</i>
available carrying multiple containers, managed by the <i>Docker
Engine</i>, within them. These <i>Points of Delivery</i> are modules of
network, compute, storage bundled with the docker application
containers that work together to deliver networking services.
</p>

<p>
The Kube-proxy will then redirect network traffic into one of the
various Pods.
</p>

<p>
Kubelet is an agent responsible to communicate with the master what
happens at node level. For this reason you will find a Kubelet for each
node. If a Pod goes down than it is for instance responsibility of the
Kubelet to communicate this so the master. This will then be
responsible for restarting the Pod.
</p>

<p>
A good understanding of the general infrastructure can be gained by
looking at the following three videos
</p>

 <iframe width="800" height="400"
 src="https://www.youtube.com/embed/DZ-Wv3XNoAk">
 </iframe>

<br>
<br>

 <iframe width="800" height="400"
 src="https://www.youtube.com/embed/xhwi3zIVR-8">
 </iframe>

<br>
<br>

 <iframe width="800" height="400"
 src="https://www.youtube.com/embed/DZ-Wv3XNoAk">
 </iframe>
</div>
</div>


<div id="outline-container-org4493503" class="outline-3">
<h3 id="org4493503">Master</h3>
<div class="outline-text-3" id="text-org4493503">
<p>
Masters are responsible for an entire cluster, they are the one that
makes the decision on which node to schedule an application.
</p>

<p>
The master has multiple components, all under the umbrella of a single
<i>control plane</i>.
</p>

<p>
The main component is the API server. API server manages scheduling
that goes through the REST services. 
</p>

<p>
Three other important components are:
</p>

<ul class="org-ul">
<li>the <i>Scheduler</i>, that interacts with the API server scheduling
piece but can as well interact directly with REST services. It
determines where to host new Pods in the cluster. This furthermore
coordinates with the Kubelet in order to determine the location of
the Pods based on the load.</li>

<li>the <i>Replication Controllers</i>: these handle the replicas through the
API services.</li>

<li>the <i>etcd</i>: manages state. It is the database of kubernetes. It
tells kubernetes who is available and what the state of all these
available things is.</li>
</ul>
</div>


<div id="outline-container-org7dbdb8e" class="outline-4">
<h4 id="org7dbdb8e">Operation</h4>
<div class="outline-text-4" id="text-org7dbdb8e">
<p>
The question that arise now is how do we specify the Master the tasks to be scheduled?
</p>

<p>
The answer to the question is a <i>YAML</i> file. Here the desired state
for our application is specified and once fed to the Master the latter
will make sure the state is guaranteed.
</p>
</div>
</div>
</div>



<div id="outline-container-org641e27e" class="outline-3">
<h3 id="org641e27e">Helm</h3>
<div class="outline-text-3" id="text-org641e27e">
<p>
Helm, the Kubernetes native package management system, is used for
application management inside an IBM Cloud Private cluster. The Helm
GitHub community curates and continuously expands a set of tested and
preconfigured Kubernetes applications. Clients use the management
console to select stable applications from a catalog and add them to
their cluster.
</p>
</div>
</div>



<div id="outline-container-org10158b0" class="outline-3">
<h3 id="org10158b0">Image Registry</h3>
<div class="outline-text-3" id="text-org10158b0">
<p>
Here is where you save your images. Registries can be either public
or private. Once you push an image to a registry is can then be
pulled by a kubernetes cluster needing the image or in general any
possible client in need for the image.
</p>

<p>
Usually the naming convention when creating a new image is as
follows:
</p>

<div class="highlight"><pre><span></span>   &lt;hostname&gt;/&lt;repository&gt;:&lt;image tag&gt;
</pre></div>

<p>
Notice that here the hostname is the hostname of the image
registry, for instance <code>docker.io</code>.
</p>
</div>
</div>
</div>


<div id="outline-container-org1cd32d6" class="outline-2">
<h2 id="org1cd32d6">Commands</h2>
<div class="outline-text-2" id="text-org1cd32d6">
<pre class="example" id="org90fccd1">
## to view the cluster info
$ kubectl cluster-info  

## to look at the different available nodes and get info about their status and roles.
$ kubectl get nodes

## to get the specifications on where all of your clusters are running.
$ kubectl config get-contexts
</pre>

<p>
Once you have a running Kubernetes cluster, you can deploy your
containerized applications on top of it. 
</p>

<p>
To do so, you create a Kubernetes Deployment configuration. The
Deployment instructs Kubernetes how to create and update instances of
your application. Once you've created a Deployment, the Kubernetes
master schedules mentioned application instances onto individual Nodes
in the cluster.
</p>

<p>
<i>Step 1: Create a Deployment</i>
</p>

<p>
As a first step it is necessary to create the necessary deployment by
providing a name for the deployment and pulling the docker image of
interest you want to run on top of it.
</p>

<pre class="example" id="org20df5d4">
$ kubectl create deployment kubernetes-bootcamp --image=gcr.io/google-samples/kubernetes-bootcamp:v1
</pre>

<p>
You can then double check the successful creating of the deployment by running:
</p>

<pre class="example" id="org3b68a16">
$ kubectl get deployments
</pre>

<p>
Notice the following outputs:
</p>

<ul class="org-ul">
<li>The READY column shows the ratio of CURRENT to DESIRED replicas</li>

<li>CURRENT is the number of replicas running now</li>

<li>DESIRED is the configured number of replicas</li>

<li>The UP-TO-DATE is the number of replicas that were updated to match the desired (configured) state</li>

<li>The AVAILABLE state shows how many replicas are actually AVAILABLE to the users</li>
</ul>

<p>
Once the application is deployed it is possible to interact with it
through an kubernetes API endpoint.
</p>

<p>
This is necessary as pods that are running inside Kubernetes are
running on a private, <i>isolated network</i>. By default they are visible
from other pods and services within the same kubernetes cluster, but
not outside that network. The API overcomes such a barrier and allows
to communicate with the application system wide.
</p>

<p>
The kubectl command can create a proxy that will forward
communications into the cluster-wide, private network.  
</p>

<p>
We now have then <i>a connection between our host</i> and the Kubernetes
cluster. The proxy enables <i>direct access</i> to the API from the host
(the <b>terminal</b> in our case).  In order to do so it is possible to run
</p>

<div class="highlight"><pre><span></span>$ kubectl proxy
</pre></div>

<p>
Once the proxy is running it is possible to get the name of the host
that have access to the API by running a <code>curl</code> command as
</p>

<div class="highlight"><pre><span></span>$ curl http://localhost:8001/version
</pre></div>

<p>
Important is moreover to understand that the API server will then
automatically create an endpoint for each <i>Pod</i> so that it is possible
to communicate directly through the specific <i>Pod</i> through your proxy.
</p>

<p>
You can get a list of running Pods by running the following command
</p>

<div class="highlight"><pre><span></span>$ kubectl get pods
</pre></div>

<p>
To save the pod name you can run
</p>

<div class="highlight"><pre><span></span>$ export POD_NAME=$(kubectl get pods -o go-template --template &#39;{{range .items}}{{.metadata.name}}{{&quot;\n&quot;}}{{end}}&#39;)
</pre></div>

<p>
Once the name of the pod is there you can interact with and access the
app deployed on it by leveraging your proxy.
</p>

<p>
You can then make a <code>curl</code> query of the form
</p>

<div class="highlight"><pre><span></span>## where the $POD_NAME has to be previously saved as above.
curl http://localhost:8001/api/v1/namespaces/default/pods/$POD_NAME/proxy/
</pre></div>

<p>
Recall that a a Pod models an application-specific "logical host" and
can contain different application containers which are relatively
tightly coupled. For example, a Pod might include both the container
with your Node.js app as well as a different container that feeds the
data to be published by the Node.js webserver. The containers in a Pod
share an IP Address and port space, are always co-located and
co-scheduled, and run in a shared context on the same Node.
</p>

<p>
Pods are the <b>atomic unit</b> on the Kubernetes platform. When we create a
Deployment on Kubernetes, that Deployment creates Pods with containers
inside them (as opposed to creating containers directly). Each Pod is
tied to the Node where it is scheduled, and remains there until
termination (according to restart policy) or deletion. In case of a
Node failure, identical Pods are scheduled on other available Nodes in
the cluster.
</p>

<p>
You can moreover get more specific information about each running pod by running
</p>

<div class="highlight"><pre><span></span>$ kubectl describe pod
</pre></div>

<p>
Notice moreover that it is possible to see standard output of an
application running within a pod by inspecting the logging record
within that Pod.  <i>This is possible as anything that the application would normally send to STDOUT becomes logs for the container within
the Pod. We can retrieve these logs using the kubectl logs command</i>
</p>

<p>
You can inspect such by running 
</p>

<div class="highlight"><pre><span></span>$ kubectl logs
</pre></div>
</div>

<div id="outline-container-org41b57fd" class="outline-3">
<h3 id="org41b57fd">Contexts</h3>
<div class="outline-text-3" id="text-org41b57fd">
<p>
As mentioned above context specify with which cluster and on which
namespace you will work.
</p>

<p>
I worked with multiple clusters throughout my experience. At some
point I needed to make order. I had too many clusters and contexts
I did not use anymore in my <code>config</code> file governing the kubectl
specifications.
</p>

<p>
In order to unset clusters and contexts use the following
</p>

<div class="highlight"><pre><span></span>   <span class="c1">## to view the contexts</span>
   kubectl config get-contexts

   <span class="c1">## to view the clusters </span>
   kubectl config get-contexts

   <span class="c1">## to remove contexts and clusters</span>
   kubectl config <span class="nb">unset</span> contexts.&lt;name&gt;
   kubectl config <span class="nb">unset</span> clusters.&lt;name&gt;
</pre></div>
</div>
</div>

<div id="outline-container-orgbe23adf" class="outline-3">
<h3 id="orgbe23adf">Direct operating within a <i>Pod</i></h3>
<div class="outline-text-3" id="text-orgbe23adf">
<p>
Notice that as a Pod is used as a running isolated environment within
a node it is possible to directly operate on it.
</p>

<p>
This is possible through the API given the name of a specific running Pod. 
</p>

<p>
For instance to run a bash session on the Pod it is possible to run:
</p>

<div class="highlight"><pre><span></span>$ kubectl exec -ti $POD_NAME bash
</pre></div>

<p>
This will open a proper bash session within the specified Pod.
</p>

<p>
We can execute commands directly on the container once the Pod is up
and running. For this, we use the exec command and use the name of the
Pod as a parameter. Let’s list the environment variables
</p>
</div>
</div>

<div id="outline-container-orge8d002a" class="outline-3">
<h3 id="orge8d002a">On Pod Lifecycle</h3>
<div class="outline-text-3" id="text-orge8d002a">
<p>
Up to know we have addressed the issues of accessing Pod locally from
the kubernetes CLI.
</p>

<p>
When we want to expose an application to the <i>"outside world"</i> we need
however to be careful. In order to see that think of the kubernetes
lifecycle.
</p>

<p>
Kubernetes Pods are mortal. Pods in fact have a lifecycle. When a
worker node dies, the Pods running on the Node are also lost. A
ReplicaSet might then dynamically drive the cluster back to desired
state via creation of new Pods to keep your application running.
</p>

<p>
This is in fact the strength of Kubernetes as an orchestration tool.
</p>

<p>
Consider now an image-processing backend with 3 replicas. Those
replicas are exchangeable; the front-end system should not care about
backend replicas or even if a Pod is lost and recreated. That said,
<i>each Pod in a Kubernetes cluster has a unique IP address</i>, even Pods on
the same Node, so there needs to be a way of automatically reconciling
changes among Pods so that your applications continue to function.
</p>

<p>
A Service in Kubernetes is an abstraction which defines a logical set
of Pods and a policy by which to access them. Services enable a loose
coupling between dependent Pods. A Service is defined using YAML
(preferred) or JSON, like all Kubernetes objects.
</p>

<p>
<i>Example of a YAML</i>:
</p>

<p>
Although each Pod has a unique IP address, those IPs are not exposed
outside the cluster without a Service. Services allow your
applications to receive traffic. Services can be exposed in different
ways by specifying a <code>type</code> in the ServiceSpec:
</p>


<ul class="org-ul">
<li>ClusterIP (default) - Exposes the Service on an internal IP in the
cluster. This type makes the Service only reachable from within the
cluster.</li>

<li>NodePort - Exposes the Service on the same port of each selected
Node in the cluster using NAT. Makes a Service accessible from
outside the cluster using <code>&lt;NodeIP&gt;:&lt;NodePort&gt;</code>. Superset of
ClusterIP.</li>

<li>LoadBalancer - Creates an external load balancer in the current
cloud (if supported) and assigns a <b>fixed, external IP to the
Service.</b> Superset of NodePort.</li>

<li>ExternalName - Exposes the Service using an arbitrary name
(specified by externalName in the spec) by returning a CNAME record
with the name. No proxy is used. This type requires v1.7 or higher
of <code>kube-dns</code>.</li>
</ul>

<p>
Moreover note that services are the abstraction that allow pods to die and
replicate in Kubernetes without impacting your application. Discovery
and routing among dependent Pods (such as the frontend and backend
components in an application) is handled by Kubernetes Services.
</p>

<p>
Services match a set of Pods using labels and selectors, a grouping
primitive that allows logical operation on objects in
Kubernetes. Labels are key/value pairs attached to objects and can be
used in any number of ways:
</p>

<ul class="org-ul">
<li>Designate objects for development, test, and production</li>
<li>Embed version tags</li>
<li>Classify an object using tags</li>
</ul>

<p>
To list the services running on your cluster you can run
</p>

<div class="highlight"><pre><span></span>$ kubectl get services
</pre></div>

<p>
To create a new service and expose it to the external traffic we'll
use the expose command with NodePort as parameter. We choose this
option as the tutorial runs through minikube and this does not support
the <i>Loadbalancer</i> option yet. Notice, however that in case this is
available it is recommended working through such an option.
</p>

<div class="highlight"><pre><span></span>$ kubectl expose deployment/kubernetes-bootcamp --type=&quot;NodePort&quot; --port 8080
</pre></div>

<p>
You can verify that your Service was properly exposed by controlling
with the <code>$ kubectl get services</code> option above.
</p>

<p>
Once the service is deployed with the internal assigned port as
specified above an external port is assigned to the service. You can
get this by inspecting the service specific characteristics
</p>

<div class="highlight"><pre><span></span>## kubectl describe services/&lt;service name&gt;
$ kubectl describe services/kubernetes-bootcamp 
</pre></div>

<p>
And you can save it in your environment through 
</p>

<div class="highlight"><pre><span></span>$ export NODE_PORT=$(kubectl get services/kubernetes-bootcamp -o go-template=&#39;{{(index .spec.ports 0).nodePort}}&#39;)
</pre></div>

<p>
You can then connect to the kubernetes app by any device by running
</p>

<div class="highlight"><pre><span></span>$ curl $(minikube ip):$NODE_PORT
</pre></div>

<p>
Notice, moreover that the deployment created automatically a <i>label</i>
for our Pod. With the <code>$ describe deployment</code> command you can simply
get to it.
</p>

<p>
Given your label (<i>in the tutorial run=kubernetes-bootcamp</i>), you can then access your Pod directly thorough it.
</p>

<p>
For instance the command
</p>

<div class="highlight"><pre><span></span>## kubectl get services -l &lt;label&gt;
$ kubectl get services -l run=kubernetes-bootcamp
</pre></div>

<p>
will get you the services running on pods with the given label.
</p>

<p>
You can add a label to a Pod by running
</p>

<div class="highlight"><pre><span></span>$ kubectl label pod $POD_NAME app=v1
</pre></div>

<p>
This will be then added to the label of the pod and will allow you to
operate on the pod accordingly.
</p>

<p>
You can finally delete an existing service through:
</p>

<div class="highlight"><pre><span></span>## kubectl delete service -l &lt;label&gt;
$ kubectl delete service -l run=kubernetes-bootcamp
</pre></div>

<p>
You will see then that after deleting the service your app deployed on the specific pod will not be externally accessible anymore, i.e.
</p>

<div class="highlight"><pre><span></span>$ curl $(minikube ip):$NODE_PORT
</pre></div>

<p>
will fail. 
</p>

<p>
Nonetheless the app can always be reached internally given the Pod
name and the selected port deployment.
</p>
</div>
</div>

<div id="outline-container-orge0336da" class="outline-3">
<h3 id="orge0336da">App scaling</h3>
<div class="outline-text-3" id="text-orge0336da">
<p>
In the previous modules we created a Deployment, and then exposed it
publicly via a Service. The Deployment created only one Pod for
running our application. When traffic increases, we will need to scale
the application to keep up with user demand.
</p>

<p>
<b>Scaling is accomplished by changing the number of replicas in a
Deployment</b>
</p>

<p>
Scaling out a Deployment will ensure new Pods are created and
scheduled to Nodes with available resources. Scaling will increase the
number of Pods to the new desired state. Kubernetes also supports
autoscaling of Pods, but it is outside of the scope of this
tutorial. Scaling to zero is also possible, and it will terminate all
Pods of the specified Deployment.
</p>

<p>
Running multiple instances of an application will require a way to
distribute the traffic to all of them. Services have an integrated
load-balancer that will distribute network traffic to all Pods of an
exposed Deployment. Services will monitor continuously the running
Pods using endpoints, to ensure the traffic is sent only to available
Pods.
</p>

<p>
To scale an deployment it is then possible to scale it specifying the
desired number of replicas by running
</p>

<div class="highlight"><pre><span></span>$ kubectl scale deployments/&lt;deployment name&gt; --replicas=4
</pre></div>

<p>
You can then verify that the number of pods have been updated by
running the <code>$ kubectl get pods</code> command. 
</p>

<p>
Notice that different pods have different internal IP addresses. You
can read them through the <code>$ kubectl describe services
services/&lt;service name&gt;</code> command.
</p>
</div>
</div>

<div id="outline-container-org5fb4ffa" class="outline-3">
<h3 id="org5fb4ffa">New releases</h3>
<div class="outline-text-3" id="text-org5fb4ffa">
<p>
Users expect applications to be available all the time and developers
are expected to deploy new versions of them several times a day. In
Kubernetes this is done with rolling updates. Rolling updates allow
Deployments' update to take place <b>with zero downtime</b> by incrementally
updating Pods instances with new ones. The new Pods will be scheduled
on Nodes with available resources.
</p>

<p>
By default, the maximum number of Pods that can be unavailable during
the update and the maximum number of new Pods that can be created, is
one. Both options can be configured to either numbers or percentages
(of Pods). In Kubernetes, updates are versioned and any Deployment
update can be reverted to previous (stable) version.
</p>

<p>
Similar to application Scaling, if a Deployment is exposed publicly,
the Service will load-balance the traffic only to available Pods
during the update. An available Pod is an instance that is available
to the users of the application.
</p>
</div>
</div>

<div id="outline-container-orgd0c5139" class="outline-3">
<h3 id="orgd0c5139">Cluster Exposure to Public Internet</h3>
<div class="outline-text-3" id="text-orgd0c5139">
<p>
Since there are three replicas of this application deployed in the
cluster, Kubernetes will load balance requests across these three
instances. 
</p>

<p>
Let's expose our application to the internet and see how Kubernetes
load balances requests.
</p>

<p>
In order to access the application, we have to expose it to the
internet using a Kubernetes Service.  
</p>

<div class="highlight"><pre><span></span>kubectl expose deployment/hello-world --type<span class="o">=</span>NodePort --port<span class="o">=</span><span class="m">8080</span> --name<span class="o">=</span>hello-world --target-port<span class="o">=</span><span class="m">8080</span>
</pre></div>

<p>
This command creates what is called a <b>NodePort Service</b>. This will
open up a port on the worker node to allow the application to be
accessed the application using that port and the IP address of the node.
</p>

<p>
List Services in order to see that this service was created.
</p>

<div class="highlight"><pre><span></span>kubectl get services
</pre></div>

<p>
Two things are needed to access this application: a worker node IP
address and the correct port. 
</p>

<p>
To get a worker node IP, rerun the get pods command with the wide
option and note any one of the node IP addresses (from the column
entitled NODE): 
</p>

<div class="highlight"><pre><span></span>kubectl get pods -o wide 
</pre></div>

<p>
Here is some sample output.
</p>

<pre class="example" id="orgb4f299e">
NAME                          READY   STATUS    RESTARTS   AGE   IP               NODE            NOMINATED NODE   READINESS GATES
hello-world-dd6b5d745-f9xjk   1/1     Running   0          7m    172.30.104.185   10.114.85.153   &lt;none&gt;           &lt;none&gt;
hello-world-dd6b5d745-m89fc   1/1     Running   0          7m    172.30.165.182   10.114.85.151   &lt;none&gt;           &lt;none&gt;
hello-world-dd6b5d745-qvs9t   1/1     Running   0          7m    172.30.69.68     10.114.85.172   &lt;none&gt;           &lt;none&gt;
Using this sample output, you could choose 10.114.85.153, 10.114.85.151, or 10.114.85.172 for the node IP address.
</pre>

<p>
Export the node IP address as an environment variable. 
Make sure to substitute the copied IP address into this command.
</p>

<div class="highlight"><pre><span></span><span class="c1"># export NODE_IP=&lt;node_ip&gt;</span>
<span class="c1"># Using the sample output, one correct command would be </span>
<span class="nb">export</span> <span class="nv">NODE_IP</span><span class="o">=</span><span class="m">10</span>.114.85.153.
</pre></div>

<p>
To get the port number, run the following command and note the port:
</p>

<div class="highlight"><pre><span></span>kubectl get services
</pre></div>

<p>
Here is some sample output. In this sample, the port number we need is 31758.
</p>

<pre class="example" id="org911e886">
NAME          TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
hello-world   NodePort   172.21.121.84   &lt;none&gt;        8080:31758/TCP   58s
</pre>

<p>
Export the port as an environment variable. Make sure to substitute
the copied port into this command.
</p>

<div class="highlight"><pre><span></span><span class="c1">## export NODE_PORT=&lt;node_port&gt;</span>
<span class="c1">## Using the sample output, the correct command would be export NODE_PORT=31758</span>
<span class="nb">export</span> <span class="nv">NODE_PORT</span><span class="o">=</span><span class="m">31758</span>
</pre></div>


<p>
Ping the application to get a response.
</p>

<div class="highlight"><pre><span></span>curl <span class="nv">$NODE_IP</span>:<span class="nv">$NODE_PORT</span>
</pre></div>

<p>
Notice that this output includes the Pod name. Run the command ten
times and note the <b>different Pod names</b> in each line of output.  
</p>

<div class="highlight"><pre><span></span><span class="k">for</span> i <span class="k">in</span> <span class="sb">`</span>seq <span class="m">10</span><span class="sb">`</span><span class="p">;</span> <span class="k">do</span> curl <span class="nv">$NODE_IP</span>:<span class="nv">$NODE_PORT</span><span class="p">;</span> <span class="k">done</span>
</pre></div>

<p>
You should see more than one Pod name, and quite possibly all three
Pod names, in the output. This is because Kubernetes load balances the
requests across the three replicas, so each request could hit a
different instance of our application.
</p>
</div>
</div>

<div id="outline-container-orgba2502f" class="outline-3">
<h3 id="orgba2502f">App Scaling, new releases, cluster configuration - Implementation</h3>
<div class="outline-text-3" id="text-orgba2502f">
</div>
<div id="outline-container-org41521dd" class="outline-4">
<h4 id="org41521dd">Replicasets</h4>
<div class="outline-text-4" id="text-org41521dd">
<p>
It manages your pods ensuring the right number of pods are always up
and running. This can mean adding or deleting pods as
needed. ReplicaSets provide the ability to replicate pods and restart
or spin up new pods when existing ones fail.
</p>

<p>
A ReplicaSet can pick an existing pod to add to the deployment or
create a new one if there are no existing pods. It does so by asking
for a list of pods from the Kubernetes API and then filtering on the
labels, as defined in the descriptor.
</p>

<p>
The ReplicaSet does not own any of the pods; instead it uses the pod
labels to decide which pods to acquire when bringing a deployment to
the desired state. The template metadata inside the YAML spec defines
the labels of potential pod candidates to add or delete.
</p>

<p>
An example of a ReplicaSet YAML would look as follows.
</p>

<div class="highlight"><pre><span></span>apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: frontend
  labels: ## here you specify the labels on which to match your
	  ## Replica and apply the defined replica conditions
    app: guestbook
    tier: frontend
spec:     ## here you specify the number of replicas you desire
  replicas: 3
  selector:
    matchLabels:
      tier: frontend
  template:   ## here your pass your image and label it accordingls.
    metadata:
      labels:
	tier: frontend
    spec:
      containers:
      - name: php-redis
	image: gcr.io/google_samples/gb-frontend:v3
</pre></div>

<p>
Notice that Replicasets are automatically created when creating
<b>Deployments</b>. You can get a list of running Replicasets in your
cluster for the specific namespace you are working in by using:
</p>

<div class="highlight"><pre><span></span>kubectl get replicaset
kubectl get rs <span class="c1">## also accepted. alias.</span>
</pre></div>

<p>
Once a replicaset is running you can change the number of replicas in
an imperative way running
</p>

<div class="highlight"><pre><span></span>kubectl scale deploy &lt;deployment-name&gt; --replicas<span class="o">=</span><span class="m">4</span>
</pre></div>
</div>
</div>

<div id="outline-container-org375a6b4" class="outline-4">
<h4 id="org375a6b4">Autoscaling</h4>
<div class="outline-text-4" id="text-org375a6b4">
<p>
Horizontal Pod Autoscaler, or HPA, enables the application to increase
the number of pods based on traffic.
</p>

<p>
You can configure the desired state in HPA (for example, the CPU and
memory).  
</p>

<p>
The master node will periodically check pod metrics and
scale to meet the desired state by updating the replicas field of the
scaled resource, such as ReplicaSets or deployment.
</p>

<p>
For instance consider the following deployment
</p>

<div class="highlight"><pre><span></span>apiVersion: apps/v1
kind: Deployment
metadata:
  name: php-apache
spec:
  selector:
    matchLabels:
      run: php-apache
  replicas: 1
  template:
    metadata:
      labels:
	run: php-apache
    spec:
      containers:
      - name: php-apache
	image: k8s.gcr.io/hpa-example
	ports:
	- containerPort: 80
	resources:
	  limits:
	    cpu: 500m
	  requests:
	    cpu: 200m
</pre></div>

<p>
You can then create a Horizonatal Pod Autoscaler imperatively using
</p>

<div class="highlight"><pre><span></span>kubectl autoscale deployment php-apache --cpu-percent<span class="o">=</span><span class="m">50</span> --min<span class="o">=</span><span class="m">1</span> --max<span class="o">=</span><span class="m">10</span>  
</pre></div>

<p>
The following command will create a Horizontal Pod Autoscaler that
maintains between 1 and 10 replicas of the Pods controlled by the
php-apache deployment we created in the first step of these
instructions. Roughly speaking, HPA will increase and decrease the
number of replicas (via the deployment) to maintain an average CPU
utilization across all Pods of 50%
</p>

<p>
The other way, working in a declarative way, would look as follows
</p>

<div class="highlight"><pre><span></span>apiVersion: autoscaling/v2beta2
kind: HorizontalPodAutoscaler
metadata:
  name: php-apache
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: php-apache
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
	type: Utilization
	averageUtilization: 50
</pre></div>
</div>
</div>



<div id="outline-container-org7605bdd" class="outline-4">
<h4 id="org7605bdd">Rolling Updates</h4>
<div class="outline-text-4" id="text-org7605bdd">
<p>
Rolling updates provide a way to roll out app changes in an automated
and controlled fashion throughout your pods.  Rolling updates work
with pod templates such as deployments.  Rolling updates allow for
rollback if something goes wrong.
</p>

<p>
An example for a Rolling update would be as follows
</p>

<div class="highlight"><pre><span></span>apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: linuxsys-deploy
  name: linuxsys-deploy
spec:
  replicas: 10
  selector:
    matchLabels:
      app: linuxsys-deploy
  minReadySeconds: 5 ## wait 5 seconds before moving on to the next
		     ## pod in the rollout stage.
  strategy: 
       type: RollingUpdate
       rollingUpdate:
	 maxSurge: 25% ## there can just be at maximum 1.25 *
		       ## &lt;specified number replicas&gt; replicas while
		       ## performing the rollout.
	 maxUnavailable: 25% ## 75% of the pods must always be available.
  template:
    metadata:
      labels:
	app: linuxsys-deploy
    spec:
      containers:
      - image: nginx:1.17-perl
	name: nginx
	ports:
	- containerPort: 80
</pre></div>
</div>
</div>


<div id="outline-container-org2fd67f8" class="outline-4">
<h4 id="org2fd67f8">Config Maps and Secrets</h4>
<div class="outline-text-4" id="text-org2fd67f8">
<p>
As software developers, we know it’s a good practice not to hard-code
configuration variables in code.
</p>

<p>
We keep them separate so that any changes in configuration do not
require code changes.  
</p>

<p>
Examples of these variables can include non-sensitive information like
environments (for example, dev, test, and prod) or sensitive
information such as API keys and account IDs.
</p>

<p>
ConfigMaps give us a way to provide configuration data to pods and
deployments so we don't have to hard-code that data in the application
code.
</p>

<p>
You can also reuse these ConfigMaps and Secrets for multiple
deployments, thereby decoupling the environment from the deployments
themselves!
</p>

<p>
Secrets work similarly to ConfigMaps but are meant for sensitive
information.
</p>

<p>
An example to do that is the following
</p>

<div class="highlight"><pre><span></span>apiVersion: apps/v1
kind: Deployment
metadata:
  name: hello-world
spec:
  selector:
    matchLabels:
      run: hello-world
  template:
    metadata:
      labels:
	run: hello-world
    spec:
      containers:
      - name: hello-world
	image: us.icr.io/sn-labs-marcohassan/hello-world:3
	ports:
	- containerPort: 8080
	envFrom:
	- configMapRef:
	    name: app-config ## here you specify that environment
			     ## variables should be specified in a
			     ## ConfigMap named app-config
      imagePullSecrets:
	- name: icr
</pre></div>

<p>
Then you can have an app with the following code
</p>

<div class="highlight"><pre><span></span>var express = require(&#39;express&#39;)
var os = require(&quot;os&quot;);
var hostname = os.hostname();
var app = express()

app.get(&#39;/&#39;, function(req, res) {
    res.send(process.env.MESSAGE + &#39;\n&#39;)    
})
app.listen(8080, function() {
  console.log(&#39;Sample app is listening on port 8080.&#39;)
})
</pre></div>

<p>
Notice now that the MESSAGE variable should come from the app-config
Configmap specified above.
</p>

<p>
You can then specify it with the following command
</p>

<div class="highlight"><pre><span></span>kubectl create configmap app-config --from-literal<span class="o">=</span><span class="nv">MESSAGE</span><span class="o">=</span><span class="s2">&quot;This message came from a ConfigMap!&quot;</span>
</pre></div>

<p>
Now after applying the deployment
</p>

<div class="highlight"><pre><span></span>kubectl apply -f deployment-configmap-env-var.yaml
</pre></div>

<p>
And deploying the new image with the new app code
</p>

<div class="highlight"><pre><span></span>docker build -t us.icr.io/<span class="nv">$MY_NAMESPACE</span>/hello-world:3 . <span class="o">&amp;&amp;</span> docker push us.icr.io/<span class="nv">$MY_NAMESPACE</span>/hello-world:3
</pre></div>

<p>
You would get the configmap text when accessing your application
running on the cluster.
</p>
</div>
</div>
</div>

<div id="outline-container-org8921185" class="outline-3">
<h3 id="org8921185">Literature</h3>
<div class="outline-text-3" id="text-org8921185">
<p>
IBM - Journey to Cloud Series.
</p>

<p>
<a href="https://kubernetes.io/docs/tutorials/kubernetes-basics/create-cluster/cluster-interactive/">Kubernetes tutorial</a>
</p>
</div>
</div>
</div>
