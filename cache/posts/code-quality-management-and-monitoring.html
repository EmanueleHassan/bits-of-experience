<style>
img {
display: block;
margin-top: 60px;
margin-bottom: 60px;
margin-left: auto;
margin-right: auto;
width: 70%;
height: 100%;
class: center;
}

.container {
  position: relative;
  left: 15%;
  margin-top: 60px;
  margin-bottom: 60px;
  width: 70%;
  overflow: hidden;
  padding-top: 56.25%; /* 16:9 Aspect Ratio */
  display:block;
  overflow-y: hidden;
}

.responsive-iframe {
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  right: 0;
  width: 100%;
  height: 100%;
  border: none;
  display:block;
  overflow-y: hidden;
}
</style>

<p>
So basically this post contains a way to statically analyze your code
in order to see the quality of it.
</p>

<p>
This is important in order find blind spots in your code. Moreover, it
is important to keep track of the history of the quality of the
software in time. So it is a very important piece of the puzzle. 
</p>

<!-- TEASER_END -->

<p>
Understand the following:
</p>

<blockquote>
<p>
In the Java space, you can choose from a wide range of open source and
commercial solutions, such as Checkstyle, PMD, Cobertura, FindBugs,
and Sonar.  Many of these tools are already available in the form of
Gradle core or third-party plugins and can be seamlessly integrated
into your build.
</p>
</blockquote>

<p>
So a good idea to this point is to include these metrics and
include them in git. Just to keep track about the quality of your
code as a time series.
</p>

<p>
In general such tools produce metrics along the following
categories:
</p>

<ol class="org-ol">
<li>Code Coverage</li>

<li>Adherence to coding standards</li>

<li>Bad coding practices and design problems</li>

<li>Overly complex, duplicated, and strongly coupled code</li>
</ol>

<div id="outline-container-orgec9c5ce" class="outline-3">
<h3 id="orgec9c5ce">Important practical aspects</h3>
<div class="outline-text-3" id="text-orgec9c5ce">
<p>
It is important to understand that you should group different code
quality metrics in different gradle tasks.
</p>

<p>
The idea is that you do not want to execute all of them together
and across the entire codebase. It will take too much time.
</p>

<p>
In this sense, you should rather decouple the tasks and also
understand the opportunity to run them for smaller portions of
your codebase.
</p>

<blockquote>
<p>
For example, during development you may want to know whether you’ve
improved the code coverage of the class you’re currently refactoring
without having to run other lengthy code quality processes.
</p>
</blockquote>
</div>
</div>


<div id="outline-container-orge62d073" class="outline-2">
<h2 id="orge62d073">Code Coverage</h2>
<div class="outline-text-2" id="text-orge62d073">
<p>
Code coverage analysis (also called test coverage analysis) is the
process of finding the areas in your code that are not exercised by
test cases.
</p>

<p>
Empirical studies show that reasonable code coverage has an
indirect impact on the quality of your code.
</p>

<p>
In any case understand the following:
</p>

<blockquote>
<p>
But despite all these benefits, code analysis <b>doesn’t replace a code
review</b> by an experienced peer; rather, it complements it.
</p>
</blockquote>

<p>
A further important question in this space is the type of coverage
metrics that you want to use.
</p>
</div>


<div id="outline-container-orgad336a6" class="outline-3">
<h3 id="orgad336a6">Types of Coverage</h3>
<div class="outline-text-3" id="text-orgad336a6">
<p>
There is in fact more than an option to consider and especially the
following:
</p>

<ul class="org-ul">
<li><i>Branch coverage</i>: Measures which of the possible execution paths
(for example, in if/else branching logic) is executed by tests.</li>

<li><i>Statement coverage</i>: Measures which statements in a code block
have been executed. (The default in pytest-coverage).</li>

<li><i>Method coverage</i>: Measures which of the methods were entered
during test execution</li>

<li><i>Complexity metrics</i>: Measures cyclomatic complexity (the number
of independent paths through a block of code) of packages,
classes, and methods.</li>
</ul>

<p>
Especially interesting to me is the <i>complexity metric</i> as it is a
metric that I want to explore on the codebase I work on as being
quite indicative.
</p>
</div>
</div>

<div id="outline-container-orgb9ed835" class="outline-3">
<h3 id="orgb9ed835">Types of Instrumentation</h3>
<div class="outline-text-3" id="text-orgb9ed835">
<p>
Essentially instrumentation is the way your programs actually
check the coverage.
</p>

<p>
I.e. as per gradle in action:
</p>

<blockquote>
<p>
The job of instrumentation is to inject instructions that are used to
detect whether a particular code line or block is hit during test
execution.
</p>
</blockquote>

<p>
Understand that there are now three types of instrumentation
possibilities in Java:
</p>

<ol class="org-ol">
<li><b>Source code</b> instrumentation adds instructions to the source
code before compiling it to trace which part of the code has
been executed.</li>

<li><b>Offline bytecode</b> instrumentation applies these instructions
directly to the compiled bytecode.</li>

<li><b>On-the-fly</b> instrumentation adds these same instructions to the
bytecode, but does this when it’s loaded by the JVM’s class
loader.</li>
</ol>

<p>
Note now, that obviously the last one is the better option, due to
the following straightforward reasoning:
</p>

<blockquote>
<p>
In the context of a continuous delivery pipeline, where bundling the
deliverable is done after executing the code analysis phase, you want
to make sure that the source or bytecode isn’t modified after the
compilation process to avoid unexpected behavior in the target
environment.  Therefore, on-the-fly instrumentation should be
preferred.
</p>
</blockquote>

<p>
Note that in other cases there are ways to avoid instrumentation
problems. However, this requires more thought and I am not
treating it here as for the moment I am going with an on the fly
implementation software.
</p>
</div>
</div>

<div id="outline-container-orge77de0d" class="outline-3">
<h3 id="orge77de0d">Software Decision</h3>
<div class="outline-text-3" id="text-orge77de0d">
<p>
I will use <i>JaCoCo</i> for running coverage and this is on the fly
implementation such that this problem is solved in the first
place. Note that JaCoCo produces <b>line and branch coverage</b>
metrics. The complexity coverage metric is missing in this sense.
</p>

<p>
If you want that cyclomatic complexity you should go with
enterprise solutions such as Cobertura. Keep it in the back of
your mind. You are likely going back with the request at some
point.
</p>

<p>
An example of cobertura report would look as follows, you see that
you have a lot information there:
</p>


<img src="../../images/Screenshot 2022-09-21 112958.png" class="center">
</div>
</div>
</div>


<div id="outline-container-orgcf1987e" class="outline-2">
<h2 id="orgcf1987e">Static Code Analysis</h2>
<div class="outline-text-2" id="text-orgcf1987e">
<p>
Recall the intro.
</p>

<p>
Basically these software covers the other points 2-4.
</p>

<p>
An overview of the available tools is the following:
</p>

<img src="../../images/Screenshot 2022-09-21 115628.png" class="center">
</div>
</div>


<div id="outline-container-org9e4c7e6" class="outline-2">
<h2 id="org9e4c7e6"><span class="todo TODO">TODO</span> understand</h2>
</div>
