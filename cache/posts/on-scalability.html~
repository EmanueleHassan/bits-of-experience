<p>
This post quickly examines two different laws of scalability: Amdahl's
law and Gustafson's law.
</p>

<p>
Both laws addresses the extent to which a problem can be speed up
through scaling out. 
</p>

<p>
The premise of setting under which the laws operate is
different. Amdahl's law assumes a <b>fix problem size</b>, while
Gustafson's law assumes an <b>an expanding problem size in the number of
machines scaling out</b>.
</p>

<!-- TEASER_END -->

<p>
This leads tot the following formula:
</p>

<p>
\[Amdahl's Law Speedup = frac{1}{1-p+p/s}\]
</p>

<p>
\[Gustafson's Law Speedup = 1-p+ps\]
</p>

<p>
<i>where in the above: p = parallelizable component 
                     s = achievable speedup through scale up</i>
</p>

<p>
It is therefore clear from the above that for a fixed problem size
continuously increasing the number of machines in order to scale out
will not be that beneficial and you might end up with a very poor
utilization of those. You have as bottleneck the non parallelizable
component and scaling out will not help.
</p>

<p>
To help visualizing the concept, please see
</p>

<style>
.container {
  position: relative;
  left: 15%;
  width: 70%;
  margin-top: 60px;
  margin-bottom: 60px;
  overflow: hidden;
  padding-top: 56.25%; /* 16:9 Aspect Ratio */
  display:block;
  overflow-y: hidden;
}

.responsive-iframe {
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  right: 0;
  width: 100%;
  height: 100%;
  border: none;
  display:block;
  overflow-y: hidden;
}
</style>

<div class="container"> 
  <iframe class="responsive-iframe" src="https://www.youtube.com/embed/oMyqwXjbnVM" frameborder="0" allowfullscreen;> </iframe>
</div>
