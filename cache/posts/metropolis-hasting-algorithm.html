<br>
<br>

<p>
This post explores the Metropolis Hasting algorithm. 
</p>

<p>
This is a second option in addition to the Gibbs Sampler previously
mentioned to sample from a desired distribution &pi; (x) without the
need to generate an i.i.d. sample from the targeted distribution but
rather leveraging the properties of Markov Chains. A short, but more
thorough explanation of the case may be found in <a href="https://marcohassan.github.io/bits-of-experience/posts/Gibbs-Sampler/">this post</a> introducing
the Gibbs Sampler.  
</p>

<!-- TEASER_END -->

<p>
As in the case of the Gibbs Sampler the basic idea of the
Metropolis-Hasting Algorithm consists in generating a reversible
Markov Chain such that the properties of the latter will assure
convergence to the stationary state represented, in such a case, by
the targeted probability density function &pi; (x). 
Once convergence occurred it is then possible to compute the
expectation of the targeted distribution by applying the CLT and
the usual inferential results of it.
</p>

<br>

<div id="outline-container-org3047307" class="outline-2">
<h2 id="org3047307">The Markov Chain Construction</h2>
<div class="outline-text-2" id="text-org3047307">
<p>
In comparison to the Gibbs Sampler that updates the a parameter at the
time in an efficient orthogonal way, the Metropolis Hastings Algorithm
is more effective in exploring leveraging the realizations from the
chosen PDF, nonetheless if the support PDF is not similar enough to
the targeted distribution the rejection step will cause the majority
of the exploration trials to be rejected and the Chain will converge
slowly to its steady state.
</p>
</div>
</div>
