<style>
img {
  display: block;
  margin-left: auto;
  margin-right: auto;
}
</style>

<p>
Here are some notes based on the <i>Artificial Intelligence:
Reinforcement Learning in Python</i> Udemy course.
</p>

<p>
This are very personal notes that do not intend to substitute the
course. The guy is good. I recommend his courses. I am enjoying and
the way he teaches with minor exercises that makes you well think is
good. 
</p>

<p>
Note that the code presented is open sourced and can be found <a href="https://github.com/lazyprogrammer/machine_learning_examples/tree/master/rl">here</a>.
</p>

<!-- TEASER_END -->

<br>
<br>


<div id="outline-container-org67337d4" class="outline-2">
<h2 id="org67337d4">Reinforcement Learning</h2>
<div class="outline-text-2" id="text-org67337d4">
<p>
Here the idea is to play with two opposite forces.
</p>

<p>
Given a problem you are interesting of finding the optimal solution
for it balancing two forces:
</p>

<ul class="org-ul">
<li>exploration</li>
</ul>

<p>
here the goal is to collect as many data as possible and as wide as
possible in order to explore the problem and all of its possible
outcomes. you don't want to be <b>greedy</b>, i.e. you do not want to use
just immediately available information as the basis of your decision/
i.e. you don't want to make locally optimal choices at each stage but
you rather want to go to the global optimal solution as quickly as
possible.
</p>

<ul class="org-ul">
<li>exploitation</li>
</ul>

<p>
you don't want to explore states and outcomes that are not beneficial
to you. you have therefore to balance the way in which you explore the
states in such a way that you collect information without harming
yourself too much. 
</p>

<p>
the above is called the <b>exploit-explore-dilemma</b>.
</p>
</div>

<div id="outline-container-orgca1c73f" class="outline-3">
<h3 id="orgca1c73f">Epsilon Greedy Theory</h3>
<div class="outline-text-3" id="text-orgca1c73f">
<p>
Here the idea is that you take the MLE action maximizing your outcome
(1- &epsilon;) of the times. While with &epsilon; probability you simply
do something random (i.e. you explore the space in a non-greedy way).
</p>

<p>
The pseudo-code for that would look approximately as this:
</p>

<br>
<br>

<img width="100%" height="100%" src="../../images/Bildschirmfoto_2020-06-22_um_09.59.32.png" class="center">

<br>
<br>

<p>
Finally, it is important that even when using this epsilon random
component you might not be interested in exploring the space
continuously. This especially for static problems not evolving over
time.
</p>

<p>
It is namely true, that for such systems you might have explored the
space sufficiently and you observed one particular state to be the
most performing among the many. Then at each exploration you loose
some benefit from deviating from the optimum decision.
</p>

<p>
It is therefore necessary that once you have built up trust and you
deem to have explored the system sufficiently you just focus on the
<b>exploitation</b> component and shrunk your epsilon to zero.
</p>

<p>
You can model the way to do that as needed in your business case.
</p>

<p>
In practice, some &epsilon; is often modeled as decaying in
time. I.e. at the beginning you explore the space the most and then
gradually you explore the more and more rarely.
</p>

<br>
<br>

<img width="50%" height="100%" src="../../images/Bildschirmfoto_2020-06-22_um_11.02.57.png" class="center">

<br>
<br>
</div>

<div id="outline-container-orga00bdae" class="outline-4">
<h4 id="orga00bdae">Epsilon-Greedy Example</h4>
<div class="outline-text-4" id="text-orga00bdae">
<div class="highlight"><pre><span></span>import numpy as np
import matplotlib.pyplot as plt
import math
</pre></div>

<p>
Global Parameters
</p>

<div class="highlight"><pre><span></span>NUM_TRIALS = 10000
EPS = 0.1
BANDIT_PROBABILITIES = [0.2, 0.5, 0.75]
</pre></div>


<p>
Define the data generation model for your bandit machine:
</p>

<div class="highlight"><pre><span></span>class Bandit:
  def __init__(self, p):
    # p: the win rate
    self.p = p
    self.p_estimate = 0
    self.N = 0
    self.correct = 0

  def pull(self):
    # draw a 1 with probability p
    return np.random.random() &lt; self.p

  def update(self, x):
    self.N +=1
    self.correct += self.pull()
    self.p_estimate = self.correct/self.N
</pre></div>

<div class="highlight"><pre><span></span>def experiment(BANDIT_PROBABILITIES):
  bandits = [Bandit(p) for p in BANDIT_PROBABILITIES]

  rewards = np.zeros(NUM_TRIALS) ## initialize zero vector
  num_times_explored = 0
  num_times_exploited = 0
  num_optimal = 0
  optimal_j = np.argmax([b.p for b in bandits])
  print(&quot;optimal j:&quot;, optimal_j)

  for i in range(NUM_TRIALS):

    # use epsilon-greedy to select the next bandit
    if np.random.random() &lt; EPS:
      num_times_explored += 1
      j = np.random.randint(len(bandits))
    else:
      num_times_exploited += 1
      j = np.argmax([b.p_estimate for b in bandits])

    if j == optimal_j:
      num_optimal += 1

    # pull the arm for the bandit with the largest sample
    x = bandits[j].pull()

    # update rewards log
    rewards[i] = x

    # update the distribution for the bandit whose arm we just pulled
    bandits[j].update(x)

  ## log the performance of your epsilon greedy model

  # print mean estimates for each bandit
  for b in bandits:
    print(&quot;mean estimate:&quot;, b.p_estimate)

  # print total reward
  print(&quot;total reward earned:&quot;, rewards.sum())
  print(&quot;overall win rate:&quot;, rewards.sum() / NUM_TRIALS)
  print(&quot;num_times_explored:&quot;, num_times_explored)
  print(&quot;num_times_exploited:&quot;, num_times_exploited)
  print(&quot;num times selected optimal bandit:&quot;, num_optimal)

  # plot the results
  cumulative_rewards = np.cumsum(rewards)
  win_rates = cumulative_rewards / (np.arange(NUM_TRIALS) + 1)
  plt.plot(win_rates)
  plt.plot(np.ones(NUM_TRIALS)*np.max(BANDIT_PROBABILITIES))
  plt.show()
</pre></div>

<div class="highlight"><pre><span></span>experiment([0.2, 0.5, 0.75])
</pre></div>

<br>
<br>

<img width="50%" height="100%" src="../../images/ob-ein-b336295bb0cecce62fa035b851c1fdaf.png" class="center">

<br>
<br>

<p>
So we are close to the real world means so that we well explored the
states.
</p>

<p>
Moreover our win rate is 0.7235; hence we are not that far from the
CLT average when always selecting the best possible machine. In this
sense the epsilon greedy algorithm performs a good job balancing the
<b>exploitation-exploration trade off</b>.
</p>

<p>
Important in the above is also the choice of the &epsilon;
parameter. Here the idea is that if you want to quickly explore the
space and have fast convergence to the most profitable machine then
you have to select a rather big epsilon. In contrast, if you are
willing to slowly reach converge to the optimal machine but have a
long-run cumulative reward (as then the deviation is small) you should
choose a small &epsilon;.
</p>
</div>
</div>
</div>

<div id="outline-container-org87427ba" class="outline-3">
<h3 id="org87427ba">Optimistic Initial Values Method</h3>
<div class="outline-text-3" id="text-org87427ba">
<p>
This is a second approach to deal with the <b>exploitation-exploration
trade-off</b>. The idea here is that instead of starting with an expected
value of zero for the mean reward of each machine you would set very
high values for the expected reward of each machine.
</p>

<p>
By setting a high initial value, the model would try to leverage on
the high expected profit for the particular machine by repeatedly
"exploiting it". It is then true that as time goes by you would
eventually learn the true moment of the machine and the expected gain
would shrink towards the true moment.
</p>

<p>
Important is therefore to understand that for such an algorithm you do
not leverage any random exploration but rather set an initial value
determining the extent to which you would explore a particular
machine. It is straightforward to see that:
</p>

<blockquote>
<p>
the higher the initial value the higher the exploration on a
praticular machine
</p>
</blockquote>

<p>
Finally, notice that we do not have any consistency property for such
algorithm. I.e. while the estimated mean of each machine converged to
the true mean for the epsilon-greedy algorithm asymptotically, here we
stop to explore a particular machine as soon as its expected mean is
below the one of the other machines. 
</p>

<p>
It is therefore true that as the highest true mean will set an anchor
on the level of the max(expected mean) of the machines we expected
that for different machines the asymptotic mean is below such anchor
but did not converge as we eventually stopped exploring such machines
and converged to the most rewarding machine.
</p>
</div>

<div id="outline-container-org38594b8" class="outline-4">
<h4 id="org38594b8">Initial Optimal Value Example</h4>
<div class="outline-text-4" id="text-org38594b8">
<p>
Global HyperParameters
</p>

<div class="highlight"><pre><span></span>NUM_TRIALS = 10000
EPS = 0.1
BANDIT_PROBABILITIES = [0.2, 0.5, 0.75]
</pre></div>

<p>
Data generating process
</p>

<div class="highlight"><pre><span></span>class Bandit:
  def __init__(self, p):
    # p: the win rate
    self.p = p
    self.p_estimate = 10
    self.N = 1.

  def pull(self):
    # draw a 1 with probability p
    return np.random.random() &lt; self.p

  def update(self, x):
    self.N += 1.
    self.p_estimate = ((self.N - 1)*self.p_estimate + x) / self.N
</pre></div>

<div class="highlight"><pre><span></span>def experiment():
  bandits = [Bandit(p) for p in BANDIT_PROBABILITIES]

  rewards = np.zeros(NUM_TRIALS)
  for i in range(NUM_TRIALS):
    # use optimistic initial values to select the next bandit
    j = np.argmax([b.p_estimate for b in bandits])

    # pull the arm for the bandit with the largest sample
    x = bandits[j].pull()

    # update rewards log
    rewards[i] = x

    # update the distribution for the bandit whose arm we just pulled
    bandits[j].update(x)


  # print mean estimates for each bandit
  for b in bandits:
    print(&quot;mean estimate:&quot;, b.p_estimate)

  # print total reward
  print(&quot;total reward earned:&quot;, rewards.sum())
  print(&quot;overall win rate:&quot;, rewards.sum() / NUM_TRIALS)
  print(&quot;num times selected each bandit:&quot;, [b.N for b in bandits])

  # plot the results
  cumulative_rewards = np.cumsum(rewards)
  win_rates = cumulative_rewards / (np.arange(NUM_TRIALS) + 1)
  plt.ylim([0, 1])
  plt.plot(win_rates)
  plt.plot(np.ones(NUM_TRIALS)*np.max(BANDIT_PROBABILITIES))
  plt.show()
</pre></div>

<div class="highlight"><pre><span></span>experiment()
</pre></div>

<br>
<br>

<img width="50%" height="100%" src="../../images/ob-ein-e6ad5beee2c95ea8c5dacc1181790e54.png" class="center">

<br>
<br>
</div>
</div>
</div>

<div id="outline-container-org2de8c61" class="outline-3">
<h3 id="org2de8c61">Upper Confidence Bound</h3>
<div class="outline-text-3" id="text-org2de8c61">
<p>
This builds on the ideas of optimistic initial value.
</p>

<p>
The idea is to model probabilistically the upper bound instead of
guessing from the CLT property as in the optimistically initial value
algorithm. 
</p>

<p>
The idea here is to choose the machine <code>j</code> not simply by taking the
$max {(expected reward)} $ at any given time, but rather to select the
machine based on the expected reward itself and the measurement error
for the specific machine; i.e. exploit:
</p>

<p>
\[\max{f(\bar{X_{j}}, \epsilon (X_{j}))}\]
</p>

<p>
The question is now on how to model the expected reward.
</p>

<p>
The idea of the authors of such model was the one of leveraging the
<b>Hoeffding's inequality</b> where the bias for your sample estimation
converges exponentially fast to zero.
</p>

<br>
<br>

<img width="30%" height="100%" src="../../images/Bildschirmfoto_2020-06-22_um_15.35.17.png" class="center">

<br>
<br>

<p>
You would then get an estimate for your error for a particular machine
at each point of time <code>t</code> by setting the left hand side equation to
some constant and then solving the equation for t. (in order to see
that look at the left hand side inequality in the inequality)
</p>

<p>
It is then possible to see that with p = \(\frac{1}{N^{4}}\) you
would obtain:
</p>

<p>
\[ t = \sqrt{2\frac{log (N)}{n_j}} \]
</p>

<p>
You would then select your most rewarding machine as 
</p>

<p>
\[ j = arg_j \max{\bar{X_j} + \sqrt{2\frac{log (N)}{n_j}}} \]
</p>

<p>
It is then clear from the formula that you would explore more:
</p>

<ul class="org-ul">
<li>a machine that has never been explored</li>

<li>a machine with high expected reward</li>
</ul>

<p>
And that when you sampled enough observations the denominator will
tend to override the effect of the denominator and you would exploit
the highest rewarding machine.
</p>
</div>

<div id="outline-container-orge6d9207" class="outline-4">
<h4 id="orge6d9207">UCB Example</h4>
<div class="outline-text-4" id="text-orge6d9207">
<div class="highlight"><pre><span></span>class Bandit:
  def __init__(self, p):
    # p: the win rate
    self.p = p
    self.p_estimate = 0.
    self.N = 0. # num samples collected so far

  def pull(self):
    # draw a 1 with probability p
    return np.random.random() &lt; self.p

  def update(self, x):
    self.N += 1.
    self.p_estimate = ((self.N - 1)*self.p_estimate + x) / self.N
</pre></div>


<div class="highlight"><pre><span></span>def ucb(mean, n, nj):
  return (mean + (math.log(n)/nj)**0.5)


def run_experiment():
  bandits = [Bandit(p) for p in BANDIT_PROBABILITIES]
  rewards = np.empty(NUM_TRIALS)
  total_plays = 0

  # initialization: play each bandit once
  for j in range(len(bandits)):
    x = bandits[j].pull()
    total_plays += 1
    bandits[j].update(x)

  for i in range(NUM_TRIALS):

    j = np.argmax([ucb(b.p_estimate, total_plays, b.N) for b in bandits])
    x = bandits[j].pull()
    total_plays += 1
    bandits[j].update(x)

    # for the plot
    rewards[i] = x
  cumulative_average = np.cumsum(rewards) / (np.arange(NUM_TRIALS) + 1)

  # plot moving average ctr
  plt.plot(cumulative_average)
  plt.plot(np.ones(NUM_TRIALS)*np.max(BANDIT_PROBABILITIES))
  plt.xscale(&#39;log&#39;)
  plt.show()

  # plot moving average ctr linear
  plt.plot(cumulative_average)
  plt.plot(np.ones(NUM_TRIALS)*np.max(BANDIT_PROBABILITIES))
  plt.show()

  for b in bandits:
    print(b.p_estimate)

  print(&quot;total reward earned:&quot;, rewards.sum())
  print(&quot;overall win rate:&quot;, rewards.sum() / NUM_TRIALS)
  print(&quot;num times selected each bandit:&quot;, [b.N for b in bandits])

  return cumulative_average
</pre></div>


<div class="highlight"><pre><span></span>run_experiment()
</pre></div>

<br>
<br>

<img width="50%" height="100%" src="../../images/ob-ein-089592f8c70f571f8112c1c3f8af6c66.png" class="center">

<br>
<br>
</div>
</div>
</div>

<div id="outline-container-org9770d38" class="outline-3">
<h3 id="org9770d38">Bayesian Bandits - Thompson Sampling Theory</h3>
<div class="outline-text-3" id="text-org9770d38">
<p>
Here the idea is the one to operate in fully bayesian setting. Please
refer to the following notes if you want to <a href="https://marcohassan.github.io/bits-of-experience/pages/papers/#bayesian">well appreciate the
section</a>.
</p>

<p>
The idea here is that instead balancing the exploiting-exploring
trade-off via a probabilistic argument as the one above which
leverages some threshold properties for the expected value bias, you
might well model the prior distribution of each machine as a beta and
the conditional likelihood of the data given the unknown parameter as
a bernoulli. 
</p>

<p>
Where &theta; represents the true expected reward for the modeled
machine and k<sub>j</sub> represents the number of times the modeled machine was
exploited and therefore the number of observations collected for it.
</p>

<p>
Given that we are dealing here with the beta exponential family it is
easy to show that in such a case the resulting posterior resulting
from the likelihood distribution of the data and the prior is a beta
distribution itself with the following moments:
</p>

<br>
<br>

<img width="50%" height="100%" src="../../images/Bildschirmfoto_2020-06-22_um_17.48.29.png" class="center">

<br>
<br>

<p>
and choosing a uninformative prior such as the uniform distribution,
which results in a Beta(1,1) distribution you would get that:
</p>

<br>
<br>

<img width="100%" height="100%" src="../../images/Bildschirmfoto_2020-06-22_um_17.55.41.png" class="center">

<br>
<br>


<p>
Hence you see that the distribution adapts to fit your data.
</p>

<p>
The idea of the Thompson Sampling is now the following:
</p>

<ol class="org-ol">
<li>sample from the prior distribution at the first iteration</li>

<li>choose the machine with the highest sample as from 1. (3.) generate a
new posterior for the machine.</li>

<li>sample from the three machine distribution (prior if no data
available) posterior otherwise. go back to 2.</li>
</ol>
</div>


<div id="outline-container-org64a8079" class="outline-4">
<h4 id="org64a8079">Thompson Sampling - Bandit Example</h4>
<div class="outline-text-4" id="text-org64a8079">
<div class="highlight"><pre><span></span> import matplotlib.pyplot as plt
 import numpy as np
 from scipy.stats import beta


 # np.random.seed(2)
 NUM_TRIALS = 2000
 BANDIT_PROBABILITIES = [0.2, 0.5, 0.75]
</pre></div>

<div class="highlight"><pre><span></span> class Bandit:
   def __init__(self, p):
     self.p = p
     self.a = 1
     self.b = 1
     self.N = 0 # for information only

   def pull(self):
     return np.random.random() &lt; self.p

   def sample(self):
     return np.random.beta(self.a, self.b) 

   def update(self, x):
     self.a += x
     self.b += 1 - x
     self.N += 1
</pre></div>

<div class="highlight"><pre><span></span>def plot(bandits, trial, idx):
  x = np.linspace(0, 1, 200)
  plt.subplot(5,5,idx)
  for b in bandits:
    y = beta.pdf(x, b.a, b.b)
    plt.plot(x, y, label=f&quot;real p: {b.p:.4f}, win rate = {b.a - 1}/{b.N}&quot;)
  plt.title(f&quot;Bandit distributions after {trial} trials&quot;)
  plt.legend()

def experiment():

  idx = 0
  bandits = [Bandit(p) for p in BANDIT_PROBABILITIES]

  sample_points = [5,10,20,50,100,200,500,1000,1500,1999]
  rewards = np.zeros(NUM_TRIALS)


  f, axes = plt.subplots(figsize = (30, 30))
  for i in range(NUM_TRIALS):
    # Thompson sampling
    j = np.argmax([b.sample() for b in bandits])

    # plot the posteriors
    if i in sample_points:
      idx += 1
      plot(bandits, i, idx)

    # pull the arm for the bandit with the largest sample
    x = bandits[j].pull()

    # update rewards
    rewards[i] = x

    # update the distribution for the bandit whose arm we just pulled
    bandits[j].update(x)

  plt.show()

  # print total reward
  print(&quot;total reward earned:&quot;, rewards.sum())
  print(&quot;overall win rate:&quot;, rewards.sum() / NUM_TRIALS)
  print(&quot;num times selected each bandit:&quot;, [b.N for b in bandits])
</pre></div>

<div class="highlight"><pre><span></span>experiment()
</pre></div>

<img width="100%" height="100%" src="../../images/ob-ein-88099e21985dfc2e30f01dd8a97bafd0.png" class="center">

<br>
<br>

<pre class="example">
total reward earned: 1533.0
overall win rate: 0.7665
num times selected each bandit: [13, 44, 1943]
</pre>


<p>
From the above you see that as you sample more from the most rewarding
function then your beta parameters adapt. At the beginning as you have
just a few samples and you have no successful draws for machine 1,2
the distribution of them has a distribution with mean &lt; 0.5 and is
strongly skewed in favour of expected reward = 0 where the most of the
samples would generate. I.e. we already tend to exploit the machine
with the highest reward: machine 3.
</p>

<p>
As you get more samples the distributions adapts according to the
data. It is clear that already after 200 samples the distribution for
the third machine is quite concentrated around its mean and therefore
the probability of sampling a higher number for the third machine and
therefore exploiting the third machine is already consistent. In fact
between obs = 200 and obs = 500 we "exploit" the first machine just 2
more times and 7 times the second machine, therefore correctly
exploiting the 3 machine 293/300 times.
</p>

<p>
Notice finally that you can expand the above to have a different
reward likelihood as the bernoulli. Depending on your case you might
well want to model the likelihood in a different way - it is however
always recommended that you set up your model to come out with
conjugate priors distributions -. The thompson sampling approach is
therefore generalizable and it is just a matter of properly specifying
the proper distributions of your likelihood and prior and come up with
the posterior (as said ideally a conjugate distribution).
</p>
</div>
</div>
</div>

<div id="outline-container-org81308ab" class="outline-3">
<h3 id="org81308ab">The General Reinforcement Learning Setting</h3>
<div class="outline-text-3" id="text-org81308ab">
<p>
The idea here is to expand on the simple bandit problem seen so far,
where you just confronted with the case of action (choice of machine)
-&gt; reward.
</p>

<p>
In a general RL framework the situation is more complex. The idea is
that you would have a setting:
</p>

<br>
<br>

<img width="50%" height="100%" src="../../images/reinforcement.svg" class="center">

<br>
<br>

<p>
The environment, i.e. the states, is generally modeled through a grid,
that would save the different states and the rewards in each cell. On
such a state grid you would base your action which would determine the
obtained reward and finally influence the new gridworld matrix. The
function mapping a state change to an action is termed a
<b>policy</b>. Notice that such policy might well be deterministic or
probabilistic. 
</p>

<p>
Another important term in the RL world is the one of an
<b>episode</b>. This is an iteration of the game that is being played. It
is similar to an epoch in Deep Learning so to say. Notice that a game
end not after each iteration but rather once the terminal state is
reached - for instance in a finance application if you lost X% of
value of your wallet; or in barrier options if the barrier was
triggered -.
</p>

<p>
Notice, that in contrast to the examples for <b>episodic tasks</b> above
mentioned with clearly defined terminating states, some games might be
<b>non-episodic</b> such that no terminal state exists.
</p>

<p>
Summing all of the components up your have the following model
</p>

<br>
<br>

<img width="50%" height="100%" src="../../images/Bildschirmfoto_2020-06-23_um_11.18.13.png" class="center">

<br>
<br>

<p>
Then with the notation
</p>

<p>
s' = s<sub>t+1</sub>, r = r<sub>t+1</sub>, s = s<sub>t</sub>, a = a<sub>t</sub>
</p>

<p>
where the lower notation denotes that the above are actual
realizations of the random variables: S<sub>t+1</sub>, R<sub>t+1</sub>, S<sub>t</sub>, A<sub>t</sub>.
</p>

<p>
You can model the probability of the expected reward and state in t+1
as:
</p>

<p>
\[ S_{t+1}, R_{t+1} \mathtt{\sim} p(s', r | s,a) \]
</p>

<p>
this will be in fact the job in RL and your role is to define such
probability for the transition to state<sub>t+1</sub>. Notice that this is the
more general formulation where the reward is stochastic given the
state and the action taken. This is useful when you deal with systems
where you do not have perfect information. 
</p>

<p>
You might have many systems where this is not the case so that the
general framework would look as:
</p>

<p>
\[ S_{t+1} \mathtt{\sim} p(s'| s,a) \]
</p>

<p>
and the reward r would be simply determined by {s,a} and therefore
does not enter the equation above.
</p>

<p>
Notice moreover that the state s<sub>t</sub> does not have to be defined by a
single observation. You might well have multiple observation to define
a state (think for instance a set of images in a video such that it is
possible from it to derive motion for the single components). 
</p>

<p>
Notice finally, that the above is analogous to a markov model with the
additional variables (a,r). This is in fact what contrast a Markov
Decision Process from a Markov Process. In an MDP you condition on the
action taken and not simply on the state. 
</p>

<p>
Notice that for small systems you might represent Markov Decision
Processes via <b>State-Action-Diagrams</b>. 
</p>

<br>
<br>

<img width="50%" height="100%" src="../../images/Bildschirmfoto_2020-06-23_um_11.53.38.png" class="center">

<br>
<br>
</div>


<div id="outline-container-orga52c386" class="outline-4">
<h4 id="orga52c386">Objective</h4>
<div class="outline-text-4" id="text-orga52c386">
<p>
Given the general setting above it is now clear that in reinforcement
learning the goal would be for the agent to decide on the action to
take that will lead to the possibility of maximizing the sum of
<i>future</i> rewards for the episode.
</p>

<p>
\[ G(t) = \sum_{\tau = 0}^{T} R(t + \tau + 1) \]
</p>

<p>
Moreover, it is usual in reinforcement learning to discount rewards
into the feature by a discount factor &gamma;. This is a very much
finance alike approach and the intuition there is that being the model
probabilistic estimates for rewards into the future are more uncertain
and you should therefore weight them less.
</p>

<p>
\[ G(t) = \sum_{\tau = 0}^{T} \gamma^{\tau} R(t + \tau + 1) \]
</p>

<p>
The question is then on how to decide on the discount factor. This is
usually set as a hyperparameter and tuned by simulation when domain
knowledge is missing.
</p>

<p>
Notice, now that the future rewards might not be possible to be
determined ex-ante at period t. This is why in general we aim at
maximizing:
</p>

<p>
\[ V_\pi(s) = E_\pi [G(t) | S_{t} = s] \]
</p>

<p>
This is the final objective function we aim to maximize in
reinforcement learning.
</p>
</div>
</div>

<div id="outline-container-orgf6645d0" class="outline-4">
<h4 id="orgf6645d0">Bellmann Equation</h4>
<div class="outline-text-4" id="text-orgf6645d0">
<p>
Notice now that as G(t) is recursive you might well write
</p>


<p>
\[ G(t + 1) = \sum_{\tau = 0}^{T} \gamma^{\tau} R((t+1) + \tau + 1) \]
</p>


<p>
such that
</p>

<p>
\[ G(t) = R(t+1) + \gamma G(t+1) \]
</p>

<p>
and 
</p>

<p>
\[ V_\pi(s) = E_\pi [R(t+1) + \gamma G(t+1) | S_{t} = s] \]
</p>

<p>
Given the law of iterated expectation (tower rule) it is then possible
to write the above as
</p>


<p>
\[ V_\pi(s) = E_\pi [R(t+1)| S_{t} = s] + \gamma E_\pi[G(t+1) | S_{t} = s] \]
</p>

<p>
\[ V_\pi(s) = E_\pi [R(t+1)| S_{t} = s] + \gamma E_\pi[E_\pi[G(t+1) | S_{t+1} = s'] | S_{t} = s]] \]
</p>

<p>
\[ V_\pi(s) = E_\pi [R(t+1) + \gamma V_\pi(s+1) | S_{t} = s]] \]
</p>

<p>
Hence the objective function has as well recursive structure.
</p>

<p>
This practically means that for solving the objective function in a
particular state you do not have to solve recursively to get G(t) and
therefore compute each individual r(t+k), but you rather need only the
objective function in the next period. 
</p>

<br>
<br>

<img width="50%" height="100%" src="../../images/Bildschirmfoto_2020-06-23_um_14.24.26.png" class="center">

<br>
<br>

<p>
and with the other two analogous equations for the other states you
are back to linear algebra such that you can solve a system of
equations and get the objective function at each state so that you can
compute the value function for a specific state.
</p>

<p>
Finally, notice in the above you are working with the expected E<sub>&pi;</sub>,
i.e. you are taking the average among all of the actions for a
particular state. This might be useful for systems where the agent
does not control the action decision.
</p>

<p>
However, in case of taking a specific action the system has to decide
on your value function would rather looks as
</p>

<br>
<br>

<img width="50%" height="100%" src="../../images/Bildschirmfoto_2020-06-23_um_14.46.07.png" class="center">

<br>
<br>


<p>
This Q-Function; i.e. action-value objective function would be the one
you minimize. 
</p>

<p>
So that it finally holds:
</p>

<p>
\[ V_\pi(s) = \sum_a \pi{(a|s)} Q_\pi(s,a) \]
</p>
</div>
</div>

<div id="outline-container-orgb92919b" class="outline-4">
<h4 id="orgb92919b">Which Objective Value function is better</h4>
<div class="outline-text-4" id="text-orgb92919b">
<p>
Notice that in machine learning you have a single ending state so that
it is easy to define if one particular parameter vector is better than
another one by simple determine the loss function of the two and
minimizing it.
</p>

<p>
In reinforcement learning it is not that trivial to define when a
policy (i.e. \(\phi: S \mapsto A\)) is better than another one as you do
not simply have to have a higher value for the state you landed in but
rather 
</p>

<p>
\[ \pi_1 \geq \pi_2 iff V_{\pi_1} \geq V{\pi_2} \forall s \in S \]
</p>

<p>
This given the stochastic nature of the transition to a state given an
action and the previous state.
</p>

<p>
It follows now that you should base your decision for the policy &pi;
and for your action based on:
</p>


<br>
<br>

<img width="50%" height="100%" src="../../images/Bildschirmfoto_2020-06-23_um_15.15.43.png" class="center">

<br>
<br>

<p>
Recall now that as 
</p>

<p>
\[ V_\pi(s) = E_\pi [R(t+1) + \gamma V_\pi(s+1) | S_{t} = s]] \]
</p>

<p>
and 
</p>

<p>
\[ V_\pi^{*} (s) = max_\pi E_\pi [R(t+1) + \gamma V_\pi(s+1) | S_{t} = s]] \]
</p>


<p>
with &pi; \(\Longleftrightarrow\) &phi;: S \(\mapsto\) A,
</p>

<p>
\[ V_\pi^{*} (s) = max_a [R(t+1) + \gamma V_\pi(s+1) | S_{t} = s]] \]
</p>

<p>
finally you have:
</p>

<br>
<br>

<img width="50%" height="100%" src="../../images/Bildschirmfoto_2020-06-23_um_15.40.51.png" class="center">

<br>
<br>

<p>
Given the above it now follows that for deciding on which policy to
take you can leverage one of the two:
</p>

<br>
<br>


<style>
 {
  box-sizing: border-box;
}

.column {
  float: left;
  width: 40%;
  padding: 5px;
}

/* Clearfix (clear floats) */
.row::after {
  content: "";
  clear: both;
  display: table;
}
</style>

<div class="row">
  <div class="column">
    <img style="width:100%" src="../../images/Bildschirmfoto_2020-06-23_um_15.43.52.png">
  </div>
  <div class="column">
    <img style="width:100%" src="../../images/Bildschirmfoto_2020-06-23_um_15.44.09.png">
  </div>
</div>

<br>
<br>

<p>
Albeit in practice it would probably make sense to use the left option
to compute the optimal policy, in practice in RL you will often work
with the right hand side equation as it is less cumbersome and
computationally faster.
</p>

<p>
This in fact does not require you to sum over two random variables (s'
and r) for many different action possibilities but rather simply to
take the maximum value in your Q-table.
</p>

<p>
This leaves us with the following outline that will be the basic
approach for each RL algorithm we are going to explore next.
</p>

<p>
I.e. do:
</p>

<ul class="org-ul">
<li>find V(s) for a given policy (evaluation / prediction problem)</li>

<li>find the best policy by finding Q<sup>*</sup> (control problem)</li>
</ul>
</div>
</div>

<div id="outline-container-orgac88696" class="outline-4">
<h4 id="orgac88696">On the Evaluation / Prediction Problem</h4>
<div class="outline-text-4" id="text-orgac88696">
<p>
The previous sections outlined the entire theory behind reinforcement
learning. We defined an objective function we aim to maximize - i.e
the bellman value function equation -.
</p>

<p>
As we saw in the previous section you might solve for each value
function by solving a linear system of equations. However, in the case
the state numbers would be big - think for instance to the well known
alpha go case - this approach would be difficult to treat.
</p>

<p>
A second solution consists in leveraging an iterative optimization
approach as in the idea of the following pseudo code:
</p>

<br>
<br>

<img width="50%" height="100%" src="../../images/Bildschirmfoto_2020-06-23_um_16.08.10.png" class="center">

<br>
<br>

<p>
The idea is that at each iteration you get closer to the true value
V(s) as each V(s') increases and therefore you gradually approach your
desired solution.
</p>
</div>
</div>
</div>
</div>
