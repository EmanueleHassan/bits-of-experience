<style>
img {
display: block;
margin-top: 60px;
margin-bottom: 60px;
margin-left: auto;
margin-right: auto;
width: 70%;
height: 100%;
class: center;
}

.container {
  position: relative;
  left: 15%;
  margin-top: 60px;
  margin-bottom: 60px;
  width: 70%;
  overflow: hidden;
  padding-top: 56.25%; /* 16:9 Aspect Ratio */
  display:block;
  overflow-y: hidden;
}

.responsive-iframe {
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  right: 0;
  width: 100%;
  height: 100%;
  border: none;
  display:block;
  overflow-y: hidden;
}
</style>

<p>
Here are some Notes about the topic of my Master Thesis - Bayesian
Networks.
</p>

<p>
Note that most of these Notes are based on <i>Probabilistic Graphical
Models - Principles and Techniques</i> (<a href="https://www.amazon.de/Probabilistic-Graphical-Models-Principles-Computation/dp/0262013193">Koller and Friedman</a>). The book is
outstanding and would recommend it. Note that here all of the pictures
you have inside stem from this book. Read them as a word by word
quotation. 
</p>

<!-- TEASER_END -->

<div id="outline-container-orgac550f2" class="outline-2">
<h2 id="orgac550f2">Bayesian Networks</h2>
<div class="outline-text-2" id="text-orgac550f2">
<p>
The general outlook.
</p>

<p>
So recall that in general you have three elements in Bayesian
Networks:
</p>

<ul class="org-ul">
<li>Representation

<ul class="org-ul">
<li><p>
how do you represent the joint probability of the events as a
network (i.e. as a graph data structure)? Can such structure
represent the joint in a compact way due to the conditional
independence relations?
</p>

<p>
<b>Note 1:</b> that such compact formulation is one of the key benefits of
Bayesian Networks as it really gives the possibility of shrinking
the amount of parameters needed to describe the full joint
probability leveraging the independence structure among the RVs.
</p>

<p>
<b>Note 2:</b> this formulation is <i>transparent</i>, i.e. highly
understandable also to non-AI experts. It is so to say highly
explainable and in this new buzz of <i>explainable AI</i> a solid
option.
</p></li>
</ul></li>

<li>Inference

<ul class="org-ul">
<li>given some information about some Parent variables, how can I
infer/compute the distribution of the children in the Network?</li>
</ul></li>

<li>Learning

<ul class="org-ul">
<li>given some observed data, how can I use such information to
construct / (infer) / learn the  <i>structure</i> of the network?</li>

<li>given some observed data, how can I learn the <i>parameters</i> of the
network? I.e. how can I use the information content of the data to
derive some plausible parameterization of the network.</li>
</ul></li>
</ul>


<p>
So these are the main tasks you have to deal with in Bayesian
Networks. Basically you can do all of the three in a very simple way,
which is from a theoretical standpoint very concrete and
straightforward or you can start to consider all the aspects of the
problem going quickly towards more complex situations.
</p>
</div>

<div id="outline-container-org132e869" class="outline-3">
<h3 id="org132e869">Representation</h3>
<div class="outline-text-3" id="text-org132e869">
<br>
<br>

<style>
.bg-svg {
  width: 40%;
  background-image: url(../../images/bayesNet1.svg);
  background-size: cover;
  height: 0;
  padding: 0; /* reset */
  padding-bottom: 92%;
  border: thin dotted darkgrey;
  float:right;
  margin-left: 5%;
}
.content p{
    display: block;
    margin: 2px 0 0 0;
}
</style>

<div style="width: 100%">
   <div style= "width: 70%; margin-left = 5%;">
      <div class="bg-svg">
   </div>
   <p>

   <br/>
   <br/>

   As mentioned bayesian networks allow us to express the joint
   through less parameters.

   <br/>
   <br/>

   The idea is that you factorize the joint as a product of the
   conditionals and given the parameterization of the conditionals you
   fully specify the joint. Given the independence structures the
   number of factorization of conditional terms is limited and the
   overall necessary parameters to specify the joint small.

   <br/>
   <br/>
   
   For instance if a Variable D is fully determined by its parents B,
   C in this graph:

   <br/>   
   <br/>
   
   Then you might well understand that given B, C you do not need
   P(D | A, B, C) parameters as P(D | B, C) suffices.
  </p>
  <br style="clear: both;" />
</div>

<br>
<br>
<br>
<br>   

<p>
A concrete example is the following:
</p>

<img src="../../images/Bildschirmfoto_2021-02-15_um_13.21.25.png">

<p>
Notice there that instead of needing 2 (Diff) * 2 (Int) * 3
(Grade) * 2 (Sat) * 2 (Let) = 48 parameters to describe the joint
you simply need 2 + 2 + 12 + 6 = 22.
</p>

<p>
Given this understanding it is immediate to see that Bayesian
Networks are defined as above, i.e. as a graph data structure to
which <i>local probabilities</i> are applied. In the specific each RV in
the graph is associated with <i>conditional probability distributions
(CPD)</i> that specify the distribution given each possible joint
assignment of values to its parents. And the graph structure
together with the CPD specifies the Bayesian Network.
</p>

<p>
A <b>second</b> representation/ definition of Bayesian Networks is to
define it via a <i>global probability P</i> together with the independence
relations determined by the graph.
</p>

<p>
To determine independence relations in graphs you can use standard
logic where the argument is essentially the following:
</p>

<blockquote>
<p>
Our intuition tells us that the parents of a variable “shield” it
from probabilistic influence that is causal in nature. In other
words, once I know the value of the parents, no information
relating directly or indirectly to its parents or other ancestors
can influence my beliefs about it. However, information about its
descendants can change my beliefs about it, via an evidential
reasoning process. (Koller and Friedman)
</p>
</blockquote>

<p>
Such that you would have the following <i>local independence
structures</i>:
</p>

<p>
\[ For each variable X_i : (X_i \perp NonDescendants X_i | Parents
   X_i) \]
</p>

<p>
Notice that such set of independence is called an I-map for a
probability distribution <i>P</i>. You then say that a graph <i>G</i> is an
I-map for <i>P</i> if it satisfies the I-map relations specified <i>I(P)</i>.
</p>

<p>
And you would ultimately have the following definition:
</p>

<img src="../../images/Bildschirmfoto_2021-02-15_um_14.50.38.png">

<p>
So that you basically take here the opposite direction, from a
joint distribution <i>P</i> and the local independence structure you
have a fully specified Bayesian Network.
</p>

<p>
<b>Note</b> that you can go from one representation to the other and the
BN is defined <b>if and only if</b> you can from one to the other.
</p>
</div>

<div id="outline-container-org15c7044" class="outline-4">
<h4 id="org15c7044">On Graph Dependencies and D-separation</h4>
<div class="outline-text-4" id="text-org15c7044">
<p>
Given the above discussion and the fact that it is possible to
determine the BN given a joint density and a Graph structure, the
question now is on how to extract the conditional independence
structures implied by a graph, i.e. to extract the I-map
relations.
</p>

<p>
In order to do that a simple algorithm exists the <i>d-separation
algorithm</i>.
</p>

<p>
The idea here is the following. You know that for three nodes X,
Y, Z there exists a dependence structure between X and Y if one of
the following conditions <b>hold</b>:
</p>

<img src="../../images/Bildschirmfoto_2021-02-15_um_15.18.31.png">

<p>
This is quite intuitive.
</p>

<p>
It follows now that we can quickly assess whether two variables
are generally conditionally independent by making reasonings
leveraging the active trails as above.
</p>

<p>
I.e. for two variables to be <b>dependent</b> there must be an active
trail as defined by the conditions above.
</p>

<p>
Notice that for instance in the student BN you can investigate
the conditional independence between SAT and Difficulty as
follows:
</p>

<img src="../../images/Bildschirmfoto_2021-02-15_um_15.25.30.png">


<p>
Generally it holds:
</p>

<img src="../../images/Bildschirmfoto_2021-02-15_um_15.26.42.png">

<p>
You can then find in the book an algorithm for checking
d-separation, if interested at any point in time. Notice that
there is are also reasonings about <i>completeness</i> and <i>soundness</i>
of d-separation. I.e. how well that covers and fully specifies
independence structures of <i>P</i>.
</p>

<p>
I write in here the definition of <i>completeness</i> and <i>soundness</i>
should it be of interest at any point at a later stage:
</p>

<p>
<i>Soundness</i>:
</p>

<p>
If two nodes X and Y are d-separated given some Z, then we are
guaranteed that they are, conditionally independent given Z.
</p>

<p>
<i>Completeness</i>:
</p>

<p>
D-separation is complete if it detects all of the possible
independencies. I.e. if two variables X and Y are independent
given Z, then they are d-separated.
</p>

<p>
Formally:
</p>

<img src="../../images/Bildschirmfoto_2021-02-19_um_09.32.33.png">
</div>
</div>

<div id="outline-container-org2851598" class="outline-4">
<h4 id="org2851598">On CPD</h4>
<div class="outline-text-4" id="text-org2851598">
<p>
So far we discussed the possibility of representing the
high-dimensional joint distribution into a product of
lower-dimensional CPDs or factors, i.e. a product of local
probabilities models.
</p>

<p>
In this section we explore more into the detail the possibility of
representing such CPDs.
</p>
</div>


<ul class="org-ul">
<li><a id="orgd12591f"></a>Tabular CPD<br />
<div class="outline-text-5" id="text-orgd12591f">
<p>
This is the most basics form of CPD. It works for spaces composed
solely of <b>discrete</b> valued RV.
</p>

<p>
It consists in expressing the \(P(X | PA_X)\) as a table that
contains the joint probability of \(X \and PA_X\).
</p>

<p>
This is essentially what was given in the example above.
</p>

<p>
<b>Note:</b> it is important to realize that the number of joint
probabilities that you have to express is given by
</p>

<p>
\[|Val(PA_X)| * |Val(X)|\]
</p>

<p>
I.e. it grows <i>exponentially</i> in the number of parents. This is a
serious problem in many settings. You can also not ask an expert
to express all such CPDs. He will loose patient at some point.
</p>

<p>
So the idea is to find a mechanism to express each and every
\(P(X | PA_X)\) for each X and \(PA_X\) but without doing the
exercise explicitly.
</p>

<p>
I.e. you should find a <i>functional formula CPD = f(X, PA<sub>X</sub>)</i> such
that you can leverage some structures represented by the
functional formula and do not have to express all of the
probabilities individually.
</p>

<p>
You can then read in the book some forms of such deterministic
CPDs. The general idea is quite simple. There might be
deterministic structures that naturally arise due to the
structure of the modeled phenomena.
</p>

<p>
Moreover for deterministic networks you might have the notion of
<code>context specific independence</code>. Here the idea is that given some
particular configuration \(X \cup Y \cup Z\) you might have
independence of X and Y given Z in this particular configuration.
</p>
</div>
</li>


<li><a id="org9310efc"></a>Context Specific CPDs for non-deterministic dependecies<br />
<div class="outline-text-5" id="text-org9310efc">
<p>
Structure in CPDs might not just arise in the case of
context-specific CPDs.
</p>

<p>
The idea is that often there is some structure such that for
certain realizations a RV X given some partial assignment to some
subset of parents $ U &sub; PA<sub>X</sub>$ the probability is fully
specified and does not depend on the remaining parents.
</p>

<p>
Two ways to capture such structure is through Tree-CPDs and
rule-based CPDs.
</p>
</div>

<ul class="org-ul">
<li><a id="org0f3a223"></a>Tree-CPDs<br />
<div class="outline-text-6" id="text-org0f3a223">
<p>
This is a very intuitive structure for every human. In fact
trees are used continuously. There is a natural tendencies for
such structures in engineering so nothing new. You saw them 100s
time.
</p>

<p>
However, what is interesting is the example. In fact it is easy
to see that by leveraging the tree structure, i.e. the context
specific structure and the resulting independencies you can
highly reduce the total number of parameters.
</p>

<p>
To understand that think of the following example:
</p>

<img src="../../images/Bildschirmfoto_2021-02-19_um_12.01.18.png" class="center">

<p>
It is immediate then to see that the above highly reduces the
number of parameters.
</p>

<img src="../../images/Bildschirmfoto_2021-02-19_um_12.03.41.png" class="center">


<p>
Notice that when we talk we say that the Tree-CPDs represent the
network context specific information. This is immediate to see
as you do not in fact consider the full structure of the network, but
you already factor out some of the independencies.
</p>

<p>
To see that consider, the following case where you would have
two recommendation letters and are applying for a job. You have
to choose among the two. Then you can represent the case in the
following ways:
</p>

<img src="../../images/Bildschirmfoto_2021-02-20_um_19.08.25.png" class="center">

<p>
It is clear that on the left you work at the network structure
not leveraging context specific information while on (b) you
already start to pack that in.
</p>
</div>
</li>

<li><a id="org2e2bbf0"></a>Rule-based<br />
<div class="outline-text-6" id="text-org2e2bbf0">
<p>
Another possibility to pack information of the network structure
by leveraging context specific information is via <code>rule-CPDs</code>.
</p>

<p>
They are defined in the following way:
</p>

<img src="../../images/Bildschirmfoto_2021-02-20_um_19.31.28.png" class="center">


<img src="../../images/Bildschirmfoto_2021-02-20_um_19.50.22.png" class="center">

<p>
It follows immediately that it basically consists in sets joint
co-occurrences of RV and assigns probabilities to such cases.
</p>

<p>
With it you can then basically express all sorts of CPDs
structures that are based on some partitioning.
</p>

<p>
It is in fact immediate to see that tree-CPDs can be easily
expressed via rule-based CPDs but the converse is not true.      
</p>
</div>
</li>
</ul>
</li>

<li><a id="orgd30e2ef"></a>Independence of Causal Influence<br />
<div class="outline-text-5" id="text-orgd30e2ef">
<p>
Here the idea is the case where you have a set of variables X<sub>i</sub>
influencing Y, such that X<sub>i</sub> can influence Y in an arbitrary
way. I.e. you assume that X<sub>i</sub> can interact with each other in
complex ways making the <b>effect of each combination unrelated to
any other possible combination</b>.
</p>

<p>
Two such models that fulfill such characteristics are
</p>

<ul class="org-ul">
<li>the noisy-or model</li>

<li>the generalized linear models.</li>
</ul>
</div>

<ul class="org-ul">
<li><a id="orgfa48b89"></a>Noisy Or Model<br />
<div class="outline-text-6" id="text-orgfa48b89">
<p>
This is a very simple model. If an event occurs then you have no
100% guarantee that the usual reaction will occur. That is there
is some noise in the model and some side reaction might happen.
</p>

<p>
Think for instance at working hard at work. Then with 90% you
might have a successful project. However, due to some random
factor, say sudden cut of budget or company restructuring, your
project might fail. This is the <i>noisy part</i> and the noisy or
model.
</p>

<p>
This is the general setting. It is then possible to express such
a noisy model through a graphical representation.
</p>

<p>
Think of the following:
</p>

<img src="../../images/bayesNet2.png"  style = "width: 30% !important;">

<p>
It follows then that W<sub>1</sub> expresses the probability of the noisy
factor taking place - i.e. budget restriction. Such that
&lambda;<sub>W</sub> = P(W<sub>1</sub> | W) = 0.9. Where W = work hard and W<sub>1</sub> = normal condition.
Notice now, the case where independently on your hard work the
team mate hard work also affects the result. Then you could be
in a situation as the following
</p>


<div  style ="height: 40%; width: 50%; margin:0 auto;">
   <img src="../../images/bayesNet3.png">
</div>

<p>
Again also the TW hard work induces a probability of success of
95%, i.e. &lambda;_<sub>TW</sub> = P(TW<sub>1</sub> | TW) = 95%, and there is a 5%
prob of failure due to restructuring and budget cut.
</p>

<p>
This is essentially the Noisy-or model. You have a deterministic
or relation influencing the project success - i.e. either your
work or your team members work. You have noise, i.e. despite the
factors you might have project failures due to some
unpredictable conditions - noise. Overall the probability of
success is given by products of lambdas. I.e. if both team work
and individual work multiply both lambdas. If just one, then
take the respective lambda etc.
</p>

<p>
More formally such model is defined as:
</p>

<img src="../../images/Bildschirmfoto_2021-02-21_um_09.48.03.png" class="center">

<p>
Notice that the <i>leak probability</i> was not discussed that
far. It consists of the probability of project success even in
the case that no hard work - for neither myself nor the team
members was put in the project.
</p>

<p>
<b>Note</b> that in such a models the parameters would be represented
by the estimation of the <i>different lambdas</i>.
</p>
</div>
</li>

<li><a id="orgb1f2108"></a>Generalized Linear Models<br />
<div class="outline-text-6" id="text-orgb1f2108">
<p>
These are networks where the interaction among the variables is
represented by generalized linear models you saw a couple of
times in your studies.
</p>

<p>
Recall that in generalized linear models you would have a linear
model
</p>

<p>
\[ f(X_1, ..., X_p) = \sum_{i}^{p} w_i * X_i  \]
</p>

<p>
That would represent the load that the parents sets on the
system. Where the load of each individual variable might be
higher or lower and is therefore weighted.
</p>

<p>
Then basically you would transform such a load into a
probability by applying a sensible transformation that could
well reflect the system work. I.e. a very wide used example is
the S-shaped structure that can be modeled via logit or probit
models.
</p>

<p>
You can also start to make inference on what happens
if&#x2026; cases. For instance in the book it is discussed on how, in
the case of a binary model, the log-odd probability changes
w.r.t. a change in one of the independent binary RV. This gives
you an idea of some possible structures and relations that could
occur in such models so that if representative of some real
world situation you can leverage on this.
</p>

<p>
<b>Note</b> that here once the transformation is defined the only
parameters left are the weights/loads entering the linear part
of the model. You should therefore specify these under this
setting.
</p>
</div>
</li>
</ul>
</li>


<li><a id="org300fbdc"></a>Continuous Variables<br />
<div class="outline-text-5" id="text-org300fbdc">
<p>
These are not discussed here. Have to move on. The idea is always
the same. You now have some continuous variables, say Y and
X. You would then have for instance a relation governed by a
normal distribution where \(Y \sim N( \beta * X, \sigma^2)\).
</p>

<p>
That would actually be the case when
</p>

<p>
\[ Y = \beta_0 + \sum_{i = 1}^{P} \beta_i X_i + \epsilon \]
</p>

<p>
where &epsilon; is gaussian N(0, &sigma;<sup>2</sup>). So again the usual
stuff.
</p>
</div>
</li>

<li><a id="orge35506c"></a>Hybrid Models<br />
<div class="outline-text-5" id="text-orge35506c">
<p>
Here the basic idea is that you have a network where you have a
mixture of continuous and discrete variables affecting other
variables.
</p>

<p>
Then one possibility to model such hybrid situation is the
following
</p>


<img src="../../images/Bildschirmfoto_2021-02-21_um_10.34.49.png" class="center">

<p>
Notice that such CLG model induces a mixture on the continuous
parents Y. Moreover it does not allow to have <i>discrete
children</i>. Notice moreover that the number of parameters here is
exponential in the number of discrete variables.
</p>

<p>
Another possibility to model hybrid models is via threshold
models, where you would easily go from continuous parents to
discrete children.
</p>

<p>
Notice that these are just very basic possibilities and the idea -
both here and in the book I guess - is to start to make you reason
about how to model such situations. The possibilities are however
uncountable and therefore it is up to you then on a project to
spend some time at the beginning to engineer the entire model and
decide on the setting.
</p>
</div>
</li>
</ul>
</div>


<div id="outline-container-org3dd4036" class="outline-4">
<h4 id="org3dd4036">On Conditional Bayesian Networks</h4>
<div class="outline-text-4" id="text-org3dd4036">
<p>
Recall that no matter the CPD definition resulting from the
network structure before jumping straight into the modeling of the
CPDs for the entire network it might well make sense to consider
to reduce the problem.
</p>

<p>
In some case you might have a general problem that could be split
into submodules. Each submodules would then be generally defined -
say exhaustive - over the entire network if <b>conditioning</b> on some
elements <code>X</code> and upon some output <code>Y</code> it's entire dependency with
the network would be sufficiently specified. All of the other
elements of the sub-module would be <b>encapsulated</b> in between.
</p>

<p>
An example could be for instance the one of expressing the
failures for a PC.
</p>

<p>
Then you might well start with determining the CPDs for each
component given the parents over the entire network. On the other
hand you might consider to decompose the problem, leveraging
<i>conditional Bayesian Network</i>.
</p>


<p>
Consider for instance the <i>hard drive</i>. Although the hard drive
has a rich internal state, the only aspects of its state that
influence objects outside the hard drive are whether it is working
properly and whether it is full. The Temperature input of the hard
drive in a computer is outside the probabilistic model and will be
mapped to the Temperature parent of the Hard-Drive variable in the
computer model.
</p>

<p>
You might then use the following Conditional CPDs to express the
system:
</p>

<img src="../../images/Bildschirmfoto_2021-02-21_um_12.19.59.png" class="center">

<p>
More formally than what previously described, albeit a bit clumsy
ad definition in my opinion:
</p>


<img src="../../images/Bildschirmfoto_2021-02-21_um_12.21.17.png" class="center">
</div>
</div>


<div id="outline-container-org4d4dde6" class="outline-4">
<h4 id="org4d4dde6"><span class="todo TODO">TODO</span> Template Based Representations</h4>
<div class="outline-text-4" id="text-org4d4dde6">
<p>
Skipped and not even read to this stage. Here also temporal dependent models.
</p>
</div>
</div>


<div id="outline-container-org6e05f26" class="outline-4">
<h4 id="org6e05f26"><span class="todo TODO">TODO</span> Gaussian Network Models</h4>
<div class="outline-text-4" id="text-org6e05f26">
<p>
Skipped and not even read to this stage.
</p>
</div>
</div>

<div id="outline-container-org4f30b04" class="outline-4">
<h4 id="org4f30b04"><span class="todo TODO">TODO</span> Exponential Families</h4>
<div class="outline-text-4" id="text-org4f30b04">
<p>
Skipped and not even read to this stage. I guess it is simply the
generalization of gaussian Network Models to the different
exponential family distributions.
</p>
</div>
</div>
</div>


<div id="outline-container-orgf8d1fd1" class="outline-3">
<h3 id="orgf8d1fd1">Inference</h3>
<div class="outline-text-3" id="text-orgf8d1fd1">
<p>
An important exercise for inference is to query
distributions. I.e. as said the task is to compute the probability
of the occurrence of some RV given some evidence <i>E</i>, i.e. a subset
of RVs that is observed.
</p>

<p>
So in general the task is to determine:
</p>

<p>
\[ P (Y | E = e) \]
</p>

<p>
where <code>Y = query variable</code> and <code>E = evidence</code>.
</p>

<p>
Given such definition of probability queries it is possible to
introduce the <b>first type</b> of query: <i>MAP queries</i>.
</p>

<p>
\[ MAP (W| e) = \operatorname*{argmax}_w P (w,e)\]
</p>

<p>
where W = all non-observed RV.
</p>

<blockquote>
<p>
I.e. in MAP queries you are interested in finding the most likely
joint assignment of the non-observed variables given the evidence.
</p>

<p>
If you perform MAP queries for a single RV Y then you are basically
computing a probability query for all of the possible realizations
y and selecting the most probable one.
</p>

<p>
Notice that the joint prob. maximizing the likelihood might well
differ from the individual RV maximizing realization.
</p>
</blockquote>


<p>
A <b>second type of query</b> is: <i>Marginal MAP Query</i>:
</p>

<p>
The idea of this is well explained in the book via example.
</p>

<p>
Imagine you have a class of disease. You want to find the most
likely disease given your evidence. Assume that you observe a
subset of symptoms E = e. You want to find the MAP assignment of
the disease Y.
</p>

<p>
The issue is now that you have non-observed symptoms: Z.
</p>

<p>
If you now have a disease that has just a small number of
associated symptoms with high probability, and you observe such
symptoms, then your MAP query will likely select this realization
as most likely.
</p>

<p>
In reality there might well be a more likely realization - i.e. a
different RV that is associated with a lot of symptoms with small
probability. The result is that when taking that into account and
therefore considering the possible influence of non-observed
symptoms the conclusion might be well different.
</p>

<p>
For this it makes sense to consider <i>marginal MAP</i> that tries in
fact to adjust for the presence of the other <b>non-observed RVs
influencing the outcome</b>.
</p>

<p>
\[ marginal MAP (Y | e) = \operatorname*{argmax}_Y  \sum_{Z}{P (Y,
   Z | e)} \]
</p>
</div>

<div id="outline-container-org3d57a81" class="outline-4">
<h4 id="org3d57a81"><span class="todo TODO">TODO</span> Exact Inference</h4>
</div>

<div id="outline-container-orgb78ed94" class="outline-4">
<h4 id="orgb78ed94"><span class="todo TODO">TODO</span> Inference as Optimization</h4>
</div>

<div id="outline-container-orgb627637" class="outline-4">
<h4 id="orgb627637"><span class="todo TODO">TODO</span> Particle Based Approximate Inference</h4>
<div class="outline-text-4" id="text-orgb627637">
<p>
These are essentially the methods you saw in stochastic simulation
course. 
</p>
</div>
</div>

<div id="outline-container-org5dcd01b" class="outline-4">
<h4 id="org5dcd01b"><span class="todo TODO">TODO</span> Map Inference</h4>
</div>

<div id="outline-container-org6bd32d5" class="outline-4">
<h4 id="org6bd32d5"><span class="todo TODO">TODO</span> Inference in Hybrid Models and Temporal Models</h4>
</div>
</div>


<div id="outline-container-org9b4204a" class="outline-3">
<h3 id="org9b4204a">Learning</h3>
<div class="outline-text-3" id="text-org9b4204a">
<p>
I will do now some brief notes on Learning. This will likely be the
matter of my Thesis.
</p>

<p>
It makes sense therefore to focus now on this, given the little
time I have now and as I have to push a bit in order to set things
correctly into the pipeline.
</p>

<p>
Recall that the idea of Learning, is to learn, either (i) the network
structure, or (ii) the parameters of the model or (iii) both, from
the data.
</p>

<p>
In some domains, the amount of knowledge required is just too large
or the expert’s time is too valuable to ask one to set up and
construct all of the network.  In others, there are simply no
experts who have sufficient understanding of the domain.  In many
domains, the properties of the distribution change from one
application site to another or over time, and we cannot expect an
expert to sit and redesign the network every few weeks.
</p>

<p>
For all of these reasons learning model parameters and structure
from the data is particularly important.
</p>

<p>
Formally, we have a distribution P<sup>*</sup> that is induced by a network
M<sup>*</sup> = (K<sup>*</sup>, &theta;<sup>*</sup>). Given a dataset D = (d[1], &#x2026;, d[m]) of M
samples of P<sup>*</sup>. Notice that such data samples are i.i.d. P<sup>*</sup>
distributed. Then given a some model family \(\tilde{M}\) that
defines a probability \(P_{\tilde{M}}\) (or \(\tilde{P}\) when
\(\tilde{M}\) is clear). I.e. we may want to learn only model
parameters for a fixed structure, or some or all of the structure
of the model.
</p>
</div>

<div id="outline-container-orgcbaea4e" class="outline-4">
<h4 id="orgcbaea4e">Goals of Learning</h4>
<div class="outline-text-4" id="text-orgcbaea4e">
<p>
Notice that we want to construct a \(\tilde{M}\) that precisely
represents the distribution P<sup>*</sup>.
</p>

<p>
Because of the limited amount of data and the fact that we might
possibly have to estimate a very high-dimensional distribution it
is clear that in practice we must select an \(\tilde{M}\) that is
just a <b>best</b> approximation of M<sup>*</sup>.
</p>

<p>
To define what a <b>best</b> approximation is, we have to specify the
goals of learning such that we can quantify how well a
distribution approximates.
</p>
</div>


<ul class="org-ul">
<li><a id="orge35347d"></a>On a Precise Density Estimation<br />
<div class="outline-text-5" id="text-orge35347d">
<p>
It is clear that if the goal of setting up a bayesian network is
the one of performing <i>inference</i>, then you might want to
<b>estimate the density at best</b> such that your inference will be the
most precise as possible.
</p>

<p>
I.e. you try to construct a model \(\tilde{M}\) such that
\(\tilde{P}\) is "close" to the generating distribution P<sup>*</sup>.
</p>

<p>
In order to measure how close the two densities lie to each other
you can use the <i>relative entropy distance</i>:
</p>

<img src="../../images/Bildschirmfoto_2021-02-22_um_12.01.01.png" class="center" style = "width: 30% !important;">

<p>
Notice however that in the above you implicitly assume that P<sup>*</sup>
is known. Obviously this is not the case in many practical cases
and it is in fact what we aim to achieve.
</p>

<p>
A solution for this is the following:
</p>

<img src="../../images/Bildschirmfoto_2021-02-22_um_12.21.43.png" class="center">

<p>
Continuing the sentence in the above, the -H<sub>p</sub> term is the
negative entropy above and the second is the <i>expected
log-likelihood</i>. It is immediate to see that the second term is
higher, the higher the probability that \(\tilde{M}\) gives to
points, sampled from the true distribution. 
</p>

<p>
As a consequence of that, it holds however that the
log-likelihood as a metric for comparing one learned model to
another, we cannot evaluate a particular \(\tilde{M}\) in how close
it is to the unknown optimum as we have lost in the above the
baseline $E<sub>P<sup>*</sup>(&xi;)</sub>(log (P<sup>*</sup>(&xi;)) - i.e. the first term
that we ignore as not depending on \(\tilde{P}\).
</p>

<p>
Notice moreover that in our discussion we will be interested in
the <i>likelihood</i> of the data given the model M - i.e. on \(l(D :
     M)\). (recall this notation).
</p>

<p>
Another option for comparing how well a model fits a distribution
is through the notion of <i>loss functions</i> \(loss(\xi : M)\). This
measures the loss a model \(M\) makes on a particular data sample,
i.e. on an instance &xi;.
</p>

<p>
Assume that you take loss function is expressed as the <i>negative
log-likelihood</i>, i.e. \(loss(\xi : M) = -
     \sum^{M}_{m=1}log(P(\xi[m]) : M)\).
</p>

<p>
Then it holds for the <i>expected loss</i>:
</p>

<img src="../../images/Bildschirmfoto_2021-02-22_um_12.54.30.png" class="center">
</div>
</li>


<li><a id="org0e3a82e"></a>Specific Prediction Tasks<br />
<div class="outline-text-5" id="text-org0e3a82e">
<p>
Notice that when assuming that you want to learn the model to
perform probabilistic inference, you implicitly state that your
aim is to make conclusions on the overall distribution <i>P<sup>*</sup></i>.
</p>

<p>
I.e. in such a case you are interested in evaluating the
probability of a full instance &xi;, i.e. the probability of an
occurrence/sample over/of the entire network.
</p>

<p>
In contrast to this setting in many situations we might be
interested in answering a whole range of queries of the form
P(Y | X).
</p>

<p>
For instance in a classification task we might be interested in
selecting an Y given X. We can then work in such a case with a
MAP assignment to Y, i.e.
</p>

<p>
\[h_{\tilde{P}} = \operatorname*{argmax}_y \tilde{P} (y | x)\]
</p>

<p>
We might then act similarly for other cases.
</p>

<p>
We might even use classification errors such as the standard <code>0/1
     loss</code>.
</p>

<p>
Another option is to focus on the general extent to which our
learned model is able to predict data generated from the
distribution.
</p>

<img src="../../images/Bildschirmfoto_2021-02-22_um_14.20.47.png" class="center" style = "width: 30% !important;">

<p>
Notice that it is immediate to see that if we <i>negate</i> the above
we immediately obtain a loss function to compute an empirical
estimate by taking the average relative to a data set.
</p>
</div>
</li>


<li><a id="orgb4abd2b"></a>Knowledge Discovery<br />
<div class="outline-text-5" id="text-orgb4abd2b">
<p>
This is another possible goal in comparison to probabilistic
inference. Here the idea is that you want to understand important
properties of the domain by observing P<sup>*</sup>.
</p>

<p>
I.e. what are the direct and indirect dependencies, what
characterizes the nature of the dependencies and so forth.
</p>

<p>
Of course, simpler statistical methods can be used to explore
the data, for example, by highlighting the most significant
correlations between pairs of variables. However, a learned
network model can provide parameters that have direct causal
interpretation and can also reveal much finer structure, for
example, by distinguishing between direct and indirect
dependencies, both of which lead to correlations in the
resulting distribution.
</p>

<p>
Notice that such a task requires a very different approach in
comparison to the prediction task.
</p>

<p>
In this setting, we really do care about reconstructing the
<b>correct model</b> \(M^*\). While before we could well have distorted
reconstructed model \(\tilde{M}\) as long as we would induce a
<b>distribution</b> similar to the one induced by \(M^*\).
</p>

<p>
So in this task we are not interested in some metric stating the
difference in the distributions defined by the models but rather
as a measure of success we should take directly something
representing the distance between \(\tilde{M}\) and \(M^*\).
</p>

<p>
This is however <b>not always achievable</b>. Even, with a large
amounts of data, the true model might not be
<i>identifiable</i>. Recall in fact that for instance the <code>network
     structure</code> itself \(K^*\), might not be well identifiable due to
the I-map discussion of the representation chapter. The best we
can achieve in this sense is to recover an I-equivalent
structure.
</p>

<p>
Such problems are exacerbated when data is limited. It might be
difficult to detect the correlation of two nodes that are in fact
related in the true model and distinguish it from some spurious
correlation in the data. Note that such a limit is less prominent
in a <b>density estimation task</b>. The reasoning is that as if the
correlation does not appear in the data than it is likely to be a
weak one.
</p>

<p>
The relatively high probability of making model identification
errors can be significant if the goal is to discover the correct
structure of the underlying distribution. So here it is important
to make some <b>confidence</b> statement about the inferred
relationship.
</p>

<p>
Thus, in a knowledge discovery application, it is far more
critical to assess the confidence in a prediction, taking into
account the extent to which it can be identified given the
available data and the number of hypotheses that would give rise
to similar observed behavior. On how to deal with it will be
analyzed in the next sections.
</p>
</div>
</li>


<li><a id="org98b8464"></a>On Learning as an Optimization Task<br />
<div class="outline-text-5" id="text-org98b8464">
<p>
Notice that in the above sections we defined some numerical
criterion to define the extent to which the distributions are
comparable to each other.
</p>

<p>
Given such numerical measures that we wish to mini- or maximize,
it follows immediately that learning can be generally seen as an
<i>optimization</i> exercise.
</p>

<p>
We have in fact a <i>hypotheses space</i>, that is a set of candidate
models and an objective function that we aim to optimize. So the
learning task essentially amounts to find a high-scoring model
within our model class.
</p>

<p>
In the next section we go a bit deeper and analyze the
ramifications of choosing one objective function over the other.
</p>
</div>


<ul class="org-ul">
<li><a id="org312b2fa"></a>Empirical Risk and Overfitting<br />
<div class="outline-text-6" id="text-org312b2fa">
<p>
As said one of the objective functions we might have is the
expected loss.
</p>

<p>
I.e. we are interested in minimizing \(E_{\xi \sim
      P^*}[loss(\xi : M)]\).
</p>

<p>
Notice, that as mentioned before as we do not know the
distribution P<sup>*</sup> we work with the empirical distribution
\<sup>P<sub>D</sub></sup>, this is given for an event <i>A</i> as follows:
</p>

<p>
\[\^{P_D}(A) =  \frac{1}{M} \sum_m 1_{\xi{m} \in A} \]
</p>

<p>
i.e. the empirical distribution is the count of instances that
are elements of the event <i>A</i> over all of the instances sampled
M. It is therefore the usual frequency.
</p>

<p>
Notice that as the number of training samples grows the
empirical distribution approaches the true distribution. This
due to the LLN.
</p>

<p>
So as you have not have no knowledge about the true P<sup>*</sup>, you use
\<sup>P<sub>D</sub></sup> as your true distribution and compute the empirical loss
over it.
</p>

<p>
However, note that there are important limits in such
approach. The dimension of Bayesian Networks distribution
increases exponentially in the number of nodes - i.e. especially
when nodes have multiple parents.
</p>

<p>
Consider for instance the following:
</p>

<img src="../../images/Bildschirmfoto_2021-02-26_um_11.03.43.png" class="center">

<p>
Moreover, recall when using the empirical distribution the usual
issue of overfitting. I.e. it might be easy to get very high
accuracy given a possibly large number of parameters. Notice
however that the empirical dist does not have to be 100%
representative for the true distribution as discussed above. So
recall that.
</p>


<p>
Recall the standard <b>bias-variance trade off</b> in this sense. We
are in the following <i>standard statistical dilemma</i>.
</p>

<img src="../../images/Bildschirmfoto_2021-02-26_um_11.21.58.png" class="center">

<p>
So generally we must take care <b>not to allow a too rich class of
possible models</b>. As in fact the error introduced by variance
may be larger than the potential error introduced by bias.
</p>

<p>
So in general recall that <b>restricting the space of possible
models</b> \(\tilde{M}\) we might have a worse performance on the
training objective, but a better distance to P<sup>*</sup> - our true
objective.
</p>

<p>
The issue however, is that you are not sure in which case you
are. It is therefore best practice to use a <b>regularization</b>
technique to counter-balance such effects. You saw examples of
these multiple times in your courses: LASSO, Ridge, Information
Criteria etc. etc.
</p>

<p>
In the case of Bayesian Network it is common practice <b>not to
just use one of these self-determining</b> regularization
techniques but rather also setting ex-ante <b>hard-constraints</b> on
the model class.
</p>

<p>
So in general when performing our Maximization exercise we use a
combination of the two, i.e.
</p>

<p>
(i) on the one hand, you set hard constraints on the model class
</p>

<p>
(ii) on the other hand, we use an optimization objective
function that <i>leads us away from overfitting</i>.
</p>
</div>
</li>


<li><a id="org3b36c79"></a>Validation (Interesting formalization of PAC bounds)<br />
<div class="outline-text-6" id="text-org3b36c79">
<p>
You can then test how well you achieved your results using
cross-validation and holdout standard techniques.
</p>

<p>
Notice that another interesting technique to determine how well
the learned model is performing is to compute <b>PAC bounds</b>. This
is something I did not encounter in my memory under this name in
my Stat courses.
</p>

<p>
Recall that we cannot guarantee with certainty the quality of
the learned model. Recall that the data set D is sampled
stochastically from P<sup>*</sup>. There is always chance that we would
have "bad luck" and sample a very unrepresentative data set so
that our empirical distribution will be very off. So with <b>PAC
bounds</b> that is probably approximately correct bounds, we want
to compute the probability of such very bad outcome and make
sure it is small enough.
</p>

<p>
I am pretty sure that this is what we did in the class of
distribution system without stating this explicitly under this
name. There we also saw computed the probability of very bad
outcomes when implementing shared coin algorithms.
</p>

<p>
So formally our goal is to prove that our learning procedure is
probably approximately correct, that is for <b>most training sets
D</b> the learning procedure will return a model whose error is
low.
</p>

<p>
I.e. assume the <i>relative entropy</i> as the loss function. Let
P<sup>*</sup><sub>M</sub> be the distribution over the data sets D of size M
sampled IID from P<sup>*</sup>. Now assume that a <i>given learning
procedure L</i> returns a model M<sub>L(D)</sub>. We search for <b>PAC bounds</b>
a results of the forms:
</p>

<img src="../../images/Bildschirmfoto_2021-02-26_um_16.18.45.png" class="center">

<p>
The number of samples M required to achieve such a bound is then
called the <b>sample complexity</b>.
</p>

<p>
<b>Note</b> - Important when computing PAC bounds is the following,
and that has well to be kept in mind:
</p>


<img src="../../images/Bildschirmfoto_2021-02-26_um_16.21.47.png" class="center">
</div>
</li>


<li><a id="orgadb25a4"></a>Discriminative vs Generative Training<br />
<div class="outline-text-6" id="text-orgadb25a4">
<p>
This goes back to the previous analysis. We discussed how we
might well be interested in a precise density estimation of the
<b>entire distribution</b> P<sup>*</sup>. However, we might also be interested
in some more <b>specific prediction task</b>.
</p>

<p>
Depending on it we have the different notion of discriminative
vs generative training. Consider in fact the case where we
want the model to perform well on a particular task, such as
predicting \(X\) from \(Y\).
</p>

<p>
Then, on the one hand, when we want to estimate the <i>entire
distribution</i> we want a <i>training regime</i> that would aim to get
the model \(\tilde{M}\) close to the overall joint distribution
P<sup>*</sup>(X, Y). This type of training is termed <b>generative
training</b>. This because we use use the trained model to
<i>generate</i> all of the variables.
</p>

<p>
Alternatively we can train the model <i>discriminatively</i>. Our
goal then is to get \(\tilde{P}(Y|X)\) to be close to
\(P^{*}(Y|X)\).
</p>

<p>
Note that a model that is trained generatively can still be
used for the specific prediction tasks. However, the converse
does not hold true.
</p>

<p>
Note, that on top of it discriminative training is less
appealing in Bayesian Networks as most of the computational
properties that facilitate Bayesian network learning do not
carry through to discriminative training. For this reason this
form of training is usually <b>performed in the context of
undirected models</b>.
</p>

<p>
Notice finally that there is a series of trade-off to consider
when deciding among <i>discriminative</i> and <i>generative</i> models.
</p>

<ul class="org-ul">
<li><p>
Generally, generative models have <i>higher bias</i>. Make more
assumptions about the form of the distribution. They encode
independence assumptions about the feature variables X. In
contrast discriminative models just make independence
assumptions on Y and about their dependence with X.
</p>

<p>
Notice that this additional bias may not have to be strictly
bad. It might help for standard bias-variance trade
off. I.e. it might help to regularize and constrain the
learned model. On the other hand you might get hurt due to
the additional constraint you would set estimating the entire
model when your interest is on some conditional setting.
</p>

<p>
Note, moreover, that as the amount of data grows the bias
effect would dominate as the variance would decrease.
</p>

<p>
Therefore as discriminative tend to be less affected by
incorrect model assumptions and because they will often
outperform the generatively trained models for large data
sets.
</p></li>
</ul>
</div>
</li>
</ul>
</li>


<li><a id="org8014d9f"></a>On Learning Tasks - Flavors determining different class of tasks<br />
<div class="outline-text-5" id="text-org8014d9f">
<p>
We will look here in greater detail to the different variants of
the learning tasks.
</p>

<p>
The input of the learning procedure is the following:
</p>

<ul class="org-ul">
<li>Some prior knowledge or constraints about \(\tilde{M}\)</li>

<li>A set D of data instances {d[1], &#x2026;, d[M]}. Note again that as
used throughout these notes a data instance is a data sample
over the entire network.</li>
</ul>

<p>
The output is then the model \(\tilde{M}\), that might include
structure, parameters or both.
</p>

<p>
There are many variants of this fairly abstract learning
problem. These depends on:
</p>

<ul class="org-ul">
<li>the extent of the constraints hat we are given about \(\tilde{M}\)</li>

<li>the extent to which the data are fully observed</li>
</ul>
</div>

<ul class="org-ul">
<li><a id="orgf170e76"></a>On Model Constraints<br />
<div class="outline-text-6" id="text-orgf170e76">
<p>
A non-reducing analysis is impossible as there is theoretically
an unbounded set of options here.
</p>

<p>
The most common constraints on our model are \(\tilde{M}\) are
the following:
</p>

<ul class="org-ul">
<li>we are given a graph structure and we only have to learn some
of the parameters. Here we do not assume that the graph
structure corresponds to the correct one, i.e. to K<sup>*</sup>.</li>

<li>we do not know the structure and have to learn both parameters
and structure.</li>

<li>we may not know the <b>set of variables</b> over which the
distribution P<sup>*</sup> is defined. I.e. we might observe just a
<i>subset</i> of these variables.</li>
</ul>

<p>
Notice that no matter the case the less prior knowledge we are
given, the <i>larger the hypotheses space</i> and the more
possibilities we need to consider when choosing the model.
</p>

<p>
Recall that for larger models you face the statistical trade off
of <i>variance-bias</i> discussed before.
</p>

<p>
Moreover, on top of that it is important to see that there is a
<i>computational trade-off</i> as well: the richer the hypotheses
space, the more difficult the search to find a high-scoring
model.
</p>
</div>
</li>

<li><a id="orgdf85c3f"></a>On Data Observability<br />
<div class="outline-text-6" id="text-orgdf85c3f">
<p>
Notice that in the thesis you will move in this direction. Here
the case is whether we are always able to observe the
realization of a RV of the bayesian network or not.
</p>

<p>
I.e. there are essentially <i>two cases</i>:
</p>

<ol class="org-ol">
<li>complete data: in this case data are <i>fully observed</i>. I.e. for
each of our training instances d[m] is a full instantiation in
all of the variables spanning our bayesian network.</li>

<li>incomplete data: in this case data are <i>partially
observed</i>. In each training instance d[m], some variables are not
observed.</li>

<li>hidden variables: here the values of such variables is <b>never
observed</b> in any training instance d[m]. This can represent
both the case of some RV that is not modeled in the bayesian
network as we do not know of its existence, as well as the
case when we know of the existence of such RV but we never
have the chance to observe it.</li>
</ol>

<p>
Note that as we will see the usual technique to deal with
unobservable data is the one of <i>hypothesize possible values</i>
for these variables.
</p>

<p>
The greater the extent to which <b>data are missing</b>, the less we
are able to hypothesize reliably values for the missing entries.
</p>

<p>
Note, moreover, that modeling hidden variables is important for
knowledge discovery. These are important to understand and
properly map the relation among variables. Think for instance in
a medical application, you want to make sure that you ascribe
the correct relation among two RV.
</p>

<p>
Note, furthermore, that modeling hidden variables might highly
reduce the complexity of the model. It is in fact possible to
leverage conditional independencies given hidden variables. To
make that visual consider the following:
</p>

<img src="../../images/Bildschirmfoto_2021-03-01_um_11.05.36.png" class="center">
</div>
</li>

<li><a id="org48de6b3"></a>Taxonomy of Learning task<br />
<div class="outline-text-6" id="text-org48de6b3">
<p>
Depending on the dimension of the axis described above - i.e. on
the specifics of the learning task we might end up in different
situations.
</p>

<p>
I.e.
</p>

<ul class="org-ul">
<li>in the case of parameter learning <b>with known structure</b>, the
problem is of the class of <b>numerical optimization</b>.</li>

<li>in the case of bayesian learning with <b>fixed structure and
with complete data</b>, the parameter learning is easily solved
and it is sometimes even possible to find closed form
solutions. I.e. no need to go in the numerical optimization
dimension. Even when this is not the case the optimization
problem is convex and can be solved easily by iterated
numerical optimization algorithms. Note, however that such
algorithm often requires <i>inference over the entire network</i>.</li>

<li>in the case the <b>structure is not given</b>, the problem
incorporates an additional level of complexity. The hypotheses
space contains an enormous - generally super-exponentially
large - set of possible structures. here the problem of
<b>structure selection</b> is generally also formulated as an
optimization problem. I.e. we have different structures and we
assign a score to them w.r.t. how well they manage to map the
information of interest. We aim then to find the network whose
score is the highest and this is essentially the <i>optimization
task</i>. Note that the <i>score</i> assigned to each network can be
computed in <i>closed form</i>.</li>

<li>in the case of <b>incomplete data</b> the problem is nasty. Here
multiple hypotheses regarding the values of unobserved
variables give rise to a <b>combinatorial range of different
alternative models</b>. This induce a non-convex, <b>multimodal
optimization problem</b>. In order to solve this it is common
practice to use the EM-algorithm. This suffers from the fact
that requires usually <b>multiple calls to inference as a
subroutine</b> making the process expensive for large
networks. Note, that if the <b>structure on the top of it is not
known</b>, it is even harder to come to a solution since we need
to <b>combine a discrete search over the network structure with a
non-convex optimization problem over the parameter space</b>.</li>
</ul>


<p>
Given this general outline it is now possible to go in the
different outlined cases in turn and see how to deal with them.
</p>
</div>
</li>
</ul>
</li>
</ul>
</div>


<div id="outline-container-org430a4ab" class="outline-4">
<h4 id="org430a4ab">Parameter Estimation with Complete Data</h4>
<div class="outline-text-4" id="text-org430a4ab">
<p>
So we start here with the first learning case. I.e. we start with
parameter learning in the case of <b>known network structure</b> and
<b>fully observed instances</b> \(D = {\xi[1], ..., \xi[m]}\).
</p>

<p>
Note, that this is also the building block for both structure
learning as well as learning from incomplete data.
</p>

<p>
There are in general two approaches to dealing with such parameter
estimation tasks:
</p>

<ol class="org-ol">
<li>Maximum Likelihood Estimation</li>

<li>Bayesian Appraoch</li>
</ol>

<p>
We will explore the two next the two cases. We will start easy,
that is with the case of a bayes network with a single
variable. We will then expand and discuss about the generalization
of the theory to arbitrary network structures. We will work with
parameter estimation in the context of structured CPDs.
</p>
</div>

<ul class="org-ul">
<li><a id="org3a30be2"></a>Maximum Likelihood - Parameter Estimation<br />
<div class="outline-text-5" id="text-org3a30be2">
<p>
So consider for the simple network with one random variable
consider the case of throwing a thumbtack - see below.
</p>

<p>
This can either fall head or tail.
</p>

<p>
The goal is to predict the probability through which the object
falls heads &theta; or tails (1 - &theta;). So in general it is to
estimate the probability of &theta;.
</p>

<img src="../../images/Bildschirmfoto_2021-03-01_um_16.18.52.png" class="center">

<p>
As you know then the MLE consists on taking the parameter for
which the likelihood is maximized. Consider for instance M = 100,
and head = 0.3, then it is straightforward to set &theta; = 0.3
and that is in fact the MLE. You can then calculate the
confidence interval via standard asymptotic theory. See the
section on asymptotic theory of your fundamentals of mathematics
statistics if interested. Or your notes on econometrics II.
</p>

<p>
To formalize the above straightforward. Consider a set of tosses
x[1], &#x2026;, x[M] that are IID. Each of these tosses is H (heads),
T (tails) with prob. &theta;, (1 - &theta;) respectively.
</p>

<p>
We define the <i>hypotheses space</i> &Theta; - i.e. the set of
parameterization we are considering - and an <i>objective function</i>
that tells us how good different hypotheses are relative to the
data D.
</p>

<p>
In MLE we then take as the objective function the likelihood of
the observations given the parameterization.
</p>

<p>
It follows immediately that given the independence property of
the realizations such objective function, i.e. the <i>likelihood</i>
is given by the following:
</p>

<img src="../../images/Bildschirmfoto_2021-03-01_um_16.41.02.png" class="center">

<p>
So that basically that is very standard intro statistical
material.
</p>

<p>
Let's leave this 101 example and let's start to consider MLE and
set it into the bayes networks frame.
</p>
</div>

<ul class="org-ul">
<li><a id="org6b464f4"></a>MLE in Bayesian Networks<br />
<div class="outline-text-6" id="text-org6b464f4">
<p>
Consider the case of a set \(\mathscr{X}\) of random variables, with an
unknown distribution \(P^{*}(X)\). We know the sample space,
i.e. the random variables and their domain.
</p>

<p>
We denote the <i>training set</i>  of samples as \(D\) and assume that
it consists of \(M\) instances (i.e. recall 1 instance = 1 sample
over the entire network; i.e. 1 realization for the entire
network) of \(\mathscr{X}\), that is &xi;[1], &#x2026;, &xi;[M].
</p>

<p>
Next we assume a <i>parametric model</i> which is defined by a
function \(P (\xi : \theta)\), i.e. read by a probability function
of an instance realization <i>given</i> a set of parameters &theta;.
</p>

<p>
We require than that for each choice of &theta;:
</p>

<p>
\[ \sum_{\xi} P(\xi : \theta)  = 1 \]
</p>

<p>
The parameter space &Theta; excludes parameterizations that <i>do
not satisfy</i> the above.
</p>

<p>
Then depending on your modeling of the space of your X you might
come up with different parametric models and possible
&Theta;. Again up to know nothing new. These are well known
things to you.
</p>

<p>
Note now that since in MLE we maximize the <i>objective
function</i> we ideally want it to be continuous - and possibly
smooth - over &theta;.
</p>

<p>
To ensure these properties, most of the theory usually require
the likelihood function to be continuous and differentiable in
&theta;.
</p>

<p>
Recall, that given the definitions of above the likelihood
function is defined as:
</p>


<p>
\[ L(\theta : D) = \prod_m P(\xi[m] : \theta)  \]
</p>


<p>
Such that the MLE is defined as:
</p>

<p>
\[ L(\hat{\theta} : D) = \operatorname*{max}_{\theta \in \Theta}
      L(\theta : D) \]
</p>

<p>
Given the proper definition for the problem, the question that
remains open is simply on how to specify the above properties in
the case of <b>Bayesian Networks</b>.
</p>

<p>
In order to give the idea we will consider the most easy
possible network. We consider the case of the set \(\mathscr{X}\)
of variables containing two random variables \(X\) and \(Y\).
</p>

<p>
Among the variables there is the following network structure:
</p>

<img src="../../images/bayesNet4.png"  style = "width: 30% !important;">

<p>
So recall that essentially our problem amounts in finding the
formula for the <i>objective function</i>, i.e. the <i>likelihood
function</i> in our network and to maximize it.
</p>

<p>
Note, that in the case of bayesian networks, our network is
parameterized by a vector containing all of the probabilities
in the different CPDs. So in our case &theta; is a vector
containing such probabilities.
</p>

<p>
In the above toy example with \(X, Y\) being a binary RVs, the
parameterization would consist of:
</p>

<ul class="org-ul">
<li>&theta;<sub>x<sup>1</sup></sub> , &theta;<sub>x<sup>0</sup></sub> that specify the two
probabilities of X</li>

<li>the conditional probabilities of the RV \(Y\),
i.e. &theta;<sub>y<sup>1</sup> | x<sup>1</sup></sub>, &theta;<sub>y<sup>0</sup> | x<sup>1</sup></sub>,
&theta;<sub>y<sup>1</sup> | x<sup>0</sup></sub>, &theta;<sub>y<sup>0</sup> | x<sup>0</sup></sub></li>
</ul>

<p>
Note that in the following we us &theta;<sub>Y | x<sup>1</sup></sub> to refer to
the set {&theta;<sub>y<sup>1</sup> | x<sup>1</sup></sub>, &theta;<sub>y<sup>0</sup> | x<sup>1</sup></sub>} and
&theta;<sub>Y | x<sup>0</sup></sub> to refer to the set {&theta;<sub>y<sup>1</sup> | x<sup>0</sup></sub>,
&theta;<sub>y<sup>0</sup> | x<sup>0</sup></sub>}. The union of these sets represents
then &theta;<sub>Y | X</sub>.
</p>

<p>
Given this and the conditional joint probability factorization
it follows that it is possible to write the probability in the
following form:
</p>


<img src="../../images/Bildschirmfoto_2021-03-05_um_10.26.08.png" class="center">

<p>
<b>Important</b> is then to see that the likelihood decomposes into
<i>two separate terms</i>. Each of this terms is a <b>local likelihood
function</b> that measures how will it predicts given the
parents.
</p>

<p>
Given this, we can quickly argue for the first very important
property of the MLE in Bayesian Networks, i.e. the
<b>decomposability property</b> of the likelihood function.
</p>

<p>
In order to see this consider the above local likelihoods. It
is then immediate to understand that each of the local
likelihood terms just depends on the local parameters for that
CPD entry. That is, for the P(x[m] : &theta;) entries the
parameter of interest is &theta;<sub>X</sub> and for the P(y[m] | x[m] :
&theta;) entries the parameter of interest are &theta;<sub>Y | X</sub>.
</p>

<p>
Given this realization it follows that 
</p>


<img src="../../images/Bildschirmfoto_2021-03-05_um_10.46.20.png" class="center">

<p>
Note now the following notation that will be used throughout
the book. Given the above the question is on how it is possible
to express
</p>

<p>
\[ \prod_{m : x[m] = x^0} P(y[m] | x[m] : \theta_{Y | x^0} ) \]
</p>

<p>
The idea is then to count both the occasions where x[m] = 0 and
y[m] = 1; and represent such number by M[x<sup>0</sup>, y<sup>1</sup>]. As well to
count the occasions where x[m] = 0 and y[m] = 0 and represent
such number as M[x<sup>0</sup>, y<sup>0</sup>].
</p>

<p>
It follows then immediately that the above reduces to:
</p>

<p>
\[ \prod_{m : x[m] = x^0} P(y[m] | x[m] : \theta_{Y | x^0} ) =
       \theta_{y^1 | x^0}^{M[x^0, y^1]} * \theta_{y^0 | x^0}^{M[x^0,
       y^0]}\]
</p>

<p>
It is then immediate to see that the above takes the form of a
multinomial likelihood function and we know that for it the
maximizing parameter solution takes the form of:
</p>


<img src="../../images/Bildschirmfoto_2021-03-05_um_11.08.11.png" class="center">

<p>
Note now that despite the fact that the above was a very toy
model, such <b>decomposability</b> property holds in general for
Bayesian Networks such that it is easy to work with the above.
</p>

<p>
I.e. in general the following holds:
</p>

<p>
Take a network structure \(\mathscr{G}\) with parameter &theta;.
</p>

<p>
Take on top of it a dataset \(\mathscr{D}\) consisting of sample
instances \(\xi[1], ..., \xi[M]\).
</p>

<p>
Given that, we can work again as in the above case, that is
first decompose the likelihood of an instance leveraging the
conditional factorization of the joint likelihood of the
instance.
</p>

<p>
Then, in a second step, exchange the order of multiplication,
such that you have the product of <b>local likelihoods</b>
</p>


<img src="../../images/Bildschirmfoto_2021-03-05_um_11.24.04.png" class="center">


<img src="../../images/Bildschirmfoto_2021-03-05_um_11.24.20.png" class="center">


<p>
<b>Important</b> is therefore to understand that the likelihood
decomposes as a product of <b>independent</b> terms, one for each
CPD in the network. This is an important property called
<b>global decomposition</b> property.
</p>

<p>
Note moreover, that if the &theta;<sub>X<sub>i</sub> | Pa<sub>X<sub>i</sub></sub></sub> are <b>disjoint</b>
then the parameter vector maximizing the global likelihood can
be easily computed:
</p>


<img src="../../images/Bildschirmfoto_2021-03-05_um_11.31.45.png" class="center">

<p>
You can then read an example in the book for the case of table
CPD if you are interested. Note that the above holds in general
for multiple CPD specifications.
</p>
</div>
</li>

<li><a id="org67a2336"></a>On Non-parameteric Models<br />
<div class="outline-text-6" id="text-org67a2336">
<p>
Note finally that although the chapter in the book deals with
parametric CPDs; in their multinomial form - as above - and as
linear Gaussians, a growing interest in the use of
<b>nonparameteric</b> Bayesian estimation methods arose.
</p>

<p>
Here, the (conditional) distribution is not defined to be in
some particular parametric class with a fixed number of
parameters, but rather the complexity of the representation is
allowed to grow as we get more data instances.
</p>

<p>
Note that in the case of discrete variables, any CPD can be
described as a table - maybe a very large one as discussed in
the representation section. In this sense, there is less need
for a non-parametric representation. In contrast in the
continuous case there is not a general "universal" parametric
distribution that is able to cover all of the possible
continuous distributions.
</p>

<p>
In this sense, instead of representing some continuous
distribution with a parameteric distribution, that might fit
well it might be sensible to use a non-parameteric
approach. If interested check in the book for more.
</p>
</div>
</li>
</ul>
</li>


<li><a id="orge735395"></a>Bayesian - Parameter Estimation<br />
<div class="outline-text-5" id="text-orge735395">
<p>
Note that here, we will start again with a 101 bayesian example.
I.e. we will look again at the Thumbtack example. We will then
generalize on that and look at the general case for bayesian
parameter estimation.
</p>

<p>
Recall that in Bayesian Stat you encode the prior knowledge about
&theta; with a probability distribution. This distribution represents
how likely we are a priori to believe the different choices of
parameters.
</p>

<p>
Once we quantify our knowledge (or lack thereof) about possible
values of &theta;, we can create a <b>joint distribution</b> over the
parameter &theta; and the data cases that we are about to observe X[1],
. . . , X[M]. This joint distribution captures our assumptions
about the experiment.
</p>

<p>
Let's go back to the thumbtack. Say that &theta; is the
probability of head. Recall that previously we assumed that
tosses are independent of each other. Note, however, that this
assumption was made when &theta; was fixed. If we do not know
&theta;, then the tosses are not marginally independent: Each toss
tells us something about the parameter &theta;, and thereby about
the probability of the next toss.
</p>

<p>
Note however that given &theta;, we cannot learn about the outcome
of one toss from observing the results of others. So we say that
the tosses are <i>conditionally independent</i> given &theta;.
</p>


<img src="../../images/Bildschirmfoto_2021-03-05_um_13.06.13.png" class="center">

<p>
Given this specification, our joint distribution is specified up
to P(&theta;). This is the <i>prior</i> distribution we assign to the
RV &theta;.
</p>

<p>
Given all of the above it follows that we have the following
<i>network structure and joint probability factorization</i>:
</p>

<img src="../../images/Bildschirmfoto_2021-03-05_um_13.09.34.png" class="center">

<p>
In practice we can then apply the Bayes Formula to get to the
posterior distribution of our parameter of interest given some
sample x[1], &#x2026;, x[M].
</p>

<p>
This is then the major difference with MLE. We use the posterior
<i>distribution</i> to compute the parameterization for our network
instead of selecting a single value for the parameter &theta;.
</p>

<p>
In order to see this, consider the case where you want to
estimate the probability of the next coin x[M+1] being head. Then
you can <b>integrate</b> over the entire distribution of &theta; instead
of relying on a single value.
</p>

<p>
The idea is that $ P(a | b) = &int; P(a, c |b) dc = &int; P(a |
c,b) * P(c | b) dc$ 
</p>

<p>
I.e. you can compute it by:
</p>

<img src="../../images/Bildschirmfoto_2021-03-05_um_15.02.02.png" class="center">

<p>
Note that the last equality above uses the fact that for the
network described conditioning on &theta; we cover the information
deriving from x[1], &#x2026;, x[M].
</p>

<p>
Going back to the thumbtack example this translates into the
following:
</p>

<img src="../../images/Bildschirmfoto_2021-03-05_um_15.10.51.png" class="center">

<p>
Note then that it follows a section in the book where you make
the same exercise for different priors. In particular with a Beta
distribution.
</p>

<p>
I skip it here as you had extensive training in bayesian in your
master and you should be able to replicate that without big
issues.
</p>

<p>
Note, moreover that there is some heuristic reasoning on how you
should choose the hyperparameters for your beta. Go read if
interested.
</p>

<p>
We turn now to a generalization of the above for bayesian
networks leaving the 101 example. 
</p>
</div>

<ul class="org-ul">
<li><a id="orga723018"></a>General Case and a Sufficient Statistics Example<br />
<div class="outline-text-6" id="text-orga723018">
<p>
Assume a general learning problem with training set
\(\mathscr{D}\), that contains iid M samples of a set tof random
variable \(\mathscr{X}\) from an unknown distribution <i>P<sup>*</sup>(X)</i>.
</p>

<p>
Also assume a parametric model \(P(\xi | \theta)\), where the
parameter belong to the space &Theta;.
</p>

<p>
Recall that MLE search the point estimate \(\hat{\theta}\) in
&Theta; maximizing the likelihood of the data. Recall that in
contrast in bayesian we include our prior belief in the model
treating the parameter itself as a RV.
</p>

<p>
We therefore do not simply have a likelihood of observing some
data given a fixed parameterization but we rather have a joint
probability of observing some data and parameterization.
</p>

<p>
So here the joint probability specifying the model is given by
\(P(D, \theta) = P(D | \theta) * P(\theta)\).
</p>

<p>
Given this we can compute the marginal likelihood of the data -
i.e. integrating the parameter out - as
</p>

<p>
\[ P(D) = \int_{\Theta} P(D | \theta) P(\theta) d\theta \].
</p>

<p>
So far therefore nothing new in comparison to the 101 case. Just
written under more general form.
</p>

<p>
We turn next to a particularly useful parameterization for the
bayesian network. I.e. we will show that when working under
particular prior settings we might end up with sufficient
statistics for which it is possible to <b>compactly represent
posterior distribution</b>.<sup><a id="fnr.1" class="footref" href="#fn.1">1</a></sup> This will ultimately lead to the
prediction task of P(x[M+1] | D) which is especially easy to
compute.
</p>

<p>
In order to see this consider the following learning problem -
i.e. the one of <i>uncertainty about the parameters of a
multinomial distribution</i>. The parameter space &Theta; is the one
of all nonnegative vectors &theta; = (&theta;<sub>1</sub>, &#x2026;, &theta;<sub>K</sub>)
such that \(\sum_k \theta_k = 1\).
</p>

<p>
The likelihood in this model has the form
</p>

<p>
\[ L(\theta : D) = \prod_k \theta_k^{M[k]}  \]
</p>

<p>
One conjugate prior to such multinomial likelihood is the
<i>Dirichlet</i> distribution.
</p>


<img src="../../images/Bildschirmfoto_2021-03-05_um_16.47.14.png" class="center">

<p>
Given such model specification it follows now the following.
</p>

<img src="../../images/Bildschirmfoto_2021-03-05_um_16.54.47.png" class="center">

<p>
This prediction is similar to prediction with the MLE
parameters. The only difference is that we added the
hyperparameters to our counts when making the prediction. For
this reason the Dirichlet hyperparameters are often called
<i>pseudo-counts</i>. We can think of these as the number of times we
have seen the different outcomes in our prior experience before
conducting our current experiment.
</p>

<p>
Note moreover the following interesting interpretation for
model:
</p>

<img src="../../images/Bildschirmfoto_2021-03-05_um_17.07.11.png" class="center">

<p>
Note; you should read the <i>mean prediction</i> above, simply as:
$\frac{\alpha_k}{\alpha} $.
</p>

<p>
Note moreover that the prediction in the above case is therefore
nothing more than a weighted average (convex combination) of the
prior mean and the MLE estimate.
</p>

<p>
The combination weights are determined by the relative magnitude
of &alpha; — the confidence of the prior (or total weight of the
pseudo-counts) — and M — the number of observed samples. Note
finally that there is convergence to MLE when the sample size M
grows to infinity.
</p>
</div>
</li>

<li><a id="org76c401b"></a>Bayesian Parameter Estimation in Bayesian Networks<br />
<div class="outline-text-6" id="text-org76c401b">
<p>
Note one final time that in the case of bayesian parameter
estimation we must specify a join distribution the data
instances and the unknown parameters.
</p>

<p>
Again as in the case of MLE we will start from a very basic
trivial network and we will generalize then to a global
Decomposition.
</p>

<p>
<b>The simple Case</b>
</p>

<p>
Again, we consider the case of the set \(\mathscr{X}\) of
variables containing two random variables \(X\) and \(Y\).
</p>

<p>
Among the variables there is the following network structure:
</p>

<img src="../../images/bayesNet4.png"  style = "width: 30% !important;">

<p>
We have training observations X[m] and Y[m] for m = 1, &#x2026;,
M. In addition we have unknown parameter vectors &theta;<sub>X</sub> and
&theta;<sub>Y|X</sub>.
</p>

<p>
Note that the meta-network might be represented as follows:
</p>

<img src="../../images/Bildschirmfoto_2021-03-05_um_17.28.09.png" class="center">

<p>
Note that, as in our simple thumbtack example, the instances are
independent given the unknown parameters.  examination of active
trails shows that X[m] and Y [m] are d-separated from X[m'] and
Y[m'] once we observe the parameter variables.
</p>

<p>
Moreover, we assume that that the individual parameters <b>are a
priori independent</b>. We believe that knowing the value of one
parameter tells us nothing about another one.
</p>

<p>
Formally:
</p>

<img src="../../images/Bildschirmfoto_2021-03-05_um_17.39.17.png" class="center">

<p>
Thus, although we use the global parameter independence in much
of our discussion, it is not always appropriate.
</p>

<p>
Once you accept global parameter independence an important
property emerges:
</p>

<p>
<b>Important - Important - Important:</b>
</p>

<img src="../../images/Bildschirmfoto_2021-03-05_um_17.48.55.png" class="center">

<p>
So this has important ramifications as it means that given the
data \(\mathscr{D}\) we can determine the posterior over &theta;<sub>X</sub>
independently of the posterior over &theta;<sub>Y|X</sub>. I.e. you can
solve each problem separately and then combine the
results.<sup><a id="fnr.2" class="footref" href="#fn.2">2</a></sup> Finally, in the case of bayesian parameter
estimation another important property arise. It tells us that
the posterior can be represented in a compact factorized form.
</p>

<p>
Note, that the global parameter independence together with the
local likelihood decomposition arising from the conditional
expression of the likelihood as done in the previous section on
MLE yields the following result:
</p>

<img src="../../images/Bildschirmfoto_2021-03-06_um_10.20.50.png" class="center">

<p>
Don't be confused by how the likelihood is defined above. Recall
that the likelihood always express P(data | parameter).
</p>

<p>
So note that we proved the claim above: <i>we can work locally and
solve each problem separately and then combine the results</i>.
</p>

<p>
Note that such <b>local decomposition</b> property extends also to the
<b>prediction task</b>. It is in fact possible to see that leveraging
the property above we have for the prediction task of x[m+1],
y[m+1]:
</p>

<img src="../../images/Bildschirmfoto_2021-03-06_um_10.30.11.png" class="center">

<p>
That through the d-separation of &theta; arising from the
complete observations, as well from the posterior probability
decomposition it is possible to be expressed as:
</p>

<img src="../../images/Bildschirmfoto_2021-03-06_um_10.32.39.png" class="center">

<p>
Note that the first equality in the pic above is not the second
equality. The first is the first term of the second equality -
the one of interest.
</p>

<p>
Note that such local decomposition holds in general for the case
of prediction task with complete data, such that it holds in
general the following formulation:
</p>

<img src="../../images/Bildschirmfoto_2021-03-06_um_10.35.38.png" class="center">

<p>
We stress that the discussion so far was based on the assumption
that the priors over parameters for different CPDs are
independent; i.e. the <b>global parameter independence
property</b>. We see that, when learning from complete data, this
assumption alone suffices to get a decomposition of the learning
problem to several “local” problems, each one involving one CPD.
</p>

<p>
<i>Example in Practice - Table CPD with Dirichlet Distribution</i>
</p>

<p>
Note now that in the book it follows a brief section
demonstrating all of the above general arguments to the case of
table-CPDs. Everything is looked again through the lenses of
<i>Dirichlet</i> priors and multinomial distribution for the
individual nodes.
</p>

<p>
You can read more into the details in the book should you be
interested in an application as that. You can also quickly go
through it should you be in need to refresh quickly the applied
approach, in order to work then with a different case.
</p>

<p>
Note that in this sense in the book, the discussion in this
chapter focuses solely on Bayesian estimation for multinomial
CPDs. Here, we have a closed form solution for the integral
required for Bayesian prediction, and thus we can perform it
efficiently. In many other representations, the situation is not
so simple. In some cases, such as the noisy-or model or the
logistic CPDs of section 5.4.2, we do not have a conjugate prior
or a closed-form solution for the Bayesian integral. In those
cases, Bayesian prediction requires numerical solutions for
high-dimensional integrals. Then check your notes. Studied
extensively how to deal with such cases.
</p>

<p>
Note that alternatively on using the more fine-grained
mathematical models for dealing with posterior integration you
can also take the short cut and work with <code>MAP</code> estimation of
the parameters, i.e. you do not integrate over the distribution
but compute the <i>maximum a posteriori</i>:
</p>

<img src="../../images/Bildschirmfoto_2021-03-06_um_12.16.14.png" class="center">


<img src="../../images/Bildschirmfoto_2021-03-06_um_12.17.41.png" class="center">
</div>
</li>
</ul>
</li>

<li><a id="orgf8153bd"></a>On Parameter Estimation with shared Parameter<br />
<div class="outline-text-5" id="text-orgf8153bd">
<p>
Here the difference is likely that the key property of global
parameter independence fades away. I.e. we do not assume it and
the complexity increases.
</p>

<p>
That is correct, in fact in the Bayesian case, we also assumed
that the priors on these distributions are independent. This
assumption is a very strong one, which often does not shared
hold in practice. We worked also for the MLE under similar
structure. In fact when working with table-CPDs we assumed that
global independence of the parameters.
</p>

<p>
In real-life systems, we often have shared parameters:
parameters that occur in multiple places across the network.
</p>

<p>
In this section, we discuss how to perform parameter estimation
in networks where the same parameters are <i>used multiple times</i>.
</p>

<p>
There are essentially two types of parameter sharing:
</p>

<ul class="org-ul">
<li>global parameter sharing</li>

<li>local parameter sharing</li>
</ul>

<p>
We will explore the two in turn next.
</p>

<p>
<b>Global Parameter Sharing:</b>
</p>

<p>
Consider a network structure \(\mathscr{G}\) over a set of
variables X = {X1, . . . , Xn}, parameterized by a set of
parameters &theta;. Each variable Xi is associated with a CPD
P(X<sub>i</sub> | U<sub>i</sub> , &theta;). Now, rather than assume that each such
CPD has its own parameterization &theta;<sub>X<sub>i</sub>|U<sub>i</sub></sub> , we assume
that we have a certain set of shared parameters that are used by
multiple variables in the network. Thus, the sharing of
parameters global parameter is global, over the entire network.
</p>

<p>
Note now the following notation to express the above. Do not
take it too seriously. Not the best expression of the above in
the below formulation in my opinion but notation that will be
used later is this so stick to it.
</p>

<img src="../../images/Bildschirmfoto_2021-03-06_um_13.07.38.png" class="center">

<p>
So the above means that you have sets &theta;<sup>k</sup> containing a subset
parameters and you assign to it the variables that use such
parameters. As parameters are shared and multiple might use the
same parameters you assign sets of variables \(\mathscr{V}^k\) to
such sets &theta;<sup>k</sup>.
</p>

<p>
Note that with the above the notation you have immediately a <b>new
likelihood decomposition</b>.
</p>

<img src="../../images/Bildschirmfoto_2021-03-06_um_13.15.26.png" class="center">

<p>
From this it follows immediately the following likelihood
function - i.e. it is simply a plug-in exercise:
</p>

<img src="../../images/Bildschirmfoto_2021-03-06_um_13.35.20.png" class="center" style = "width: 30% !important;>

<p>
Given that you can quickly examine the new behaviour of the
likelihood under this global parameter sharing setting:
</p>

<img src="../../images/Bildschirmfoto_2021-03-06_um_13.35.20.png" class="center">

<p>
And through this you would get a different MLE taking this
global structure into account
</p>

<p>
\[ \hat{\theta^{k}_{y_k | w_k}}  = \frac{M_k[y_k,
       w_k]}{M_k[w_k]}\]
</p>

<p>
This aggregation of sufficient statistics applies not only to
multinomial distributions. Indeed, for any distribution in the
linear exponential family, we can perform precisely the same
aggregation of sufficient statistics over the variables in
\(\mathscr{V}^k\). 
</p>

<p>
Note finally that the above has important implications. It
means that a new observation influences the parameter of
possibly another CPD that shares the same global
parameter. This has both positive as well as negative
influence: that can be summarized in the usual concept of
<i>bias-variance</i> trade off.
</p>

<p>
Otherwise check the following paragraph for the length
verbalization of that:
</p>

<p>
When we share parameters, multiple observations from within the same
network contribute to the same sufficient statistic, and thereby help
estimate the same parameter. Reducing the number of parameters allows
us to obtain <i>parameter estimates that are less noisy</i> and closer to the
actual generating parameters. This benefit comes at a price, since it
requires us to make an assumption about the domain. If the two
distributions with shared parameters are actually different, the
estimated parameters will be a (weighted) average of the estimate we
would have had for each of them separately. When we have a small
number of instances, that approximation may still be beneficial, since
each of the separate estimates may be far from its generating
parameters, owing to sample noise. When we have more data, however,
the shared parameters estimate will be worse than the individual
ones. We return to this issue in section 17.5.4, where we provide a
solution that allows us to gradually move away from the shared
parameter assumption as we get more data.
</p>


<p>
<b>Local Parameter Sharing:</b>
</p>

<p>
I skip it here. The reasoning is similar to the previous
setting. Go read it in the book if interested at any point.
</p>

<p>
<b>Bayesian Inference with Shared Parameters:</b>
</p>

<p>
To perform Bayesian estimation, we need to put a prior on the
parameters. In the case without parameter sharing, we had a
separate (independent) prior for each parameter. This model is
clearly in violation of the assumptions made by parameter
sharing. If two parameters are shared, we want them to be
identical, and thus it is inconsistent to assume they have
<b>independent prior</b>.
</p>

<p>
Consider the <i>global parameter sharing</i> discussion above. Then
for Bayesian networks the strategy is usually to introduce a
prior over each <i>set of parameters</i> &theta;<sup>k</sup> . If we impose an
independence assumption for the priors of the different sets,
we obtain a shared-parameter version of the global parameter
independence assumption. Hence, this is the general strategy
for dealing with shared parameters in Bayesian settings.
</p>

<p>
Note, however that some of the implications previously
discussed do not hold anymore under this setting.
</p>

<p>
If on the one hand it still holds that it is possible to
decompose the posterior as a product of the subsets priors and
likelihoods, i.e. to write:
</p>

<img src="../../images/Bildschirmfoto_2021-03-07_um_10.06.55.png" class="center" style = "width: 30% !important;>

<p>
On the other hand when computing the likelihood of a new
observation it is different. While the previous section
treating the case for bayesian inference leveraged some
independence property this is not possible anymore here due to
the effect of shared parameters.
</p>

<p>
I.e. in the previous section we could operate as follows:
</p>

<img src="../../images/Bildschirmfoto_2021-03-07_um_10.19.24.png" class="center">

<p>
When we work with shared parameters the above does not
hold. I.e. the posterior parameters are not independent we
cannot write their expectation as a product of expectations.
</p>

<p>
Then you should pay more care when dealing with the above.
</p>

<p>
This concludes this section. Note that many things were not
discussed too much into the detail but you have to learn to
move fast in life. You can go back to the book should you want
more info. Note also that there is a final section discussing
<i>hierarchical priors</i> that was not treated here.
</p>
</div>
</li>
</ul>
</div>


<div id="outline-container-orgf519ecd" class="outline-4">
<h4 id="orgf519ecd">Parameter Estimation under Uncertainty - Partially observed Data</h4>
<div class="outline-text-4" id="text-orgf519ecd">
<p>
There are essentially three cases on why partially observed Data
might arise:
</p>

<ul class="org-ul">
<li>missing data by accident</li>

<li>observations not made - for instance in medical setting - some
patients some measurements others not</li>

<li>hidden variables - values are never observed. forgot/not
included in the model</li>
</ul>

<p>
We will see how incomplete data poses both <i>foundational and
computational</i> problems.
</p>

<p>
The <i>foundational problem</i> concern in formulating an appropriate
learning task and <i>specifying what we can expect to learn</i> from such
data.
</p>

<p>
The <i>computational problems</i> arise from the complications incurred
by incomplete data and the construction of algorithms that address
these complication.
</p>
</div>


<ul class="org-ul">
<li><a id="org8ad1486"></a>The Learning Problem with Incomplete Data<br />
<div class="outline-text-5" id="text-org8ad1486">
<p>
A central concept in the discussion of learning so far was the
likelihood function that measures the probability of the data
induced by different choices of models and parameters.
</p>

<p>
Note that the likelihood is central both to the bayesian
procedure - and for computing the posterior - as well to the MLE
procedure.
</p>

<p>
The question is then on how to frame the likelihood in the case
of incomplete data. This is done through the <i>marginal concept</i>.
</p>

<p>
That means that the likelihood of an incomplete instance is
simply the marginal probability given our model.
</p>

<p>
In an example you would then have the following:
</p>

<p>
Suppose the domain consists of two random variables X and Y , and
in one particular instance we observed only the value of X to be
x[m], but not the value of Y . Then, it seems natural to assign     
the instance the probability P(x[m]).
</p>

<p>
This approach is very intuitive and might at first seems
flawless. However, the approach embodies some rather strong
assumptions about the nature of our data, and especially the data
generating process.
</p>

<p>
In fact when dealing with missing data you can think of the data
generating process as a two folded process:
</p>

<ol class="org-ol">
<li>generate the data from the model.</li>

<li>determine - possible through a probabilistic model - which
values we get to observe and which ones are hidden from us.</li>
</ol>

<p>
As for the second case a basic example is the one of playing
<i>risiko</i> with friends. You roll the dices. Sometimes they fall
out of the table. You do not observe the result. This is possibly
a hiding mechanism. Note now that in this particular situation we
do not have a learning process. Consider however the same setting
for the case of the thumbtack example discussed above. Then as
the result of a thumbtack out of the table looks quite different
than the one falling on the table - i.e. we might have a
different data generating process - we ignore the samples falling
out of the table. We will see next that <i>ignoring</i> some samples
might indeed be the right way to deal with the issue. However,
this might not always be the case. Think for instance when you
have a reporting person that tells you the thumbtack got out of
the table in every case where the thumbtack actually falls on
tail. Then ignoring such observations would lead you to
misleading conclusions.
</p>

<p>
What you can do in the false reporting case is to use the
knowledge that every missing value is “tails” and count it as
such. Note that this leads to <b>very different likelihood function</b>
(and hence estimated parameters) from the strategy that we used
in the previous case.
</p>

<p>
This is an example of why it is important to understand and
determine the data generating process in order to properly
specify the likelihood function and come to your desired
solution.
</p>

<p>
To define the problem and set it into mathematical notation in
this section we will hence work with two variables for each
experiment. I.e. a \(X\) variable for the <i>actual</i> flip outcome and
an \(O_X\) for the observed outcome that tells us whether the flip
was observed or not.
</p>

<img src="../../images/Bildschirmfoto_2021-03-07_um_13.26.33.png" class="center">

<p>
Let \(X = {X_1, ..., X_n}\) be some flip outcome sample. Let \(O_X =
     {O_{X_1}, ..., O_{X_n}}\) be the observation realization.
</p>

<p>
Note that the <i>observability model</i> is a joint distribution
\(P_{missing} (X,O_X) = P(X) * P(O_X | X)\) with parameters &theta;
for P(X) and &phi; for P(O<sub>X</sub> | X).
</p>

<p>
We <i>define</i> a new RV \(Y\) summing up the information from \(X\) and
\(O_X\). I.e. the variable Y takes the following shape:
</p>

<img src="../../images/Bildschirmfoto_2021-03-07_um_18.35.42.png" class="center">

<p>
Given such definitions it is straightforward to see that for the
likelihood of our model for the case of <i>randomly</i> non-observed
variables it holds that:
</p>

<p>
\[P(Y = 1) = \theta\phi \\ P(Y = 0) = (1 − \theta)\phi \\ P(Y =
     ?) = (1 − \phi)\]
</p>

<p>
such that the overall likelihood function becomes:
</p>

<img src="../../images/Bildschirmfoto_2021-03-07_um_18.56.51.png" class="center">

<p>
<b>Important - Important - Important:</b> is to understand that in
this case the likelihood decomposes as in the case of complete
data seen in the previous chapter. The idea is that above there
is a clear function depending on &theta; and the realization of X,
i.e. be it M[0] or M[1] and a clear function depending on &phi;
depending both on the realization of X and Y. You can then solve
for the maximum of these two functions separately and achieve the
global optimum. This is in fact what is written above in an
implicit way and this the way to solve this problem.
</p>

<p>
Consider, in contrast, the case of <i>voluntarily</i> removing some
observations.
</p>

<p>
Now the parameter &phi; determining the observation of the flip
realization does depend on the outcome of the coin flip.
</p>

<p>
So we would ultimately have two different possible
parameterization: &phi;<sub>O<sub>X</sub>|x = heads</sub> , &phi;<sub>O<sub>X</sub>|x =
tails</sub>. Both express the probability of observing the
flip (o = o<sup>1</sup>) in the case that is specified in the conditional
part. Note that this represented by the meta-network above. You
can then observe the dependency of the RV O<sub>X</sub> on X in the
above. This represent in fact such described conditional
dependency.
</p>

<p>
Note that in this case when we get an observation Y = ?, we
essentially observe the value of O<sub>X</sub> but not of X. In this case,
due to the direct edge between X and O<sub>X</sub>, the context-specific
independence properties of Y do not help: X and O<sub>X</sub> are
correlated, and therefore so are their associated
parameters. Thus, we cannot conclude that the likelihood
decomposes. In poor words, we cannot say that observing Y = ?
there is no relation among O<sub>X</sub> and X as was the case before when
observing Y = ?, and hence o = o<sup>0</sup> did not given any information
for the relation among X and Y.
</p>

<p>
This is straightforward to see when writing down explicitly the
likelihood function.
</p>

<img src="../../images/Bildschirmfoto_2021-03-07_um_19.32.23.png" class="center">

<p>
As we can see, the likelihood function in this example is more
complex than the one in the previous example. In particular,
there is no easy way of decoupling the likelihood of &theta; from
the likelihood of &phi;{O<sub>X</sub>|x<sup>1</sup>} and &phi;<sub>O<sub>X</sub>|x<sup>0</sup></sub>. This makes
sense, since different values of these parameters imply different
possible values of X when we see a missing value and so affect
our estimate of &theta;. Note therefore that here you cannot
perform the optimization of the likelihood for the functional
terms separately by checking the likelihood for the observation
X and Y respectively. Here you have to optimize the two
concurrently because changing the <b>parameter governing the data
generating process</b> for the flip realization also affects the
observation outcome.
</p>

<p>
Note that modeling the observation variables observed and writing
a likelihood for these down, i.e. in other words of the process
generating the missing values, might quickly lead to a complex
likelihood as in the case above. Another possibility is to focus
just on the data generating process for the variables of
interest - say \(X\) here. In some cases this might yield a problem
that is more tractable. The question now is on how to specify
this generating process for the variables of interest <b>decoupling
it from the observation mechanism</b>. This will be the topic of the
next session.
</p>

<p>
<b>Decoupling the Observation Mechanism - Get easier Likelihood</b>
</p>

<p>
So the question of interest is on <i>when it is safe to ignore the
observation variables all together</i>?
</p>

<p>
We saw in the previous discussion that it was safe to ignore the
unobserved variables when the <b>observation mechanism</b> is
completely <b>independent of the domain variables</b> of interest.
</p>

<p>
This is a result that generally holds and it makes sense
therefore to formalize such a case:
</p>

<img src="../../images/Bildschirmfoto_2021-03-07_um_21.24.15.png" class="center">

<p>
Note that in the case of data <i>missing completely at random</i> the
likelihood of X and O<sub>X</sub> <b>decomposes as a product</b> as previously
seen. As mentioned as well this has the nice property that we can
then maximize the terms independently. Note moreover that in such
a case you cannot just only maximize and get your parameters of
interest independently. You can in fact <b>ignore the parameters
governing the observation mechanism all together</b>.
</p>

<p>
The idea of why this in fact holds true is the following:
</p>

<blockquote>
<p>
The implications of the decoupling is that we can maximize the
likelihood of the parameters of the distribution of X without
considering the values of the parameters governing the
distribution of O<sub>X</sub>. Since we are usually only interested in the
former parameters, we can simply ignore the later parameters.
</p>
</blockquote>

<p>
Note that the assumption that the data governing mechanism is
such that data are <i>missing completely at random</i> is a quite
strong one. Luckily <i>missing completely at random</i> data is a
<b>sufficient but not necessary condition</b> for the decomposition of
the likelihood function.
</p>

<p>
I.e. it is possible to prove a more general condition where,
rather than assuming marginal independence between O<sub>X</sub> and the
values of X, we assume only that the observation mechanism is
<b>conditionally independent</b> of the underlying variables given other
observations.
</p>

<p>
One example of <i>conditional independence</i> is the following:
</p>

<p>
Consider flipping two coins. You observe the result of the first
coin flip X<sub>1</sub>. Based on this you decide whether you will hide or
not the second coin toss. It follows that conditioning on
observing X<sub>1</sub>, X<sub>2</sub> and O<sub>X<sub>2</sub></sub> are independent.
</p>

<p>
Expressing the above situation in terms of its likelihood gives
the following with &theta;<sub>X<sub>1</sub></sub> and &theta;<sub>X<sub>2</sub></sub> representing the
parameters for the likelihood of the flip realizations of heads
and &phi;{O<sub>X<sub>2</sub></sub> | x<sub>1</sub><sup>1</sup>}, &phi;{O<sub>X<sub>2</sub></sub> | x<sub>1</sub><sup>0</sup>}:
</p>

<ul class="org-ul">
<li>there are six possible cases:

<ul class="org-ul">
<li>4 cases where we observe both of the coins realization -&gt; all
possible combinations.</li>

<li>2 cases where we just observe the first coin.</li>
</ul></li>
</ul>

<p>
For the first class - where both are observed the probability is
straightforward. Both values of X<sub>1</sub> and X<sub>2</sub> play a role such that
both &theta;<sub>X<sub>1</sub></sub> and &theta;<sub>X<sub>2</sub></sub> enter the relation.
</p>

<p>
Note that in contrast to that for the second class this is not
the case. Consider for instance \(P(X_1 = x_1^1, O_{X_1} = o^1,
     O_{X_2} = o^0)\) then the value of X<sub>2</sub> does not play a role at
all. I.e. the probability is expressed as $ &theta;{X<sub>1</sub>} * (1 -
&phi;{O<sub>X<sub>2</sub></sub> | x<sub>1</sub><sup>1</sup>})$. This because of the <b>conditional
independence</b> described above. I.e. given the first realization
and conditioning on the information of this whether we observe
the second value or not does not depend on the realization
itself.
</p>

<p>
Writing down all of the 6 possibilities and rearranging terms
yields:
</p>

<img src="../../images/Bildschirmfoto_2021-03-08_um_10.03.31.png" class="center">

<p>
Hence we achieved the <i>local factorization property</i> and we
achieved our goal.
</p>

<p>
A formalization of the above to the general case follows.
</p>

<img src="../../images/Bildschirmfoto_2021-03-08_um_10.16.20.png" class="center">

<p>
In other words we have <i>missing at random (MAR)</i> data when we
have independence among o<sub>X</sub> and x<sup>y</sup><sub>hidden</sub> given
x<sup>y</sup><sub>observed</sub>.
</p>

<p>
This essentially means as expressed above that the <i>observation
pattern gives us no additional information about the hidden variables given the observed variables</i>.
</p>

<p>
\[ P_{missing}(x^y_{hidden} | x^y_{obs}, o_X) = P_{missing}(x^y_{hidden} | x^y_{obs}) \]
</p>

<p>
This essentially means the following important property
</p>

<img src="../../images/Bildschirmfoto_2021-03-08_um_10.25.51.png" class="center">

<p>
Note that in the expression above the first term only involves
the parameters &phi; and the second only the parameters &theta;. So
that if we are interested in our model on the parameters for the
flip coin realizations and not on the one of the observation
mechanism we can focus on the latter and decompose the problem.
</p>

<p>
I.e. recall <b>bottom line</b>:
</p>

<img src="../../images/Bildschirmfoto_2021-03-08_um_10.32.43.png" class="center">

<p>
We will continue the chapter with the assumption that the <i>MAR</i>
condition holds so that we can just focus on the <i>likelihood</i> of
the realizations that were actually observed without having to
consider the cases where the hiding mechanism kicked in. 
</p>
</div>
</li>

<li><a id="org28cd180"></a>On the Likelihood Function - given the MAR assumption<br />
<div class="outline-text-5" id="text-org28cd180">
<p>
We will check here the likelihood  function and we will see that
both the properties of <i>global decomposability</i> as well as <i>local
decomposability</i> get lost in the case of partially observed data
with important consequences for the computational complexity in
the evaluation of the likelihood function.
</p>

<p>
Assume MAR and consider the following structure: we have a
network \(\mathscr{G}\) over a set of variables X. In general, each
instance has a different set of observed variables. We will
denote by O[m] and o[m] the observed variables and their values
in the m’th instance, and by H[m] the missing (or hidden)
variables in the m’th instance. We use L(&theta; : D) to denote
the probability of the observed variables in the data.
</p>

<p>
Recall the previous derivation, i.e.
</p>

<img src="../../images/Bildschirmfoto_2021-03-08_um_10.25.51.png" class="center">

<p>
It follows now that given the above for the likelihood it holds
marginalizing out the hidden variables as above, and ignoring the
observability model - the first term - for the reasons discussed
above, that:
</p>

<p>
\[ L(\theta : D) = \prod_{m=1}^M P(o[m] | \theta)\]
</p>

<p>
Note that the above notation might be a bit confusing. I keep it
for consistency with the reference book. The <code>o</code> above in the
latter equation does not refer at all to the observability
mechanism. It simply represents the realization of the x<sub>obs</sub>.
</p>

<p>
It might appear that the problem of learning with missing data
does not differ substantially from the problem of learning with
complete data. We simply use the likelihood function in exactly
the same way. Although this intuition is true to some extent,
the computational issues associated with the likelihood
function are substantially more complex.
</p>

<p>
To understand why it becomes <b>nasty</b> understand the following
issue:
</p>

<img src="../../images/Bildschirmfoto_2021-03-08_um_11.39.46.png" class="center">

<p>
Now consider the case with partially observed data. Recall that
in such a case the likelihood function is the <b>sum of all of
the possible complete (observed) likelihood functions</b>. This is
in fact what marginalizing over the hidden variables means.
</p>

<p>
In the specific for the above example this means (<b>important
example</b>):
</p>

<img src="../../images/Bildschirmfoto_2021-03-08_um_11.47.43.png" class="center">

<p>
Note that we can think of the situation using a geometric
intuition. <i>Each one of the complete data likelihood defines a
unimodal function</i>. Their sum, however, can be multimodal. In
the worst case, the likelihood of each of the possible
assignments to the missing values contributes to a different
peak in the likelihood function. The total likelihood function
can therefore be quite complex.
</p>

<img src="../../images/Bildschirmfoto_2021-03-08_um_11.55.42.png" class="center">

<p>
Moreover, on the top of this we <i>loose the property of
parameter independence</i> and hence of <b>local
decomposability</b>. This can be quickly seen by the coupling of
the parameters &theta;<sub>x<sup>1</sup></sub> <b>and</b> &theta;{y<sup>1</sup> | x<sup>1</sup>} and
&theta;{y<sup>1</sup> | x<sup>0</sup>} as in the above example.
</p>

<p>
This means that you cannot look at the local case x<sup>0</sup> or x<sup>1</sup>
and derive the likelihood from there such that the likelihood
is the product of terms involving local observations. There is
in fact an interaction among <b>local-observations</b> when computing
the likelihood.
</p>


<p>
You can also see that the <b>local decomposability</b> property gets
lost by looking at the meta-network:
</p>

<img src="../../images/Bildschirmfoto_2021-03-08_um_11.59.48.png" class="center" style = "width: 40% !important;">


<p>
It is obvious that when we observe X[m] the trail between
&theta;<sub>X</sub> and &theta;{Y|X} is not active. The observation of
X[m] makes the information on &theta;<sub>X</sub> superfluous. When these
are not observed however obtaining a sample of Y might be
informative on the effect of \theat<sub>X</sub> due to the indirect
influence. It means then that &theta;<sub>X</sub> and &theta;<sub>Y|X</sub> are
not independent anymore as being both influenced by the same
observations.
</p>

<p>
While it is clear that the local decomposability goes lost in
the case of partially observed data, the question that arises
is whether the <b>global decomposability</b> goes lost as well.
</p>

<p>
I.e. the recall that in the case of complete data we observed
that we could treat the likelihood between <i>different CPDs</i>
independently. I.e. we could maximize the likelihood of each
separately and the maximization of each of these CPDs combined
would yield the overall maximum for the combined likelihood.
</p>

<p>
<i>Local vs Global Decomposability</i>: So to put it in one sentence
as the line was not drawn in a super clear way in these notes
so far, for the local decomposability you are interested in a
<i>single CPD</i> say for instance P(Y|X) as in the case just
discussed above. The question if you can decompose the
likelihood for the different local realizations, say x<sup>0</sup> or
x<sup>1</sup>. In contrast, for the case of <i>global decomposability</i> you
are interested in whether you can decompose the overall
likelihood for different CPDs. I.e. for instance in a model of
the following form with H being a <i>hidden variable</i>
</p>

<img src="../../images/bayesNet5.png"  style = "width: 30% !important;">

<p>
the question is whether we can treat each likelihood
separately - that is just depending on the individual CPDs of H
on X, and H on Y.
</p>

<p>
Note that in the network above the likelihood can be
represented as:
</p>

<img src="../../images/Bildschirmfoto_2021-03-08_um_14.49.02.png" class="center">

<p>
When we had complete data, we rewrote the likelihood function
as a product of local likelihood functions, one for each
CPD. This decomposition was crucial for estimating each CPD
independently of the others. In this example, we see that the
likelihood is a product of sum of products of terms involving
different CPDs. <b>The interleaving of products and sums means
that we cannot write the likelihood as a product of local likelihood functions.</b>
</p>

<p>
Again the <i>global decomposability</i> is lost; this because we do
not observe the variable H. Hence we cannot decouple the
estimation of P(X | H) from that of P(Y | H). I.e. both depend
on how we reconstruct H and observations of both will alter the
way we construct and parameterize H via its likelihood.
</p>

<p>
Hence for the general case it holds that:
</p>

<img src="../../images/Bildschirmfoto_2021-03-08_um_14.58.06.png" class="center">

<p>
<b>Important - important - important:</b> This has <i>very serious
consequences</i>. It implies that to compute the likelihood
function we need to <b>perform inference for each
instance</b>. I.e. we need to compute the probability of each of
the observations. Note that this problem might be intractable
depending on the network structure and the pattern of the
missing values.
</p>

<p>
So overall the bottom result is that
</p>

<blockquote>
<p>
in the presence of partially observed data, we have lost all of the
important properties of our likelihood function: its unimodality, its
closed form representation, and the decomposability as a product of
likelihoods for the different parameters. Without these properties,
the learning problem becomes substantially more complex.
</p>
</blockquote>
</div>
</li>

<li><a id="org3c5ebbd"></a>On the identifiability issue<br />
<div class="outline-text-5" id="text-org3c5ebbd">
<p>
A further issue when dealing with partially observed data is the
one of the identifiability of the model. The idea is that it
might not be possible to uniquely identify the model from the
data.
</p>

<p>
The issue is well explained in the following two examples in the
book
</p>

<img src="../../images/Bildschirmfoto_2021-03-08_um_15.16.18.png" class="center">

<img src="../../images/Bildschirmfoto_2021-03-08_um_15.16.48.png" class="center">

<p>
So  the identifiability difficulties are clear. It is as well
clear that collecting more data will not be of any help.
</p>

<p>
Recall in this sense that a model is identifiable if each choice
of parameters implies a different distribution over the observed
variables. Nonidentifiability implies that there are parameter
settings that are indistinguishable given the data.
</p>

<p>
There is then a brief discussion on the non-identifiability in
the case of hidden data and the definition of <b>locally
identifiable</b>. This is important in that it states that you do not
have to guarantee identifiability over the entire parameter
space, but in neighboring regions. This means that you can have
two different parameterization that lead to the same likelihood
but they should be far apart so that you can use locally some
techniques to reach the maximum.
</p>

<p>
Finally, note that a nonidentifiable model <i>does not</i> mean that we
should not attempt to learn models from data. But it does mean
that we should be careful not to read into the learned model more
than what can be distinguished given the available data.
</p>
</div>
</li>


<li><a id="orgc967c2c"></a>On Parameter Learning<br />
<div class="outline-text-5" id="text-orgc967c2c">
<p>
So far we discussed and clarified the issues that arise when
working with partially observed data. This especially in relation
to the difficulty of specifying the likelihood function and the
fact that this does not decompose in such a neat way as it was
the case under complete data.
</p>

<p>
The question that we will tackle in this section is on how to
deal with the optimization of the likelihood function given its
multimodal shape and in general how to deal with the
computational difficulties we discussed in the previous section.
</p>

<p>
We will see in this sense that there are essentially two
approaches for dealing with the above:
</p>

<ul class="org-ul">
<li>the gradient ascent</li>

<li>the expectation-maximization algorithm</li>
</ul>
</div>

<ul class="org-ul">
<li><a id="org1513332"></a><span class="todo TODO">TODO</span> The (stochastic) gradient ascent<br />
<div class="outline-text-6" id="text-org1513332">
<p>
Skipped as not being the focus of the thesis. Go back to it when
you have time. You have the session with Radu soon. No time to
deal with this session in the detail now.
</p>

<p>
It is the very well known algorithm though. Very much used in
optimization even with the <i>stochastic</i> component variant.
</p>
</div>
</li>

<li><a id="orgc13191f"></a>The expectation-maximization algorithm<br />
<div class="outline-text-6" id="text-orgc13191f">
<p>
This is also fairly standard. There is a ton of literature on
this and it is used in many ML problems. Note that in contrast
to the gradient ascent it is not a general purpose optimization
algorithm for non-linear functions. It is rather much more
tailored to the application to the likelihood
functions. Moreover, it is especially well-suited for dealing
with exponential families as there solutions are often
analytically computable.
</p>

<p>
You can check the following two videos. They give you an
intuition on why the EM work and recap quickly why it is used in
such cases:
</p>

<div class="container"> 
  <iframe class="responsive-iframe" src="https://www.youtube.com/embed/AnbiNaVp3eQ" frameborder="0" allowfullscreen;> </iframe>
</div>

<div class="container"> 
  <iframe class="responsive-iframe" src="https://www.youtube.com/embed/X9yF2djExhY" frameborder="0" allowfullscreen;> </iframe>
</div>

<p>
Let's turn to the book notes after having refreshed the
algorithm.
</p>

<p>
There the argument proposed is the following.
</p>
<blockquote>
<p>
When learning with missing data, we are actually trying to solve
two problems at once: learning the parameters, and hypothesizing
values for the unobserved variables in each of the data
cases. Each of these tasks is fairly easy when we have the
solution to the other. Given complete data, we have the
statistics, and we can estimate parameters using the MLE
formulas we discussed previously.
</p>

<p>
Conversely, given a choice of parameters, we can use
probabilistic inference to hypothesize the likely values (or the
distribution over possible values) for unobserved
variables. Unfortunately, because we have neither, the problem
is difficult.
</p>
</blockquote>

<p>
Then the EM proceed iteratively. I.e. you would start with an
arbitrarily - or maybe a heuristic based - parameter selection
and you would solve this chicken-egg problem of above
iteratively. I.e. you would impute the data according to some
inference step and then based on this you would compute a new
paramterization as seen in the previously discussed MLE derivation -
i.e. the one with the sum term.
</p>

<p>
As we will show then such iterative method improves the
parameters, i.e.
</p>

<p>
Each of these operations can be thought of as taking an “uphill”
step in our search space. More precisely, we will show (under
very benign assumptions) that: each iteration is guaranteed to
improve the log-likelihood function; that this process is
guaranteed to converge; and that the convergence point is a
fixed point of the likelihood function, which is essentially
always a <b>local</b> maximum.
</p>

<p>
Concretely in one example the above would look as follows.
</p>

<p>
<i>Example:</i>
</p>

<p>
Consider the following simple meta-network for the example:
</p>

<img src="../../images/Bildschirmfoto_2021-03-12_um_09.43.51.png" class="center" style = "width: 40% !important;">

<p>
As we discussed previously in the case the likelihood in the
case of fully observable data for the MLE for the global shared
parameters would look as:
</p>

<img src="../../images/Bildschirmfoto_2021-03-12_um_09.47.23.png" class="center" style = "width: 40% !important;">

<p>
It is therefore clear that in the case of complete data we can
easily compute the MLE parameters.
</p>

<p>
Consider now the case of missing data. For instance \(o = [a^1,
      ?, ?, d^0]\).
</p>

<p>
Then there are four combinations for the realization of the
missing variables.
</p>

<p>
The idea is that given an initial parameterization you can
compute the probability of each of this possible realization
using inference.<sup><a id="fnr.3" class="footref" href="#fn.3">3</a></sup> 
</p>

<p>
What you can then actually do is to treat each of the possible
combination as an actual observation, i.e. an instantiation and
weight each of this according to the probability with which it
occurs. You would then have a new <i>weighted</i> complete dataset on
which you can do standard MLE estimation as discussed above.
</p>

<p>
In general the following holds - where instead of sufficient
statistics we would in fact work with <b>expected sufficient
statistics</b>:
</p>

<img src="../../images/Bildschirmfoto_2021-03-12_um_10.13.31.png" class="center">

<p>
You can then see in the example that when you compute your MLE
estimates according to the formula above they are quite
different from the initial parameters. So you took the uphill
movement discussed, with an <code>expectation step</code> the one where you
weight each possible occurrence - and a <code>maximization step</code>,
where you compute the new parameters. Both are written in one
shot in the above.
</p>

<p>
This intuition seems nice. However, it may require an
unreasonable amount of computation. To compute the expected
sufficient statistics, we must sum over all the completed data
cases. The number of these completed data cases is much larger
than the original data set. For each o[m], the number of
completions is exponential in the number of missing values.
</p>

<p>
Fortunately, it turns out that there is a better approach to
computing the expected sufficient statistic than simply summing
over all possible completions.
</p>

<p>
To understand this consider the following expected sufficient
statistics:
</p>

<img src="../../images/Bildschirmfoto_2021-03-12_um_10.49.13.png" class="center">

<p>
Therefore it is clear from the above that the new formula is
identical, except that we have substituted the indicator
variable with a probability that is somewhere between 0 and 1.
</p>

<p>
If in a certain data case we get to observe C, the indicator
variable and the probability are the same. Thus, we can view the
expected sufficient statistics as filling in soft estimates for
hard data when the hard data are not available.
</p>

<p>
So notice that you use a sort of posterior probability in the
inference step when computing expected sufficient statistics.
</p>

<p>
So in general for the case of <code>Table CPDs</code> you can work as
follows:
</p>

<ul class="org-ul">
<li>choose an initial parameterization &theta;<sup>0</sup>, then execute the
following phases for t = 0, 1, &#x2026;:</li>
</ul>

<img src="../../images/Bildschirmfoto_2021-03-12_um_11.07.13.png" class="center">

<img src="../../images/Bildschirmfoto_2021-03-12_um_11.07.37.png" class="center">

<p>
Note that the maximization step is straightforward given the
expected sufficient statistics.
</p>

<p>
The question is rather on how to compute the conditional
probabilities necessary for computing the expectation term.  For
this you must resort to one inference technique over the network
\({\mathscr{G}, \theta^t}\) such as the <i>clique tree</i> or <i>cluster
graph</i> algorithm. These are not described in these notes. You
can read about them in the book. Important however is to
understand that the properties of such algorithms allows us to
<b>do all of the required inference for each data case using one
run of message-passing calibration</b>.
</p>

<p>
Note that the discussion of above does not just hold for the
case of table-CPDs but it can rather be quickly generalized to
arbitrary cases, where you have a sufficient statistics that
characterizes your probability function. Think for instance to
the case of an exponential family PDF, then it holds:
</p>

<img src="../../images/Bildschirmfoto_2021-03-12_um_11.38.44.png" class="center">

<p>
So it is now clear the algorithmic scope of the
EM-algorithm. The question is on why the convergence property
applies. We saw in the video above a very rough
motivation. I.e. it is possible to interpret it as an iterative
way of finding the necessary condition for an optimum - i.e. the
fact that the derivative is 0.
</p>
</div>
</li>

<li><a id="orgcc53fe5"></a>Mathematics of EM<br />
<div class="outline-text-6" id="text-orgcc53fe5">
<p>
We will now proceed and do a more robust analysis of that to
understand the mathematical motivation of the algorithm.
</p>

<p>
The basic idea is the following: at each iteration can be viewed
as maximizing an <i>auxiliary function</i>, rather than the actual
likelihood function. The crucial part of the analysis is to show
how the auxiliary function relates to the likelihood function we
are trying to maximize.
</p>

<p>
As we will show, the relation is such that we can show that the
parameters that maximize the auxiliary function in an iteration
also have <i>better likelihood</i> than the parameters with which we
started the iteration.
</p>

<p>
It holds therefore:
</p>

<img src="../../images/Bildschirmfoto_2021-03-12_um_12.31.20.png" class="center" style = "width: 30% !important;">

<p>
To understand why this holds and the fact with the <i>auxiliary
variable</i> consider the following - i.e. start with the following
setting:
</p>

<img src="../../images/Bildschirmfoto_2021-03-12_um_16.08.27.png" class="center">

<p>
Note that in the case of fully observed data, the <b>score</b>
expressing how well a set of parameters is, was the
log-likelihood. In the case of partially observed data we work
with the <i>expected log-likelihood</i> as mentioned above, i.e. as
discussed the <b>score</b> would be given by:
</p>

<img src="../../images/Bildschirmfoto_2021-03-12_um_16.12.30.png" class="center">

<p>
From this it follows immediately inserting the log-likelihood
for the case of table CPDs that was shown before and using the
linearity of the expectation that:
</p>

<img src="../../images/Bildschirmfoto_2021-03-12_um_16.20.56.png" class="center">

<p>
This expression has precisely the same form as the
log-likelihood function in the complete data case, but using the
expected counts rather than the exact full-data counts. The
implication is that instead of summing over all possible
completions of the data, we can evaluate the expected
log-likelihood based on the expected counts.
</p>

<p>
The crucial point here is that the log-likelihood function of
complete data is <b>linear in the counts</b>. This allows us to use
linearity of expectations to write the expected likelihood as a
function of the expected counts.
</p>

<p>
An analogous result arise in the case of exponential
family. There it is also possible to arrive to a log-likelihood
function that is linear in the sufficient statistics.
</p>

<p>
The idea is now that the two steps are:
</p>

<ul class="org-ul">
<li>you compute the expected sufficient statistics under the <code>Q</code>
measure.</li>

<li>then maximizing the expected likelihood is the same task as
the maximization in the case of <i>complete data</i>. the only
difference lies in the fact that you work with the expected
number counts instead of the actual number of counts.</li>
</ul>

<p>
You see therefore the classical two steps of the EM algorithm.
</p>

<p>
Recall now that in the above the expected sufficient statistics
is computed as:
</p>

<img src="../../images/Bildschirmfoto_2021-03-12_um_16.46.21.png" class="center" style = "width: 60% !important;">

<p>
And that \(Q(\mathscr{H})\) is the distribution:
</p>

<img src="../../images/Bildschirmfoto_2021-03-12_um_16.49.16.png" class="center">

<p>
Note that each iteration uses a different \(\mathscr{Q}\)
distribution, i.e. a <b>different parameterization</b>, and thus we
cannot relate the optimization taken in one iteration to the
ones made in the subsequent one.
</p>

<p>
However, note that the choice of \(\mathscr{Q}(\mathscr{H}) =
      P(\mathscr{H} | \mathscr{D}, \theta^t)\) allows us to prove that
at each EM iteration we improve the likelihood.
</p>

<p>
In order to understand this we create the new <i>auxiliary
function</i> discussed above. We will let such function be
dependent both on &theta; and on \(\mathscr{Q}\) at each step and we
will optimize this.
</p>

<p>
In order to understand the <i>auxiliary function</i> and its relation
to the log-likelihood, start by considering the <b>energy
functional</b> expressing the total energy in the system. This can
be expressed as follows:
</p>


<p>
\[ F[P, Q] = E_Q [log(\tilde{P})] + H_Q(\mathscr{X}) \]
</p>

<p>
where, \(P = \frac{\tilde{P}}{Z}\) such that \(\tilde{P}\) is the
unnormalized state probability and \(H_Q\) is the negative
entropy of the observed particles.
</p>

<p>
It is then immediate to see that given such energy functional
function you can re-express the logarithm of the normalizing
constant Z as follows:
</p>

<p>
\[ log(Z) = F[P, Q] + D(Q || P) \]
</p>

<p>
where \(D(Q || P)\) is the relative entropy or the KL-divergence
criteria.
</p>

<p>
To see this is immediate as:
</p>

<p>
\[ F[P, Q] + D(Q || P) = E_Q [log(\tilde{P})] +
      H_Q(\mathscr{X}) + E_Q [log(Q(\mathscr{X}))] -
      E_Q[log(P(\mathscr{X}))]\]
</p>

<p>
The result follows by inserting \(P = \frac{\tilde{P}}{Z}\) and
noting that the negative entropy term cancels in the above.
</p>

<p>
Inserting now our probability measures for the \(P =
      \frac{\tilde{P}}{Z}\) expression, and noting the consequences of
this we can come to important appreciations that will lead us to
understand the key results of this section, i.e. the <i>relation
between the auxiliary function and the log-likelihood</i>:
</p>

<img src="../../images/Bildschirmfoto_2021-03-13_um_10.23.37.png" class="center">

<p>
Note that the above is central piece of information.
</p>

<p>
From the last corollary, i.e. the last two equalities we can
derive important understandings.
</p>

<p>
<i>Key Insights from the second equality:</i>
</p>

<p>
I.e. by the fact that both the entropy H<sub>Q</sub> is non-negative (log
of 0-1) as well as the relative entropy is non-negative by the
same argument (ratio of two positive numbers) we know that the
<b>log-likelihood is a lower bound on the expected log-likelihood
relative to Q for any choice of Q</b>.
</p>

<p>
Moreover note the important fact that selecting \(Q(\mathscr{H})
      = P(\mathscr{H} | \mathscr{D}, \theta)\) the relative entropy
term in the second equality becomes 0. Then you are just left
with the entropy term that in such case is the overall measure
on the difference between the expected log-likelihood and the
real log-likelihood.
</p>

<p>
<i>Key Insights from the first equality:</i>
</p>

<p>
Here it is possible to see that the energy functional is a lower
bound for the log-likelihood. This for the same argument for the
relative entropy as above.
</p>

<p>
Moreover note, that when choosing - as in our case - \(Q(\mathscr{H})
      = P(\mathscr{H} | \mathscr{D}, \theta)\) then the <b>energy
functional and the log-likelihood are equal</b>. This practically
means that when optimizing the energy functional you are
maximizing the log-likelihood as well.
</p>

<p>
The question is then on how to optimize the energy functional.
</p>

<p>
We will next see that with the EM algorithm you are actually
optimizing the energy functional. We will see next the way
through which such an optimization is performed.
</p>

<p>
We will in fact show that the EM algorithm optimizes the energy
functional F through a <i>coordinate ascent</i> optimization (same as
Gibbs sampling - optimize the function in one axis at the
time). We start with some choice θ of parameters. We then search
for Q that maximizes \(F[\theta, Q]\) while keeping &theta;
fixed. Next, we fix Q and search for parameters that maximize
\(F[\theta, Q]\). We continue in this manner until convergence.
</p>

<p>
In order to do that consider the following:
</p>

<p>
<b>Step 1 - Optimization of Q:</b>
</p>

<img src="../../images/Bildschirmfoto_2021-03-13_um_12.02.34.png" class="center">

<p>
Note that the mentioned corollary above is the one at the end of
the previous pic. The intuition is the one expressed in the
reasonings above for the first equation. You have in general a
lower bound on F given by log-likelihood. If you now choose the
distribution Q in the way described above you know that you have
reached the lower bound and that such lower bound is
tight. I.e. it is straightforward to see that your are at the
maximum for a <b>given &theta;</b>.
</p>

<p>
Note that when computing \(P(\mathscr{H} | \mathscr{D},
      \theta^t)\) by which you are going to weight your observations
you are in your E-step, i.e. in your expectation step. Such that
it is possible to conclude by saying that :
</p>

<blockquote>
<p>
we can view the E-step as implicitly optimizing Q by using
\(P(\mathscr{H} | \mathscr{D}, \theta^t)\) in computing the
expected sufficient statistics.
</p>
</blockquote>

<p>
<b>Step 2 - Optimization of &theta;:</b>
</p>

<img src="../../images/Bildschirmfoto_2021-03-13_um_12.12.08.png" class="center">

<p>
I.e. this second step is exactly the maximization step of the EM
algorithm.
</p>

<p>
<b>So it is generally clear that the EM steps corresponds to two
coordinate ascent optimization procedures.</b>
</p>

<p>
And generally
</p>

<blockquote>
<p>
We conclude that EM performs a variant of hill climbing, in the
sense that it improves the log-likelihood at each
step. Moreover, the M-step can be understood as maximizing a
lower-bound on the improvement in the likelihood. Thus, in a
sense we can view the algorithm as searching for the largest
possible improvement, when using the expected log-likelihood as
a proxy for the actual log-likelihood.
</p>
</blockquote>

<p>
<b>Beautiful! You now have an understanding of why the EM algo
works!</b>
</p>
</div>
</li>


<li><a id="orgc48b61a"></a>Hard-assignment EM<br />
<div class="outline-text-6" id="text-orgc48b61a">
<p>
The hard-assignment EM, also iterates over two steps: one in
which it completes the data given the current parameters
&theta;<sup>t</sup> , and the other in which it uses the completion to
estimate new parameters &theta;<sup>t</sup>+1.
</p>

<p>
However, rather than using a soft completion of the data, as in
standard EM, it selects for each data instance o[m] the <b>single
assignment</b> h[m] that maximizes \(P(h | o[m], \theta^t)\).
</p>

<p>
Although hard-assignment EM is similar in outline to EM, there
are important differences. In fact, hard-assignment EM can be
described as optimizing a different objective function, one that
involves <i>both</i> the learned parameters <i>and the learned assignment</i>
to the hidden variables.
</p>

<p>
You can refer to the book for more. However, note the following
important property, for instance in the case of using EM
algorithm for performing bayesian clustering:
</p>

<blockquote>
<p>
Although hard-assignment EM is similar in outline to EM, there
are important differences. In fact, hard-assignment EM can be
described as optimizing a different objective function, one that
involves both the learned parameters and the learned assignment
to the hidden variables.
</p>
</blockquote>
</div>
</li>
</ul>
</li>

<li><a id="org5a91d26"></a>Uncertain Evidence<br />
<div class="outline-text-5" id="text-org5a91d26">
<p>
This is the topic of the thesis. Essentially the idea is that now
you are not dealing with the case of totally unobserved or
missing data. You rather have an uncertain evidence. This means
an evidence that comes with a probability assigned to it.
</p>

<p>
The algorithm is quite intuitive and we will deal with it in this
section. It is a natural extension of the material covered that
far.
</p>

<p>
The idea is the one of using [Pearl 1998]<sup><a id="fnr.4" class="footref" href="#fn.4">4</a></sup> to extend the
bayesian network with new nodes with CPDs representing the
likelihood evidence gained from uncertain data.
</p>

<p>
I.e. this is best explained on an example - consider the ASIA
network as in the paper of reference.
</p>

<img src="../../images/Bildschirmfoto_2021-03-14_um_09.39.27.png" class="center">

<p>
Then consider that you have doctor handwritings stating whether a
person had <code>Dyspoea</code> or not. Such writings are analyzed via NLP
such that whether a patient had Dyspoea or not is just associated
with a <b>likelihood</b>. This is in fact the case of <i>uncertainty</i>
evidence we are interested in. I.e. you do not have a missing
observation but also do not know its value precisely.
</p>

<p>
The idea is then that you can convert such structure into a new
Bayesian Network where you either <b>observe</b> or have <b>missing</b>
evidence. However, you do not have uncertain evidence. In such a
way you can then effectively treat the network in the usual way
as it would ultimately fall in the classical structure.
</p>

<p>
In order to reach such a structure what you do as in Pearl 1998
is to extend the network by adding a node
<code>Variable_Observed</code>. You would then assign to this variable the
realization <code>True</code> i.e. observed, and that would kick in with the
given likelihood given the parent (i.e. kick in with the
likelihood arising from the NLP model). This is quite
straightforward to understand and you can quickly understand then
that like this the situation is well mapped.
</p>

<p>
To make this 100% clear for when you are coming back in a couple
of time. Consider the ASIA network above. Consider you have NLP
that reads the doctor handwritings for the <code>Dyspoea</code> case. With
0.7% the NLP returns <code>Dyspoea = True</code> when Dyspoea occurs.
</p>

<p>
Then you can you construct your network as follows.
</p>

<img src="../../images/Bildschirmfoto_2021-03-14_um_10.18.40.png" class="center">

<p>
With this modification you can therefore treat any uncertain
evidence and convert it to standard Bayesian Networks with just
observed or missing evidence.
</p>

<p>
We will now show that it is possible to apply the EM algorithm to
such augmented network.
</p>

<p>
Consider the EM algorithm in the case of a Bayesian Network
without uncertain evidence. Then the situation would look as
follows:
</p>

<img src="../../images/Bildschirmfoto_2021-03-14_um_10.33.42.png" class="center">

<p>
So you see that it basically consists in the expectation step
where you compute the expected sufficient statistics as
previously described in the <a href="#org2dbc362">expected<sub>suff</sub><sub>stat</sub></a> image above. And
the maximization step where you essentially take the MLE for your
distribution. So basically nothing new compared to what was
discussed above. We simply put this in an algorithm frame.
</p>

<p>
For the case of <b>uncertain evidence</b> this looks very similar, you
just have to augment the algorithm with a step where you would
actually extend and reframe the Bayesian Network to account for
uncertain evidence as previously done.
</p>

<p>
In algorithmic terms we would do the following:
</p>

<img src="../../images/Bildschirmfoto_2021-03-14_um_10.57.39.png" class="center">

<p>
So basically it is 1:1 the same after augmenting the
network. Note that there is some notation inconsistency in the
algorithm above. Some M[.] should be \(\overline{M[.]}\). Discuss
with it with Radu.
</p>

<p>
The only thing that is left over is to show that the properties
of the EM algorithm are not altered by the Network expansion,
i.e. we have to guarantee the same properties for the EM such
that such coordinate ascent argumentation holds and we will
<i>converge to a global maximum</i>.
</p>

<p>
This is possible to see by noting that again you can easily show
that the <b>log-likelihood</b> is <b>linear in the sufficient
statistics</b>. Given this condition as discussed it is then
possible to insert the expected sufficient statistics over the
measure \(\mathscr{Q}(\mathscr{H}) = P(\mathscr{H} | \mathscr{D},
     \theta^t)\) and we will have all of the desired properties.
</p>

<p>
For the MLE you would then insert the MLE solution for the
parameters. This amounts in the case of <b>table CPD</b> to:
</p>

<img src="../../images/Bildschirmfoto_2021-03-14_um_12.42.15.png" class="center" style = "width: 40% !important;">
</div>
</li>
</ul>
</div>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1">1</a></sup> <div class="footpara"><p class="footpara">
Note that With this you would then have in these notes both a
way to write down the likelihood in compact form via sufficient
statistics in the case of multinomial parameterization arising
from the MLE. As well as a way to write in compact form the
posterior distribution in the case of working with bayesian
parameter learning. Note that both assume you are ready to
accept the described prior/parameter space settings.
</p></div></div>

<div class="footdef"><sup><a id="fn.2" class="footnum" href="#fnr.2">2</a></sup> <div class="footpara"><p class="footpara">
Note this again similarly to the MLE case. 
</p></div></div>

<div class="footdef"><sup><a id="fn.3" class="footnum" href="#fnr.3">3</a></sup> <div class="footpara"><p class="footpara">
See the textbook for the exact computation of this. It is
quite straightforward - just an application of the bayes formula.
</p></div></div>

<div class="footdef"><sup><a id="fn.4" class="footnum" href="#fnr.4">4</a></sup> <div class="footpara"><p class="footpara">
Pearl, J. 1988. Probabilistic Reasoning in Intelligent Systems:
Networks of Plausible Inference. San Francisco, CA, USA: Morgan
Kaufmann Publishers Inc. ISBN 1558604790.
</p></div></div>


</div>
</div>
