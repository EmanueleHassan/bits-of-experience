<p>
So I will start to collect a bit of random thoughts about information
theory.
</p>

<p>
I must say that I regret not having properly studied it at Univeristy
time. I remember that I wanted to check at it at some point. I was
especially interested in theory about wavlets and fourier transform.
</p>

<p>
Now I got more interested in it as I think that by approaching
scientifically the noise that is out there you might be able to reduce
it and properly intercept the messages.
</p>

<p>
On the top of it, it's beautiful the way you can apply information
theory to many different scientific fields. 
</p>

<!-- TEASER_END -->


<div id="outline-container-orgdedcaf4" class="outline-2">
<h2 id="orgdedcaf4">On some basic concepts</h2>
<div class="outline-text-2" id="text-orgdedcaf4">
<p>
So this is quite interesting.
</p>

<p>
The basis is in this paper:
</p>

<blockquote>
<p>
Abstractly, information can be thought of as the resolution of
uncertainty. In the case of communication of information over a noisy
channel, this abstract concept was formalized in 1948 by Claude
Shannon in a paper entitled <code>A Mathematical Theory of Communication</code>.
</p>
</blockquote>

<p>
Note that the essential idea is the following:
</p>

<p>
Information is thought of as a set of possible messages, and the
goal is to send these messages over a noisy channel, and to have
the receiver reconstruct the message with low probability of error,
in spite of the channel noise.
</p>

<p>
So note that if somebody wants to convey messages across a channel
it will most likely respect this basic theory.
</p>

<p>
Now there is as well a major result:
</p>

<blockquote>
<p>
Shannon's main result, the noisy-channel coding theorem showed that,
in the limit of many channel uses, the rate of information that is
asymptotically achievable is equal to the channel capacity, a quantity
dependent merely on the statistics of the channel over which the
messages are sent.
</p>
</blockquote>

<p>
So there are now a couple of questions:
</p>

<ul class="org-ul">
<li><i>What are these statistics?</i></li>

<li><p>
<i>Think about the internet. How did it affected these statistics</i>.
</p>

<p>
The channel is getting bigger. But if the information is the
same how do you keep the message alive and keep tranmitting it
<i>with a low frequency of error</i>? Do you increase the repetitions
of the message?
</p>

<p>
The case I have in mind is the one where the receiver is
not known. It is out there in the wild. I do not think that the
theory developed addresses this case. I think it rather assume
that receiver and sender are synchronized. They work along the
same frequency so to say.
</p></li>
</ul>

<p>
Anyways, come back later at it.
</p>
</div>
</div>


<div id="outline-container-orgeeaa93e" class="outline-2">
<h2 id="orgeeaa93e">Some Basic Measures Quantifying Uncertainty</h2>
<div class="outline-text-2" id="text-orgeeaa93e">
</div>
<div id="outline-container-orgfc63a4c" class="outline-3">
<h3 id="orgfc63a4c">Entropy</h3>
<div class="outline-text-3" id="text-orgfc63a4c">
<p>
This is a way to <b>measure</b> the <b>average uncertainty in a random
variable</b>.
</p>

<img src="../../images/Screenshot 2023-06-15 112549.png" class="center">

<p>
Note the log in base 2. That is often used in computer science
given the important of the binary system.
</p>
</div>
</div>

<div id="outline-container-orgf2fa674" class="outline-3">
<h3 id="orgf2fa674">Mutual Information</h3>
<div class="outline-text-3" id="text-orgf2fa674">
<p>
The reduction in uncertainty <i>due to another random variable</i> is
called the mutual information.
</p>

<img src="../../images/Screenshot 2023-06-15 114155.png" class="center">

<p>
So you can see that you can interpret it as the reduction in
average uncertainty from knowing the other random variable. It is
hence as well a measure of dependence among the two random
variables.
</p>
</div>
</div>
</div>


<div id="outline-container-org758fd5b" class="outline-2">
<h2 id="org758fd5b">IN-PROGRESS continue by reading the following</h2>
<div class="outline-text-2" id="text-org758fd5b">
<p>
Continue by reading <a href="https://www.cs.miami.edu/home/burt/learning/Csc524.142/LarsTelektronikk02.pdf">the following</a>. -&gt; Not very good. More giving
background on what generally happened.
</p>

<p>
It goes little in the details of the formulas and you cannot really
understand much from it.
</p>
</div>


<div id="outline-container-org9ee42e6" class="outline-3">
<h3 id="org9ee42e6">Downloaded the book - Elements of Information theory</h3>
<div class="outline-text-3" id="text-org9ee42e6">
<p>
Read it. Should be quite interesting.
</p>
</div>
</div>
</div>
