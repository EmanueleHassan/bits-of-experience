<br>
<br>


<p>
This post continues the discussion started a few times ago on <a href="https://marcohassan.github.io/bits-of-experience/posts/spark-session-initalization/">RDD and
Spark</a>.
</p>

<p>
I will try to go here in the lifecycle of an RDD and will present the
major Transformation and Actions functions. I will moreover touch on
the physical implementation of Spark as this will give you the mental
tools to properly understand how to properly structure your Spark
workflow in order to optimize the performance.   
</p>

<!-- TEASER_END -->

<br>

<div id="outline-container-org0824d02" class="outline-2">
<h2 id="org0824d02">On RDDs</h2>
<div class="outline-text-2" id="text-org0824d02">
<p>
A brief overview on RDDs was given in the previous post and you are
referred to it for a brief introduction.
</p>

<p>
RDDs are lazy. This, means that only if the data is needed for a
certain computation the data is read from the underlying storage
system.
</p>

<p>
An RDD in Spark is simply an immutable distributed collection of
objects. Each RDD can be split into multiple partitions, which may be
computed on different nodes of the cluster.
</p>

<p>
The typical RDD lifecycle is as follows:
</p>

<ul class="org-ul">
<li>An RDDs is first created from stable storage or by some Python objects.</li>
</ul>

<p>
RDDs offer then two types of operations: <b>transformations</b> and <b>actions</b>.
</p>

<ul class="org-ul">
<li><b>Transformations</b> create a new RDD from an existing one.
Transformations are lazy, meaning that no transformation is executed
until you execute an action.</li>

<li><b>Actions</b> compute a result based on an RDD, and either return it to
the driver program or save it to an external storage system (e.g.,
HDFS). This is the end of the lifecycle.</li>
</ul>

<p>
Transformations and actions are different because of the way Spark
computes RDDs. Although you can define new RDDs any time, Spark
computes them only in a <b>lazy</b> fashion, that is, the first time they
are used in an <b>action</b>.
</p>

<p>
For the creation of RDDs and the partitions of them please refer to
the previous post I will now briefly introduce the physical execution
of spark before illustrating some of the most the key transformations
and actions.
</p>

<br>
</div>
</div>


<div id="outline-container-orgdbec995" class="outline-2">
<h2 id="orgdbec995">Transformations</h2>
<div class="outline-text-2" id="text-orgdbec995">
<p>
Following are examples of some of the common transformations
available.
</p>

<p>
For a detailed list, see <a href="https://spark.apache.org/docs/2.0.0/programming-guide.html#transformations">RDD Transformations</a>
</p>

<p>
Run some transformations below to understand this better.
</p>

<p>
<b>Note:</b> If some of the queries are taking too long to complete, try
restarting the kernel, and rerunning the cell <i>above</i>.
</p>


<div class="highlight"><pre><span></span>from pyspark.sql import SparkSession

spark = SparkSession \
    .builder \
    .master (&quot;local[8]&quot;) \
    .appName(&quot;My first Spark Session&quot;) \
    .getOrCreate()

sc = spark.sparkContext
</pre></div>

<div class="highlight"><pre><span></span> fruits = sc.textFile(&#39;file:///Users/marcohassan/Desktop/spark_files_data/fruit.txt&#39;)
</pre></div>

<div class="highlight"><pre><span></span> fruits.collect()
</pre></div>

<pre class="example">
['Peaches',
 'Apples',
 'Strawberries',
 'Grapes',
 'Oranges',
 'Apples',
 'Peaches',
 'Melon']
</pre>
</div>


<div id="outline-container-orgcfc606c" class="outline-3">
<h3 id="orgcfc606c">On the filter transformation</h3>
<div class="outline-text-3" id="text-orgcfc606c">
<p>
A further important transformation is the one allowing the projection
in relational algebra terms. This is taken care by the <code>filter</code>
transformation.
</p>

<p>
It just projects the RDDs objects satisfying the given criteria of
the boolean function specified within the <code>filter</code>.  
</p>

<div class="highlight"><pre><span></span> # filter
 shortFruits = fruits.filter(lambda fruit: len(fruit) &lt;= 5)
 shortFruits.collect()
</pre></div>

<pre class="example">
['Melon']
</pre>



<div class="highlight"><pre><span></span>shortFruits = fruits.filter(lambda fruit: fruit.lower() == &quot;oranges&quot;)
shortFruits.collect()
</pre></div>

<pre class="example">
['Oranges']
</pre>
</div>
</div>

<div id="outline-container-org5842da7" class="outline-3">
<h3 id="org5842da7">On the Map Transformation</h3>
<div class="outline-text-3" id="text-org5842da7">
<p>
The map function takes each element for the RDD and applies a
function <code>mapping</code> the element input to a transformed new RDD. It is
therefore an action.
</p>

<div class="highlight"><pre><span></span> # map
 fruitsReversed = fruits.map(lambda fruit: fruit[::-1]) ## the fruit[::-1] inverts the letters of the word

 # Note: the `collect` command is NOT a Transformation, it is an Action
 # used here for the purposes of showing the results! Just use it when
 # you know that the action will be small enough to be handled by the
 # memeory of the machine you are working on. Otherwise, no chance you
 # will be able to display your results and you will better have to
 # save the results on a HDFS cluster.
 fruitsReversed.collect()
</pre></div>

<pre class="example">
['sehcaeP',
 'selppA',
 'seirrebwartS',
 'separG',
 'segnarO',
 'selppA',
 'sehcaeP',
 'noleM']
</pre>


<div class="highlight"><pre><span></span> # map
 fruitsReversed = fruits.map(lambda fruit: fruit + &quot;bau&quot; * 3) 

 fruitsReversed.collect()
</pre></div>

<pre class="example">
['Peachesbaubaubau',
 'Applesbaubaubau',
 'Strawberriesbaubaubau',
 'Grapesbaubaubau',
 'Orangesbaubaubau',
 'Applesbaubaubau',
 'Peachesbaubaubau',
 'Melonbaubaubau']
</pre>
</div>
</div>


<div id="outline-container-org83361cc" class="outline-3">
<h3 id="org83361cc">On the FlatMap Function</h3>
<div class="outline-text-3" id="text-org83361cc">
<p>
The <code>flatmap</code> transformation extends the <code>map</code> function giving the
possibility to the users to return an object of higher dimension than
the map input and then flattening it in one go.
</p>

<p>
Important is to understand that here the object is <i>flattened</i> and
saved into a new RDD. So that you won't notice the different map
output when observing the transformed RDD. I.e. you won't have
different subsets indicating the higher dimensional objects returned
by your flatmap.
</p>

<div class="highlight"><pre><span></span> # flatMap
 characters = fruits.flatMap(lambda fruit: list(fruit))
 characters.collect()
</pre></div>

<pre class="example">
['P',
 'e',
 'a',
 'c',
 'h',
 'e',
 's',
 'A',
 'p',
 'p',
 'l',
 'e',
 's',
 'S',
 't',
 'r',
 'a',
 'w',
 'b',
 'e',
 'r',
 'r',
 'i',
 'e',
 's',
 'G',
 'r',
 'a',
 'p',
 'e',
 's',
 'O',
 'r',
 'a',
 'n',
 'g',
 'e',
 's',
 'A',
 'p',
 'p',
 'l',
 'e',
 's',
 'P',
 'e',
 'a',
 'c',
 'h',
 'e',
 's',
 'M',
 'e',
 'l',
 'o',
 'n']
</pre>

<p>
Notice how this stays in contrast to a simple map performing the same
operation.
</p>

<div class="highlight"><pre><span></span>characters = fruits.map(lambda fruit: list(fruit))
characters.collect()
</pre></div>

<pre class="example">
[['P', 'e', 'a', 'c', 'h', 'e', 's'],
 ['A', 'p', 'p', 'l', 'e', 's'],
 ['S', 't', 'r', 'a', 'w', 'b', 'e', 'r', 'r', 'i', 'e', 's'],
 ['G', 'r', 'a', 'p', 'e', 's'],
 ['O', 'r', 'a', 'n', 'g', 'e', 's'],
 ['A', 'p', 'p', 'l', 'e', 's'],
 ['P', 'e', 'a', 'c', 'h', 'e', 's'],
 ['M', 'e', 'l', 'o', 'n']]
</pre>
</div>
</div>


<div id="outline-container-org7f361ab" class="outline-3">
<h3 id="org7f361ab">On the Union Transformation</h3>
<div class="outline-text-3" id="text-org7f361ab">
<p>
This returns the classical relational algebra union of two different
RDDs.
</p>

<div class="highlight"><pre><span></span>yellowThings = sc.textFile(&#39;file:///Users/marcohassan/Desktop/spark_files_data/yellow.txt&#39;)
</pre></div>

<div class="highlight"><pre><span></span> # union
 fruitsAndYellowThings = fruits.union(yellowThings)
 fruitsAndYellowThings.collect()
</pre></div>

<pre class="example">
['Peaches',
 'Apples',
 'Strawberries',
 'Grapes',
 'Oranges',
 'Apples',
 'Peaches',
 'Melon',
 'giallo',
 'biondo',
 'yellow',
 'gelb',
 'gold',
 'blond',
 'jeune']
</pre>
</div>
</div>

<div id="outline-container-org573bcc9" class="outline-3">
<h3 id="org573bcc9">On the intersection Transformation</h3>
<div class="outline-text-3" id="text-org573bcc9">
<p>
This returns the intersection for two RDDs. It is then clear that it
is possible to obtain the relational algebra set difference from it.
</p>

<div class="highlight"><pre><span></span> # intersection
 yellowFruits = fruits.intersection(yellowThings)
 yellowFruits.collect()
</pre></div>

<pre class="example">
[]
</pre>


<div class="highlight"><pre><span></span><span class="nb">echo</span> <span class="s2">&quot;Oranges&quot;</span> &gt;&gt; /Users/marcohassan/Desktop/spark_files_data/yellow.txt
</pre></div>

<div class="highlight"><pre><span></span>yellowFruits = fruits.intersection(yellowThings)
yellowFruits.collect()
</pre></div>

<pre class="example">
['Oranges']
</pre>


<p>
Get the set difference, i.e. the objects present in a RDD but not in
the other
</p>

<div class="highlight"><pre><span></span>a = yellowFruits.collect()

fruit = fruits.filter(lambda x: x not in a)
fruit.collect()
</pre></div>

<pre class="example">
['Peaches', 'Apples', 'Strawberries', 'Grapes', 'Apples', 'Peaches', 'Melon']
</pre>
</div>
</div>


<div id="outline-container-orgb99f95a" class="outline-3">
<h3 id="orgb99f95a">On the distinct transformation</h3>
<div class="outline-text-3" id="text-orgb99f95a">
<p>
This allows to take the distinct objects in an RDD
</p>

<div class="highlight"><pre><span></span> # distinct
 distinctFruitsAndYellowThings = fruitsAndYellowThings.distinct()
 distinctFruitsAndYellowThings.collect()
</pre></div>

<pre class="example">
['Peaches',
 'Apples',
 'Oranges',
 'giallo',
 'gelb',
 'blond',
 'Strawberries',
 'jeune',
 'Melon',
 'biondo',
 'yellow',
 'gold',
 'Grapes']
</pre>
</div>
</div>

<div id="outline-container-org750bc1a" class="outline-3">
<h3 id="org750bc1a">On the Reduce Transformation</h3>
<div class="outline-text-3" id="text-org750bc1a">
<p>
The reduce function is powerful albeit its logic is not
straight. You will have to exercise it at first. Its logic is as
follows
</p>

<p>
<img src="/images/Bildschirmfoto_2020-05-04_um_17.54.18.png" alt="nil"/>
</p>

<div class="highlight"><pre><span></span>  input_list = sc.parallelize(range(5))
  print(input_list.collect())

  print(input_list.map(lambda x: x ** 3).collect())

  sum_of_cubes = input_list.map(lambda x: x ** 3).reduce(lambda x, y: x + y)
  product_of_cubes = input_list.map(lambda x: x ** 3).reduce(lambda x, y: x * y)

  print(&quot;\nsum of cubes %s:&quot; % sum_of_cubes)
  print(&quot;product of cubes %s:&quot; % product_of_cubes)
</pre></div>

<pre class="example">
[0, 1, 2, 3, 4]
[0, 1, 8, 27, 64]

sum of cubes 100:
product of cubes 0:
</pre>



<br>
</div>
</div>
</div>


<div id="outline-container-org827223f" class="outline-2">
<h2 id="org827223f">Working with <i>key-value</i> pairs</h2>
<div class="outline-text-2" id="text-org827223f">
<p>
Spark provides special operations on RDDs containing key/value pairs.
These RDDs are called <i>pair RDDs</i>. Pair RDDs are a useful building
block in many programs, as they expose operations that allow you to
<b>act on each key in parallel</b> or regroup data across the network.
</p>
</div>


<div id="outline-container-org043005b" class="outline-3">
<h3 id="org043005b">Group by Key</h3>
<div class="outline-text-3" id="text-org043005b">
<p>
Notice that in spark, in contrast to MapReduce, the input must not be
of key-value store type. But if you are dealing with key-value pairs
objects such as hash-tables then there are plenty of functions that
will help you to deal with and operate at the key level.
</p>

<p>
One prominent example for the above is the <code>groupByKey</code> that allows
you to perform a given function for each key.
</p>

<p>
To understand that look at the following transformation
</p>


<div class="highlight"><pre><span></span>yellowThingsByFirstLetter = yellowThings.map(lambda thing: (thing[0], thing)).groupByKey()

print(yellowThingsByFirstLetter.collect())
</pre></div>

<pre class="example">
[('g', &lt;pyspark.resultiterable.ResultIterable object at 0x119ff27d0&gt;), ('b', &lt;pyspark.resultiterable.ResultIterable object at 0x119ff2490&gt;), ('y', &lt;pyspark.resultiterable.ResultIterable object at 0x119ff2550&gt;), ('j', &lt;pyspark.resultiterable.ResultIterable object at 0x119ff25d0&gt;), ('O', &lt;pyspark.resultiterable.ResultIterable object at 0x119ff2290&gt;)]
</pre>


<p>
The above returned a set of tuples involving a key being the first
letter and a value being an <code>iterable</code> spark object on which it is
possible to perform the desired function.
</p>

<p>
Notice that above we first applied a function generating our key-value tuple
</p>

<div class="highlight"><pre><span></span>print(yellowThings.map(lambda thing: (thing[0], thing)).collect())
</pre></div>


<pre class="example">
[('g', 'giallo'),
 ('b', 'biondo'),
 ('y', 'yellow'),
 ('g', 'gelb'),
 ('g', 'gold'),
 ('b', 'blond'),
 ('j', 'jeune'),
 ('O', 'Oranges')]
</pre>


<p>
and then grouping based on the keys of such newly transformed RDD.
</p>

<p>
For the function you can apply, you can be as imaginative as you want
</p>

<div class="highlight"><pre><span></span> # groupByKey
 yellowThingsByFirstLetter = yellowThings.map(lambda thing: (thing[0], thing)).groupByKey()
 for letter, lst in yellowThingsByFirstLetter.collect():
	 print(&quot;For letter&quot;, letter)
	 for obj in lst:
		 print(&quot; &gt; &quot;, obj)
</pre></div>

<pre class="example">
For letter g
 &gt;  giallo
 &gt;  gelb
 &gt;  gold
For letter b
 &gt;  biondo
 &gt;  blond
For letter y
 &gt;  yellow
For letter j
 &gt;  jeune
For letter O
 &gt;  Oranges
</pre>

<div class="highlight"><pre><span></span>a = [1,2,3,4]

list(map(lambda x: x+1, a))
</pre></div>

<pre class="example">
[2, 3, 4, 5]
</pre>



<div class="highlight"><pre><span></span>print(yellowThingsByFirstLetter.map(lambda x : (x[0], list(x[1]))).collect())

print(yellowThingsByFirstLetter.map(lambda x : (x[0], [i + &quot; &quot; + i for i in list(x[1])] )).collect())

## notice finally that maps exists in python also outside of spark
## with the following syntax map(function, iterable object)
## it is clear therefore that you can do games such as
print(yellowThingsByFirstLetter.map(lambda x : (x[0], list(map(lambda y: y*2, list(x[1]))))).collect())
</pre></div>

<pre class="example">
[('g', ['giallo', 'gelb', 'gold']), ('b', ['biondo', 'blond']), ('y', ['yellow']), ('j', ['jeune']), ('O', ['Oranges'])]
[('g', ['giallo giallo', 'gelb gelb', 'gold gold']), ('b', ['biondo biondo', 'blond blond']), ('y', ['yellow yellow']), ('j', ['jeune jeune']), ('O', ['Oranges Oranges'])]
[('g', ['giallogiallo', 'gelbgelb', 'goldgold']), ('b', ['biondobiondo', 'blondblond']), ('y', ['yellowyellow']), ('j', ['jeunejeune']), ('O', ['OrangesOranges'])]
</pre>
</div>
</div>


<div id="outline-container-orge17a695" class="outline-3">
<h3 id="orge17a695">On the Mapvalues Transformation</h3>
<div class="outline-text-3" id="text-orge17a695">
<p>
Mapvalues operates on PairRDDs, i.e. data of the key-value form,
meaning RDDs of the form <code>RDD[(A, B)]</code>. In that case, mapValues
operates on the value only (the second part of the tuple), while map
operates on the entire record (tuple of key and value).
</p>

<div class="highlight"><pre><span></span>print(&quot;Original data: %s&quot; % numFruitsByLength.collect())

print(&quot;Mapvalued data: %s&quot; % numFruitsByLength.mapValues(lambda x: x*3).collect())
</pre></div>

<pre class="example">
Original data: [(4, 2), (5, 2), (6, 4)]
Mapvalued data: [(4, 6), (5, 6), (6, 12)]
</pre>


<p>
Notice that this might reduce by a product the complexity of your
code. Think for instance at:
</p>

<div class="highlight"><pre><span></span>aba = sc.parallelize([(&quot;hel&quot;, 7), (&quot;hel&quot;, 9), (&quot;bye&quot;, 5), (&quot;bye&quot;, 1),  (&quot;bye&quot;, 1)])

print(&quot;Number per key: %s&quot; % aba.groupByKey().map(lambda x : (x[0], len(list(x[1])))).collect())

# vs.

print(&quot;Number per key: %s&quot; % abba.groupByKey().mapValues(len).collect())
</pre></div>

<pre class="example">
Number per key: [('hel', 2), ('bye', 3)]
Number per key: [('hel', 2), ('bye', 2)]
</pre>
</div>
</div>


<div id="outline-container-orgcfe9e12" class="outline-3">
<h3 id="orgcfe9e12">On the Reduce by Key Transformation</h3>
<div class="outline-text-3" id="text-orgcfe9e12">
<p>
Given the above understanding of the Reduce transformation it is
clear that the reduce by key is essentially a reduce function on
each of the key iterable values.
</p>

<div class="highlight"><pre><span></span>  fruits = sc.parallelize([&quot;apple&quot;, &quot;orange&quot;, &quot;java&quot;, &quot;call++&quot;])

  numFruitsByLength = fruits.map(lambda fruit: (len(fruit), 2))
  print(&quot;Data : %s&quot; % numFruitsByLength.collect())

  numFruitsByLength = numFruitsByLength.reduceByKey(lambda x, y: x + y)
  print(&quot;Sum value by keys: %s&quot; % numFruitsByLength.collect())
</pre></div>

<pre class="example">
Data : [(5, 2), (6, 2), (4, 2), (6, 2)]
Sum value by keys: [(4, 2), (5, 2), (6, 4)]
</pre>



<br>


<br>
</div>
</div>
</div>

<div id="outline-container-orga646536" class="outline-2">
<h2 id="orga646536">Actions</h2>
<div class="outline-text-2" id="text-orga646536">
<p>
As mentioned above Actions compute a result based on an RDD, and
either return it to the driver program or save it to an external
storage system (e.g., HDFS). This is the end of the lifecycle.
</p>

<p>
The most prominent example of an action is <code>collect()</code>. Important is
however to keep in mind that this should be used only when you are
sure that your local hardware might be able to deal with the
collected RDD. We are dealing with <code>Big Data</code> and therefore your
operations should have compressed enough your problem so that you
might able to collect something in a meaningful way. The alternative,
to save to HDFS might be otherwise necessary. 
</p>

<p>
Following are examples of some of the common actions available. For a
detailed list, see <a href="https://spark.apache.org/docs/2.3.0/programming-guide.html#actions">RDD Actions</a>.
</p>
</div>

<div id="outline-container-org811f61c" class="outline-3">
<h3 id="org811f61c">Count</h3>
<div class="outline-text-3" id="text-org811f61c">
<div class="highlight"><pre><span></span>   # count
   print(&quot;Data: %s&quot; % fruits.collect())
   numFruits = fruits.count()
   numFruits
</pre></div>

<pre class="example">
Data: ['apple', 'orange', 'java', 'call++']

4
</pre>
</div>
</div>


<div id="outline-container-orga15368d" class="outline-3">
<h3 id="orga15368d">Take</h3>
<div class="outline-text-3" id="text-orga15368d">
<p>
This might be especially useful also given the issues when dealing
with <code>collect()</code> mentioned above.
</p>

<p>
It is essentially the <code>LIMIT</code> function of SQL
</p>

<div class="highlight"><pre><span></span>    # take
    first3Fruits = fruits.take(3)
    first3Fruits
</pre></div>

<pre class="example">
['apple', 'orange', 'java']
</pre>
</div>
</div>


<div id="outline-container-org857150c" class="outline-3">
<h3 id="org857150c">Aggregate</h3>
<div class="outline-text-3" id="text-org857150c">
<p>
This is again a tricky one. Please
refer to the link: <a href="https://stackoverflow.com/questions/28240706/explain-the-aggregate-functionality-in-spark/38949457">Explaination of Aggregate</a>. 
</p>

<div class="highlight"><pre><span></span>    seqOp = (lambda local_result, list_element: (local_result[0] + list_element, local_result[1] + 1) )
    combOp = (lambda some_local_result, another_local_result: (some_local_result[0] + another_local_result[0], some_local_result[1] + another_local_result[1]) )

    sc.parallelize([1, 2, 1, 2]).aggregate((0, 0), seqOp, combOp)
</pre></div>

<pre class="example">
(6, 4)
</pre>




<br>
</div>
</div>
</div>


<div id="outline-container-orgda1b005" class="outline-2">
<h2 id="persistence-caching">Persistence (Caching)</h2>
<div class="outline-text-2" id="text-persistence-caching">
<p>
Spark's RDDs are by default recomputed each time you run an action on
them. 
</p>

<p>
This is however suboptiomal. Just think of the following situation:
</p>

<img width="100%" height="100%" src="../../images/Bildschirmfoto_2020-05-23_um_14.01.28.png" class="center">

<p>
Then it is clear that no matter the final RDD transformation, you will
have to do the same set of operations. In such a case recomputing the
entire DAG makes no sense and persisting the result that is
continuously used makes sense.
</p>

<p>
This is done using <code>RDD.persist()</code>. After computing it the first time,
Spark will store the RDD contents in memory (partitioned across the
machines in your cluster), and reuse them in future actions.
Persisting RDDs <b>on disk</b> instead of <b>memory</b> is also possible.
</p>

<p>
If you attempt to cache too much data to fit in memory, Spark will
automatically evict old partitions using a Least Recently Used (LRU)
cache policy. This practically means, that for the <b>memory-only
storage</b> levnels, it will recompute these partitions the next time they
are accessed, while for the <b>memory-and-disk ones</b>, it will write them
out to disk. In either case, this means that you don't have to worry
about your job breaking if you ask Spark to cache toxo much
data. However, caching unnecessary data can lead to eviction of useful
data and more recomputation time. Finally, RDDs come with a method
called <code>unpersist()</code> that lets you manually remove them from the
cache.
</p>


<br>
</div>
</div>

<div id="outline-container-org65798f5" class="outline-2">
<h2 id="org65798f5">Literature</h2>
<div class="outline-text-2" id="text-org65798f5">
<p>
<a href="https://www.systems.ethz.ch/courses/spring2020/bigdataforeng/material">Big Data for Engineers - ETH course</a>
</p>

<p>
<a href="https://stackoverflow.com/questions/36696326/map-vs-mapvalues-in-spark">MapValues Stackoverflow</a>
</p>
</div>
</div>
