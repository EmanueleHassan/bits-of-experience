<p>
Ok. So it might be that for the next project I will work one more time
with Spark.
</p>

<p>
This time in production and it is quite a heavy project that should
last for quite some time.
</p>

<p>
All depends how I well I will perform tomorrow at the interview. So
now I am taking some more time to go over the material.
</p>

<p>
One especially interesting feature that I already used before; but
unfortunately I did not make notes for, is the possibility of
specifying particular functions via the PySpark raw API and make them
available throughout your spark session when working via data-frames -
be it via SparkSQL or via pandas.
</p>

<p>
Note that this notes are based on <a href="https://jaceklaskowski.gitbooks.io/mastering-spark-sql/content/spark-sql-udfs.html">this source</a>. Looks like a very
exhaustive source. So check at it again should this project really
start. There you might make sense of all of that. 
</p>

<!-- TEASER_END -->

<div id="outline-container-org5adf0fc" class="outline-2">
<h2 id="org5adf0fc">User Defined Functions</h2>
<div class="outline-text-2" id="text-org5adf0fc">
<p>
So the usual way to work to specify the functions you would need to
work with is via <b>user-defined-functions</b>.
</p>

<p>
This is a very neat feature when working with Spark as it will
allow you to define functions you can then apply to data-frames.
</p>

<p>
So if you are working with data in tabular format it might well
makes sense to work in such a way leveraging such customizable
functions.
</p>

<p>
The idea of udf when working with Python is now the following:
</p>

<blockquote>
<p>
User-Defined Functions (aka UDF) is a feature of Spark SQL to
define new Column-based functions that extend the vocabulary of
Spark SQL’s DSL for transforming Datasets.
</p>

<p>
Historically, user-defined functions operate one-row-at-a-time, and thus
suffer from high serialization and invocation overhead. As a
result, many data pipelines define UDFs in Java and Scala and then
invoke them from Python.
</p>

<p>
Now, with the rise of <a href="https://arrow.apache.org/">Apache Arrow</a>, it is now possible to define
low-overhead, high-performant user-defined functions directly in
Python.
</p>
</blockquote>

<p>
Specifically so far it was just possible in python to use the
SparkSQL user-defined functions and perform row by row operations.
</p>

<p>
You can think for instance at the following:
</p>

<div class="highlight"><pre><span></span>   <span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">udf</span>

   <span class="c1"># Use udf to define a row-at-a-time udf</span>
   <span class="nd">@udf</span><span class="p">(</span><span class="s1">&#39;double&#39;</span><span class="p">)</span>
   <span class="c1"># Input/output are both a single double value</span>
   <span class="k">def</span> <span class="nf">plus_one</span><span class="p">(</span><span class="n">v</span><span class="p">):</span>
	 <span class="k">return</span> <span class="n">v</span> <span class="o">*</span> <span class="n">v</span>

   <span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s1">&#39;v2&#39;</span><span class="p">,</span> <span class="n">plus_one</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">v</span><span class="p">))</span>

   <span class="c1">## Other possible way</span>

   <span class="k">def</span> <span class="nf">squared</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
     <span class="k">return</span> <span class="n">s</span> <span class="o">*</span> <span class="n">s</span>

   <span class="n">spark</span><span class="o">.</span><span class="n">udf</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="s2">&quot;squaredWithPython&quot;</span><span class="p">,</span> <span class="n">squared</span><span class="p">)</span>
</pre></div>

<p>
Vis à vis the following new vectorized udf or stated differently
pandas-udf:
</p>

<div class="highlight"><pre><span></span>   <span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

   <span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">col</span><span class="p">,</span> <span class="n">pandas_udf</span>
   <span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="n">LongType</span>

   <span class="c1"># Declare the function and create the UDF</span>
   <span class="k">def</span> <span class="nf">multiply_func</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
       <span class="k">return</span> <span class="n">a</span> <span class="o">*</span> <span class="n">b</span>

   <span class="n">multiply</span> <span class="o">=</span> <span class="n">pandas_udf</span><span class="p">(</span><span class="n">multiply_func</span><span class="p">,</span> <span class="n">returnType</span><span class="o">=</span><span class="n">LongType</span><span class="p">())</span>

   <span class="c1"># The function for a pandas_udf should be able to execute with local Pandas data</span>
   <span class="n">x</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
   <span class="nb">print</span><span class="p">(</span><span class="n">multiply_func</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">))</span>
   <span class="c1"># 0    1</span>
   <span class="c1"># 1    4</span>
   <span class="c1"># 2    9</span>
   <span class="c1"># dtype: int64</span>

   <span class="c1"># Create a Spark DataFrame, &#39;spark&#39; is an existing SparkSession</span>
   <span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">]))</span>

   <span class="c1"># Execute function as a Spark vectorized UDF</span>
   <span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">multiply</span><span class="p">(</span><span class="n">col</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">),</span> <span class="n">col</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
   <span class="c1"># +-------------------+</span>
   <span class="c1"># |multiply_func(x, x)|</span>
   <span class="c1"># +-------------------+</span>
   <span class="c1"># |                  1|</span>
   <span class="c1"># |                  4|</span>
   <span class="c1"># |                  9|</span>
   <span class="c1"># +-------------------+</span>
</pre></div>
</div>
</div>

<div id="outline-container-orga013869" class="outline-2">
<h2 id="orga013869">Note and understand the difference between vectorized udf and toPandas</h2>
<div class="outline-text-2" id="text-orga013869">
<p>
Note that the two are quite different.
</p>

<p>
Although it might come naturally to work with pandas and so you
might be tempted to call the convert <code>toPandas ()</code> function and
work with Pandas methods etc. this is not recommended as the entire
benefit of working with distributed data would be gone.
</p>

<p>
In this sense, when calling the spark method what you would
actually do is to 
</p>






<p>
spark first class citizens to pandas 
</p>
</div>
</div>
