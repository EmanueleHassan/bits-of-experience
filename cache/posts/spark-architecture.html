<br>
<br>

<p>
After the different posts written on setting up a spark session and
working with RDDs <a href="https://marcohassan.github.io/bits-of-experience/categories/spark/">available here</a>, this post briefly goes a more into
the detail of the spark physical layer.
</p>

<p>
It will introduce the concept of DAGs and will make a comparison of
spark and the more restrictive <a href="https://marcohassan.github.io/bits-of-experience/posts/mapreduce/">MapReduce</a>. 
</p>

<!-- TEASER_END -->

<br>

<div id="outline-container-org6bf58f6" class="outline-2">
<h2 id="org6bf58f6">Position Relative to MapReduce</h2>
<div class="outline-text-2" id="text-org6bf58f6">
<p>
Albeit both MapReduce and Spark are the two major architectures that
make parallel computation on distributed networks possible there are
two key differences between the two that is important to understand.
</p>

<p>
The first one is on the first class citizens of the two. 
</p>

<ul class="org-ul">
<li>While MapReduce works with key-value objects and restricts the user
to work with such, i.e. obligate to rephrase tabular and tree data
with a index column representing the key</li>

<li>Spark first class citizens are RDDs discussed in the <a href="https://marcohassan.github.io/bits-of-experience/posts/spark-session-initalization/">previous
post</a>. No strict key-value is necessary and on the logical level you
might even think of the data as being of their natural shape when
writing your queries and/or performing the desired computations.</li>
</ul>


<p>
The second concerns the topology of the computation. While in
MapReduce each MapReduce operation requires the output of a key-value
tuple per key-value input in the mapping phase as well as a possible
aggregation of key-values in the reducing phase and there is therefore
some linearity of the operations, Spark topology relies on <b>DAGs</b>,
i.e. a <b>directed acyclic graph</b>.
</p>

<br>
<br>

<img width="100%" height="100%" src="../../images/Bildschirmfoto_2020-05-24_um_11.56.34.png" class="center">

<br>
<br>

<p>
It is clear that being the graph directed there is one way the
operation works and no "inverse function" can be performed going back
to the previously existing RDD. The inverse function must be manually
written by the user and must generate a new RDD.
</p>

<p>
Moreover, it is acyclic. This is necessary as would there be a cycle
than you would run into a <b>chicken egg problem</b>, in CS terms a
<b>deadlock</b>. You would need a not at the time existing RDD to perform
some operation.
</p>

<p>
Finally, due to the graph component it is possible to generate
multiple RDDs from a single one in one operation. In this sense in
contrast to MapReduce where simply a <b>reduce</b> operation exists, spark
provides the option also for exploding an RDD so to say.
</p>
</div>
</div>


<div id="outline-container-orgd81811e" class="outline-2">
<h2 id="orgd81811e">On the physical layer</h2>
<div class="outline-text-2" id="text-orgd81811e">
<p>
The architecture of Spark is analogous to the one of MapReduce
<a href="https://marcohassan.github.io/bits-of-experience/posts/mapreduce/">discussed here</a> with the YARN improvement <a href="https://marcohassan.github.io/bits-of-experience/posts/yarn/">discussed here</a>.
</p>

<p>
The architecture is always of Master-Slave type, each nodes has
slots/containers - i.e. virtualized resources attached - and simply
the type of computations, i.e. the maps, carried out on each core
executor is different in the two architectures. 
</p>

<p>
The more specific idea is then the following for each DAG job you need
to perform spark breaks this into different tasks that are assigned to
the different containers (executors in spark terminology) and within
each executor such tasks are further assigned to the different virtual
cores.
</p>

<br>
<br>

<img width="100%" height="100%" src="../../images/Bildschirmfoto_2020-05-24_um_12.53.39.png" class="center">

<br>
<br>

<p>
The question that arises is how such tasks arise?
</p>

<p>
To understand that we have to go back and consider again the expensive
shuffling and the key objective of minimizing this in the space of
distributed computing.
</p>

<p>
What spark does in order to fulfill this requirement is to group a
series of RDD transformations together in a way that these can be
computed locally without the need of shuffling data around. Such a
vertical grouping of transformations is called a <b>stage</b>.
</p>

<p>
A spark job is defined then by the sequence of all of the <b>stages</b>. 
</p>

<p>
In this context tasks are then inferred from the stages by performing
the desired in Spark specified maps in an optimized way. Multiple
tasks from different local stages might be done concurrently as long
as it is possible to parallelize over them.
</p>

<br>
<br>

<img width="100%" height="100%" src="../../images/Bildschirmfoto_2020-05-24_um_14.46.20.png" class="center">

<br>
<br>

<p>
As final note, recall that when storing data on HDFS it is important
always to hold the most homogeneous data on which the most common operations
are done on the same or on neighboring HDFS 128 MB blocks. This will
allow you to have the minimal amount of possible shuffling when
operating on your data. In this sense, if data are not physically
stored in the way it makes the most sense for you and your typical
queries/computation, then it makes sense to pre-partition the data by
doing the operation once and then persisting your results. 
</p>

<br>
<br>

<img width="100%" height="100%" src="../../images/Bildschirmfoto_2020-05-24_um_14.59.23.png" class="center">


<br>
<br>
<br>
</div>
</div>


<div id="outline-container-orge80d3b8" class="outline-2">
<h2 id="orge80d3b8">Literature</h2>
<div class="outline-text-2" id="text-orge80d3b8">
<p>
<a href="https://www.systems.ethz.ch/courses/spring2020/bigdataforeng/material">Big Data for Engineers - ETH course</a>
</p>
</div>
</div>
