<style>
img {
  display: block;
  margin-left: auto;
  margin-right: auto;
}
</style>

<br>
<br>

<p>
This post briefly introduce two key concepts for representing NLP
text data. The first is simply based on some frequency metric given a
text corpus, the second <code>word2vec</code> tries a more elegant and elaborate
approach by trying to extract the semantical representation of words
in a low dimensional space.
</p>

<p>
Both will be briefly explained. 
</p>

<!-- TEASER_END -->

<br>

<div id="outline-container-orgcefb408" class="outline-2">
<h2 id="orgcefb408">Approach 1: On a Frequency Representation</h2>
<div class="outline-text-2" id="text-orgcefb408">
<p>
The first approach consists in directly leveraging the vocabulary
associated with the category that you aim to categorize.
</p>

<p>
This is then essentially the idea of the algorithm that you have
created manually for the SRZ project. The idea is simply to use a
more standard ML model for classification such as logistic
regression, SVM or Naive Bayes.
</p>

<p>
The concept is however always the same. Get all the words related to
one category and assign the new text to a category based on a metric
using such bag of words. If a new corpus of words arrives that has no
relation to the one observed in the training dataset, then
well&#x2026; your algorithm will completely fail the classification task.
</p>

<p>
More explicitly, The bag of words associated with a specific category
gives rise to the features matrix. This is a vector representation of
the words. A straight representation could be for instance a
one-hot-encoding of the words available for each class. But more
advanced vector representations involving the term frequency might be
used.
</p>

<p>
Some quick introduction to the idea behind some standard
representations belonging to this category might be found under the
<a href="https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/">following link</a>.
</p>

<p>
You can then apply standard classification algorithms to such vector
representation of your text corpus.
</p>


<br>
</div>
</div>


<div id="outline-container-org7cc56fa" class="outline-2">
<h2 id="org7cc56fa">Approach 2: On a Semantic Representation</h2>
<div class="outline-text-2" id="text-org7cc56fa">
<p>
A second technique that is more involving but conceptually cleaner is
the one of using Word2Vec embeddings. This consists in extracting the
semantic meaning of a word by looking in which context it
appears. I.e. it inspects the "friends" of the word to understand the
word itself. This is a nice <a href="https://en.wikipedia.org/wiki/Anthropomorphism#In_film,_television,_and_video_games">anthropomorphic</a> way to think about the
approach I recently read on a blog, which I particularly enjoyed.
</p>

<p>
You can get the gist of the idea behind word2vec <a href="https://towardsdatascience.com/word-to-vectors-natural-language-processing-b253dd0b0817">here</a>.
</p>

<p>
The mathematical idea is essentially to extract a vector
representation for each word based on a 1-layered neural network
trained on your corpus of contex-related data.
</p>

<p>
More concretely you can think about it in the following simplified
way: given a N-bag of N-related words around a center word <b>V<sub>C</sub></b>
represented by a vector, think of a hidden layer of N-nodes (neurons)
assigning weights to each of the input N-words such that the
probability of observing the N-bag of words given the central word is
maximized. Such weights would represent then the vector
representation of each word.
</p>

<br>
<br>

<img width="70%" height="100%" src="../../images/Bildschirmfoto_2020-05-20_um_21.24.25.png" class="center">

<br>
<br>

<p>
The probability is then represented in the most simple case by the
softmax funtion of the doct-products of the N-bag of words vectors
and the centroid vector.
</p>

<br>
<br>

<img width="60%" height="60%" src="../../images/Bildschirmfoto_2020-05-20_um_20.11.40.png" class="center">


<br>
<br>

<p>
The weights are obtained by standard backpropagation with a loss
function representing the cross entropy between the truth vector and
the softmax "probability" vector. 
</p>

<p>
The final architecture is well summarized on the below picture.
</p>

<br>
<br>

<img width="80%" height="80%" src="../../images/Bildschirmfoto_2020-05-20_um_22.13.03.png" class="center">

<br>
<br>

<p>
For a more in depth analysis of the key mathematical ideas of
word2vec the following video is advised:
</p>

<br>
<br>

<style>
.container {
  position: relative;
  left: 15%;
  width: 70%;
  overflow: hidden;
  padding-top: 56.25%; /* 16:9 Aspect Ratio */
  display:block;
  overflow-y: hidden;
}

.responsive-iframe {
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  right: 0;
  width: 100%;
  height: 100%;
  border: none;
  display:block;
  overflow-y: hidden;
}
</style>

<div class="container"> 
  <iframe class="responsive-iframe" src="https://www.youtube.com/embed/ERibwqs9p38" frameborder="0" allowfullscreen;> </iframe>
</div>

<br>
<br>

<p>
Interesting is then the choice on how you define the bag of N-words
used by the algorithm. A quick introduction to the topic might be
found <a href="https://kavita-ganesan.com/comparison-between-cbow-skipgram-subword/">here</a>.
</p>

<p>
Notice, that the approach not only has the benefit of being
theoretically more sound but it also gives the advantage that if the
semantic embeddings of your text corpus have been properly inferred
via ML algorithms and can be represented in low dimensional space you
might start classifying your text corpus based on the geometrical
representation of your classes and the text you aim to classify. The
idea is that if the text and the class share the same semantical
meaning they should lie in some close geometrical subspace. Using
some unsupervised algorithm to define such geometrical subspace (or
even using some simple metrics) you can then properly classify.
</p>
</div>
</div>
