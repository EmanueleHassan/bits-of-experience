<br>
<br>


<p>
This second post present the basic set up of a Spark session and goes
over the basic transformations and actions that applies to Spark
RDDs. These are necessary given the immutability of RDDs.
</p>

<p>
RDDs are saved in-memory by default. Nonetheless when the memory of
the machine is not sufficient to handle the data, RDDs are splitted to
disk.
</p>

<p>
Finally, RDDs are lazy. This, means that only if the data is needed
for a certain computation the data is read from the underlying storage
system.
</p>

<!-- TEASER_END -->

<div id="outline-container-sec-1" class="outline-2">
<h2 id="sec-1">Session Set Up</h2>
<div class="outline-text-2" id="text-1">
<p>
:header-args:ipython: :session kernel-11036.json :exports both :results output
</p>

<p>
First of all it is necessary to create entry point to programming
Spark with the Dataset and DataFrame API. This is done through the
SparkSession function of the pyspark module. This will allow you to
create DataFrame, register DataFrame as tables, execute SQL over
tables, cache tables, and read parquet files (a special kind of files
particularly suited for big data storage and processing).
</p>

<p>
When you initiate a spark session you can specify a multitude of
arguments that will affect the general scope of your session. You can
find a reference for session arguments under <a href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SparkSession">this official Spark link</a>.
The configuration parameters specified will be passed from your Spark
driver application to the SparkContext. Some of these parameters define properties of your
Spark driver application and some are used by Spark to allocate
resources on the cluster such as, the number, memory size and cores
uses by the executors running on the workernodes.
</p>

<p>
Among the other I underline:
</p>

<ul class="org-ul">
<li>master = ["local", "local<sup><a id="fnr.1" name="fnr.1" class="footref" href="#fn.1">1</a></sup>", “spark://master:7077”], where the
first element is setting the Spark master locally and the second
sets the Spark master locally with 4 cores, while the third option
is an example of selecting a Spark standalone cluster.
</li>

<li>config = either a key value pair of an existing <i>SparkConf</i> containing the
configuration arguments or the <i>SparkConf</i> object itself.
</li>

<li>getOrCreate() = Gets an existing SparkSession or, if there is no
existing one, creates a new one based on the options set in this
builder.
</li>
</ul>

<div class="highlight"><pre><span></span>import findspark
findspark.init()

from pyspark.sql import SparkSession

spark = SparkSession \
    .builder \
    .master (&quot;local&quot;) \
    .appName(&quot;My first Spark Session&quot;) \
    .getOrCreate()
</pre></div>

<p>
Good you just created your first Spark Session. In the next section I
will start to check at the different data import options and data tweaks.
</p>
</div>
</div>

<div id="outline-container-sec-2" class="outline-2">
<h2 id="sec-2">Data import</h2>
<div class="outline-text-2" id="text-2">
<div class="highlight"><pre><span></span>df = spark.sparkContext.parallelize([(1, 2, 3, &#39;a b c&#39;),
				   (4, 5, 6, &#39;d e f&#39;),
				   (7, 8, 9, &#39;g h i&#39;)]).toDF([&#39;col1&#39;, &#39;col2&#39;, &#39;col3&#39;,&#39;col4&#39;])

df.show ()
</pre></div>
</div>
</div>


<div id="outline-container-sec-3" class="outline-2">
<h2 id="sec-3">Literature</h2>
<div class="outline-text-2" id="text-3">
<p>
<a href="https://runawayhorse001.github.io/LearningApacheSpark/rdd.html">https://runawayhorse001.github.io/LearningApacheSpark/rdd.html</a>
</p>

<p>
<a href="https://blogs.msdn.microsoft.com/bigdatasupport/2015/09/14/understanding-sparks-sparkconf-sparkcontext-sqlcontext-and-hivecontext/">https://blogs.msdn.microsoft.com/bigdatasupport/2015/09/14/understanding-sparks-sparkconf-sparkcontext-sqlcontext-and-hivecontext/</a>
</p>

<p>
<a href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SparkSession">http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SparkSession</a>
</p>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" name="fn.1" class="footnum" href="#fnr.1">1</a></sup> <p>DEFINITION NOT FOUND.</p></div>


</div>
</div>
