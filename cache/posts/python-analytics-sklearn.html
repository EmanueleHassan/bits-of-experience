<p>
This is a repository where I hold some notes about the major sklearn
analytics algorithm in order to perform your data science. 
</p>


<!-- TEASER_END -->


<div id="outline-container-org77ce705" class="outline-2">
<h2 id="org77ce705">General Pattern</h2>
<div class="outline-text-2" id="text-org77ce705">
<p>
The standard way of working with your data is first performing the
relevant exploratory data.
</p>

<p>
For it you can refer to the relevant separate post.
</p>

<p>
There you would inspect the data both analytically as well as
visually.
</p>

<p>
The goal is to understand what is available in the data.
</p>

<p>
The distribution of the data etc. 
</p>

<p>
In such a way it would be possible for you to create the relevant
features from the data. 
</p>

<p>
When creating the features it will be important to <b>transform</b> the
data such that they take a meaningful shape. You might need to
one-hot-encode, standardize, impute etc.  Note that you might fit
data as well in order to get to the transform the data
accordingly. In this sense, you will find a <code>fit_transform</code> API in
sklearn.
</p>

<p>
After the data have been properly transformed it is time to fit the
actual analytical model. Here you would take one of the relevant <b>supervised</b> /
<b>unsupervised</b> models and you would ultimately fit them to the
main chunck of the data. 
</p>

<p>
Finally, you would estimate / predict a new existing chunck of the
data given your fitted model and its parameters.
</p>
</div>


<div id="outline-container-org7d92ca2" class="outline-3">
<h3 id="org7d92ca2">Fit</h3>
</div>




<div id="outline-container-org69111c5" class="outline-3">
<h3 id="org69111c5">Transform</h3>
<div class="outline-text-3" id="text-org69111c5">
<p>
Here are mentioned some of the classical techniques for tranforming
your data in order to make them meaningful / well behaved.
</p>
</div>


<div id="outline-container-orgbc6973e" class="outline-4">
<h4 id="orgbc6973e">The preprocessor object</h4>
<div class="outline-text-4" id="text-orgbc6973e">
<p>
In order to pre-process the data sklearn offers the preprocessor
object class.
</p>


<p>
There you have many different APIs that are useful in order to
process the data.
</p>

<p>
We will explore them next.
</p>
</div>

<ul class="org-ul">
<li><a id="orgc195c92"></a>Dataset<br />
<div class="outline-text-5" id="text-orgc195c92">
<p>
In this section I will work with the datasets shipped with the
sklearn package.
</p>

<p>
In order to use the data use the following:
</p>

<div class="highlight"><pre><span></span>     from sklearn.datasets import fetch_california_housing as fdh
     import pandas as pd
     import numpy as np
</pre></div>

<div class="highlight"><pre><span></span>     df_housing = fdh (as_frame=1) [&#39;data&#39;]

     df_housingValue = fdh (as_frame=1)[&#39;target&#39;]

     df_housing.head (5)
</pre></div>
</div>
</li>



<li><a id="org0461a61"></a>Binarizer<br />
<div class="outline-text-5" id="text-org0461a61">
<p>
This converts data into binary given a certain threshold. 
</p>

<p>
You could for instance create a future in the above for checking
whether the data are in the northern hemisphere or in the
southern one in the following way:
</p>

<div class="highlight"><pre><span></span>     from sklearn.preprocessing import Binarizer
</pre></div>

<div class="highlight"><pre><span></span>     binary = Binarizer () ## note that it holds a parameter threshold. The
			   ## default is 0 and is good for the exercise of
			   ## Northern and Southern Hemisphere.

     binary.fit (np.array (df_housing [&quot;Latitude&quot;]).reshape (-1,1)) ## Note
								    ## that
								    ## the
								    ## Binarizer
								    ## takes
								    ## a 2D
								    ## array
								    ## as
								    ## input. You
								    ## therefore
								    ## need
								    ## to
								    ## reshape
								    ## your
								    ## data
								    ## and
								    ## wrap
								    ## your
								    ## array
								    ## into
								    ## a
								    ## list.
</pre></div>


<div class="highlight"><pre><span></span>     df_housing [&quot;Northern Hemisphere&quot;] = binary.transform (np.array (df_housing [&quot;Latitude&quot;]).reshape (-1,1)).flatten () ## Flatten
															  ## out
															  ## back
															  ## to
															  ## a
															  ## 1D
															  ## array
</pre></div>


<div class="highlight"><pre><span></span>     df_housing.head (5)
</pre></div>



<p>
Note that you can specify the threshold yourself. Think about the
case of Housing age. You want to create a binary if older than
the age or not.
</p>

<p>
You can then use the following:
</p>

<div class="highlight"><pre><span></span>     df_housing [&quot;OldHouse&quot;] = Binarizer (threshold= df_housing.HouseAge.mean ()) \
			       .fit_transform (np.array (df_housing [&quot;HouseAge&quot;]).reshape (-1,1)).flatten ()
</pre></div>


<div class="highlight"><pre><span></span>     df_housing.head (10)
</pre></div>
</div>
</li>


<li><a id="org520a2e4"></a>QuantileTransformer<br />
<div class="outline-text-5" id="text-org520a2e4">
<p>
Note that this estimates the quantiles according to a parameteric
distribution that you impose on the data. 
</p>

<p>
It does not compute the quantiles based on the empirical
distribution of the data. Check in this sense the <code>qcut</code> method.
</p>

<p>
Note that this preprocessor function allows to estimate and fit
the quantiles of a uniform and normal distribution.
</p>

<div class="highlight"><pre><span></span>     from sklearn.preprocessing import QuantileTransformer
</pre></div>

<p>
So note that the exercise is different in comparison to the
quantile cut.
</p>

<p>
The idea is to compute <code>n_quantiles</code> quantiles from the actual
data. And then interpolate across them according to a
distribution of choice - be uniform or normal - in order to
compute the rest of the quantiles.
</p>
</div>


<ul class="org-ul">
<li><a id="org13522ff"></a>Plot histogram of the data<br />
<div class="outline-text-6" id="text-org13522ff">
<p>
In order to properly understand the operation we first plot the
histogram.
</p>

<p>
From it you can immediately infer that the data is log-normally
distributed.
</p>

<div class="highlight"><pre><span></span>      import matplotlib.pyplot as plt
      %matplotlib inline 
      %config InlineBackend.figure_format = &#39;png&#39;
</pre></div>

<div class="highlight"><pre><span></span>      plt.hist(df_housingValue, bins = 20)
</pre></div>

<img src="../../images/HistogramHousing.png" class="center">

<p>
We check next to the quantiles and the QuantileTransformer in
order to properly understand what operations the two perform on
the data.
</p>
</div>
</li>

<li><a id="org6603166"></a>Quantile Transformation<br />
<div class="outline-text-6" id="text-org6603166">
<p>
Note that weird distribution with heavy outliers or skewed
distribution might be sub-optimal in order to fit statistical
models to your data.
</p>

<p>
This because they the parameter estimation might be biased by
such properties of the data.
</p>

<p>
In this sense it makes often sense to inspect the data and
create new features that are less likely to bias your results.
</p>
</div>

<ul class="org-ul">
<li><a id="org0dbdf43"></a>Plot the relevant transformer<br />
<ul class="org-ul">
<li><a id="orgac83f2c"></a><span class="todo TODO">TODO</span> go over again in a better fashion over the data.<br />
<div class="outline-text-8" id="text-orgac83f2c">
<p>
It imposes a distribution on the data. It estimates the
parameters of the distribution, given the empirical
distribution.
</p>

<p>
It then computes the quantiles for the data values given that
assumed distribution. 
</p>

<p>
If you plug then the values in the quantile transforming
function you would get back normalized values.
</p>


<p>
It transforms the data x in quantiles such that they fit the
relevant distribution.
</p>

<p>
Note that even such transformation has limits. See the
histogram plot that you created.
</p>



<p>
It is a transformation that tries to achieve a uniform
distribution of the assigned quantiles.
</p>

<div class="highlight"><pre><span></span>	plt.hist (QuantileTransformer(n_quantiles=5, random_state=0).fit_transform (np.array (df_housingValue).reshape (-1,1)).flatten (), 20) 
</pre></div>
</div>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>


<li><a id="orgdb9ea76"></a>Standard Scaler<br /></li>



<li><a id="orgc2e1791"></a>MaxAbsScalerp<br /></li>



<li><a id="org435aea0"></a>LabelEncoder<br /></li>



<li><a id="org2140dc1"></a>OneHotEncoder<br /></li>



<li><a id="orgcdbd690"></a>KernelCenterer<br /></li>



<li><a id="org1367aae"></a>FunctionTransformer<br /></li>



<li><a id="org017c627"></a>FunctionTransformer<br /></li>
</ul>
</div>
</div>




<div id="outline-container-org163c7f8" class="outline-3">
<h3 id="org163c7f8">Predict</h3>
</div>


<div id="outline-container-org331fa14" class="outline-3">
<h3 id="org331fa14"><span class="todo TODO">TODO</span> create a different posts listing the different fitting metrics</h3>
</div>


<div id="outline-container-orga3bb64f" class="outline-3">
<h3 id="orga3bb64f"><span class="todo TODO">TODO</span> adjust the post on pipelines</h3>
<div class="outline-text-3" id="text-orga3bb64f">
<p>
Not 100% clear in its current form how that works. 
</p>
</div>
</div>
</div>

<div id="outline-container-org623b61a" class="outline-2">
<h2 id="org623b61a">Imputer</h2>
</div>




<div id="outline-container-org8a5956e" class="outline-2">
<h2 id="org8a5956e">Linear Regression</h2>
<div class="outline-text-2" id="text-org8a5956e">
</div>

<div id="outline-container-org829b09e" class="outline-3">
<h3 id="org829b09e">Data</h3>
<div class="outline-text-3" id="text-org829b09e">
<div class="highlight"><pre><span></span>   import pandas as pd
   import numpy as np
   from matplotlib import pyplot as plt

   # Generate &#39;random&#39; data
   np.random.seed(0)
   X = 2.5 * np.random.randn(100) + 1.5   # Array of 100 values with mean = 1.5, stddev = 2.5
   res = 0.5 * np.random.randn(100)       # Generate 100 residual terms
   y = 2 + 0.3 * X + res                  # Actual values of Y

   # Create pandas dataframe to store our X and y values
   df = pd.DataFrame(
       {&#39;X&#39;: X,
	&#39;y&#39;: y}
   )

   # Show the first five rows of our dataframe
   df.head()
</pre></div>
</div>
</div>


<div id="outline-container-orgf21167a" class="outline-3">
<h3 id="orgf21167a">Getting the Regression Coefficients Manually via Analytical Solution</h3>
<div class="outline-text-3" id="text-orgf21167a">
<p>
Get the relevant coefficients of the regression:
</p>

<div class="highlight"><pre><span></span>   # Calculate the mean of X and y
   xmean = np.mean(X)
   ymean = np.mean(y)

   # Calculate the terms needed for the numator and denominator of beta
   df[&#39;xycov&#39;] = (df[&#39;X&#39;] - xmean) * (df[&#39;y&#39;] - ymean)
   df[&#39;xvar&#39;] = (df[&#39;X&#39;] - xmean)**2

   # Calculate beta and alpha
   beta = df[&#39;xycov&#39;].sum() / df[&#39;xvar&#39;].sum()
   alpha = ymean - (beta * xmean)
   print(f&#39;alpha = {alpha}&#39;)
   print(f&#39;beta = {beta}&#39;)
</pre></div>


<div class="highlight"><pre><span></span>   ypred = alpha + beta * X
</pre></div>
</div>

<div id="outline-container-org852c776" class="outline-4">
<h4 id="org852c776">Matplotlib of regression</h4>
<div class="outline-text-4" id="text-org852c776">
<p>
Plot the relevant results:
</p>

<div class="highlight"><pre><span></span>    import matplotlib.pyplot as plt
    %matplotlib inline 
    %config InlineBackend.figure_format = &#39;png&#39;
</pre></div>


<div class="highlight"><pre><span></span>    # Plot regression against actual data
    plt.figure(figsize=(12, 6))
    plt.plot(X, ypred)     # regression line
    plt.plot(X, y, &#39;ro&#39;)   # scatter plot showing actual data
    plt.title(&#39;Actual vs Predicted&#39;)
    plt.xlabel(&#39;X&#39;)
    plt.ylabel(&#39;y&#39;)

    plt.show()
</pre></div>

<img src="../../images/Regression.png" class="center">
</div>
</div>
</div>


<div id="outline-container-org62ffdca" class="outline-3">
<h3 id="org62ffdca">Model Fit and Prediction</h3>
<div class="outline-text-3" id="text-org62ffdca">
</div>
<div id="outline-container-orgf0a837a" class="outline-4">
<h4 id="orgf0a837a">The standard way to fit and predict in skit-learn</h4>
<div class="outline-text-4" id="text-orgf0a837a">
<div class="highlight"><pre><span></span>    from sklearn.linear_model import LinearRegression

    # Initialise and fit model
    lm = LinearRegression()
    model = lm.fit(X.reshape(-1, 1), y)
</pre></div>


<div class="highlight"><pre><span></span>    print(f&#39;alpha = {model.intercept_}&#39;)
    print(f&#39;betas = {model.coef_}&#39;)
</pre></div>


<div class="highlight"><pre><span></span>    model.predict(X.reshape(-1, 1))
</pre></div>
</div>
</div>
</div>
</div>
