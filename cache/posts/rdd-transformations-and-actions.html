<br>
<br>


<p>
This second post present the basic set up of a Spark session and goes
over the basic of creating Spark RDDs, the first-class citizens of
Spark. An RDD is a <code>Resilient Distributed Dataset</code>. It is Resilient as
it can be recomputed if cluster failures happens. It is distributed as
an RDD can be distributed among cores and machines. And it is finally
a dataset.
</p>

<p>
RDDs are saved in-memory by default. Nonetheless when the memory of
the machine is not sufficient to handle the data, RDDs are splitted to
disk.
</p>

<p>
Finally this post outlines a bit the difference among RDDs and
DataFrames and tries to clarify that.
</p>

<!-- TEASER_END -->

<br>


<div id="outline-container-orgb4aedbb" class="outline-2">
<h2 id="orgb4aedbb">Session Set Up</h2>
<div class="outline-text-2" id="text-orgb4aedbb">
<p>
First of all it is necessary to create entry point to programming
Spark with the Dataset and DataFrame API. This is done through the
SparkSession function of the pyspark module. This will allow you to
create DataFrame, register DataFrame as tables, execute SQL over
tables, cache tables, and read parquet files (a special kind of files
particularly suited for big data storage and processing).
</p>

<p>
Unless you will work as a Data Engineer your job will not be the one
of initiating a spark session however, should that be the case or if
you will want to run Spark locally for training yourself as I am doing
you can follow the instructions below.
</p>

<p>
At a high level, every Spark application consists of a <b>driver
program</b> that launches various parallel operations on a cluster. The
driver program contains your application's main function and defines
distributed datasets on the cluster, then applies operations to them.
</p>

<p>
Driver programs access Spark through a <b>SparkContext</b> object, which
represents a connection to a computing cluster.
</p>

<p>
When you initiate such <b>SparkContext</b> you can specify a multitude of
arguments that will affect the general scope of your session. You can
find a reference for session arguments under <a href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SparkSession">this official Spark link</a>.
Such configuration parameters specified will be passed from your Spark
driver application to the SparkContext. Some of these parameters
define properties of your Spark driver application and some are used
by Spark to allocate resources on the cluster such as, the number,
memory size and cores uses by the executors running on the
workernodes.
</p>

<p>
Among the other I underline:
</p>

<ul class="org-ul">
<li>master = ["local", "local[4]", “spark://master:7077”], where the
first element is setting the Spark master locally and the second
sets the Spark master locally with 4 cores, while the third option
is an example of selecting a Spark standalone cluster.</li>

<li>config = either a key value pair of an existing <i>SparkConf</i> containing the
configuration arguments or the <i>SparkConf</i> object itself.</li>

<li>getOrCreate() = Gets an existing SparkSession or, if there is no
existing one, creates a new one based on the options set in this
builder.</li>
</ul>

<div class="highlight"><pre><span></span>import findspark
findspark.init()

from pyspark.sql import SparkSession

spark = SparkSession \
    .builder \
    .master (&quot;local&quot;) \
    .appName(&quot;My first Spark Session&quot;) \
    .getOrCreate()
</pre></div>

<p>
Good you just created your first Spark Session. In the next section I
will start to check at the different data import options and data tweaks.
</p>

<p>
Notice that to set the correct amount of cores to use on your local
machine you can use the following command
</p>

<div class="highlight"><pre><span></span>sysctl hw.physicalcpu hw.logicalcpu
</pre></div>

<p>
You can go as high as the number of your logical cpus.
</p>

<br>
</div>
</div>

<div id="outline-container-org9ec033c" class="outline-2">
<h2 id="org9ec033c">Data Creation</h2>
<div class="outline-text-2" id="text-org9ec033c">
</div>

<div id="outline-container-orgdca61b2" class="outline-3">
<h3 id="orgdca61b2">From Python objects</h3>
<div class="outline-text-3" id="text-orgdca61b2">
<p>
Here you use the parallelize method to a local Python
collection to form an RDD.
</p>

<div class="highlight"><pre><span></span>spark.sparkContext.parallelize([(1, 2, 3, &#39;a b c&#39;),
				(4, 5, 6, &#39;d e f&#39;),
				(7, 8, 9, &#39;g h i&#39;)]).collect()
</pre></div>

<pre class="example">
[(1, 2, 3, 'a b c'), (4, 5, 6, 'd e f'), (7, 8, 9, 'g h i')]
</pre>


<p>
You might even create different partitions of the RDDs when creating
one. This is done by entering the different partitions in a list and
selecting the number of partitions.
</p>

<div class="highlight"><pre><span></span>rdd = spark.sparkContext.parallelize([(1, 2, 3, &#39;a b c&#39;),
				   (4, 5, 6, &#39;d e f&#39;),
				   (7, 8, 9, &#39;g h i&#39;)], 3)

print(rdd.getNumPartitions())

print (rdd.glom ().collect())
</pre></div>

<pre class="example">
3
[[(1, 2, 3, 'a b c')], [(4, 5, 6, 'd e f')], [(7, 8, 9, 'g h i')]]
</pre>


<p>
The <code>glom()</code> method above returns an RDD created by coalescing all
elements within each partition into a list.
</p>
</div>
</div>

<div id="outline-container-orgdc9fd3f" class="outline-3">
<h3 id="orgdc9fd3f">From a stable Storage</h3>
<div class="outline-text-3" id="text-orgdc9fd3f">
<p>
It is possible to create an RDD from multiple stable storage options.
</p>

<p>
<i>Local File system:</i>
</p>
<ul class="org-ul">
<li><code>file</code>: Read from the local file system.</li>
</ul>

<p>
<i>HDFS:</i>
</p>
<ul class="org-ul">
<li><code>hdfs</code>: Read from a Hadoop Distributed File System.</li>
</ul>

<p>
<i>Block Storage:</i>
</p>
<ul class="org-ul">
<li><code>s3</code> : Read from AWS S3 Storage.</li>
<li><code>wasb</code>: Read from Azure Blob Storage.</li>
</ul>

<p>
The blow illustrates such an example.
</p>

<div class="highlight"><pre><span></span><span class="c1"># sc is the Spark Context object automatically created for you</span>
<span class="n">fruits</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="p">(</span><span class="s1">&#39;wasb:///example/data/fruits.txt&#39;</span><span class="p">)</span>
<span class="n">yellowThings</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="p">(</span><span class="s1">&#39;wasb:///example/data/yellowthings.txt&#39;</span><span class="p">)</span>
</pre></div>


<br>
</div>
</div>
</div>

<div id="outline-container-org150b45c" class="outline-2">
<h2 id="org150b45c">RDD vs DataFrame</h2>
<div class="outline-text-2" id="text-org150b45c">
</div>

<div id="outline-container-org8138169" class="outline-3">
<h3 id="org8138169">The idea of DataFrames</h3>
<div class="outline-text-3" id="text-org8138169">
<p>
The conversion from Spark RDDs to Dataframe came essentially with
Spark 2.0. The idea was to bring Spark RDDs under the hood of SQL
and leverage the many benefits from the very long lasting research
literature on SQL. See <a href="https://marcohassan.github.io/bits-of-experience/posts/Apache%20Spark%20SQL/">Catalyst and Project Tungsten</a>.
</p>

<p>
A data frame is in fact "tabular" data: a data structure
representing cases (rows), each of which consists of a number of
observations or measurements (columns).
</p>

<p>
The idea for making that possible was then the one of having a
schema where each value of an RDD would contain a row of the table
you aim to represent.
</p>

<p>
A futher benefit of creating a dataframe representation from RDD is
the one of <code>columnar storage</code>. Here the key concept is the one of
standardization. Once you have a tabular representation of your
data through RDDs it is possible to save columns which are likely
highly standardized together reaching the highest possible
efficiency. This means concretely that you therefore save memory
and reduce the computation time.
</p>
</div>
</div>

<div id="outline-container-org6470fb9" class="outline-3">
<h3 id="org6470fb9">Creation of DataFrames</h3>
<div class="outline-text-3" id="text-org6470fb9">
<p>
A spark RDD can be created in multiple ways among the others by:
</p>


<ul class="org-ul">
<li><p>
Transforming an existing RDD into a dataframe
</p>

<div class="highlight"><pre><span></span>    df = spark.sparkContext.parallelize([(1, 2, 3, &#39;a b c&#39;),
					 (4, 5, 6, &#39;d e f&#39;),
					 (7, 8, 9, &#39;g h i&#39;)]).toDF([&#39;col1&#39;, &#39;col2&#39;, &#39;col3&#39;,&#39;col4&#39;])

    df.show ()
</pre></div>

<pre class="example">
+----+----+----+-----+
|col1|col2|col3| col4|
+----+----+----+-----+
|   1|   2|   3|a b c|
|   4|   5|   6|d e f|
|   7|   8|   9|g h i|
+----+----+----+-----+

</pre></li>
</ul>


<ul class="org-ul">
<li><p>
Importing your data as a RDD dataframe
</p>

<p>
This is achieved through the <code>read ()</code> method of the
<code>pyspark.sql</code> API. It returns a DataFrameReader that can be used
to read data in as a DataFrame.
</p>

<div class="highlight"><pre><span></span>    df = spark.read.csv(&quot;file:///Users/marcohassan/Desktop/my_test.csv&quot;)

    df.show ()
</pre></div>

<pre class="example">
+-----+------+-------+-------+
|  _c0|   _c1|    _c2|    _c3|
+-----+------+-------+-------+
|hello| world| hello2| world2|
+-----+------+-------+-------+

</pre></li>
</ul>

<p>
Notice that you can read among the many different file formats and
not just the classical tabular <code>csv</code> format.
</p>

<p>
<code>read.json ()</code>
<code>read.parquet ()</code>
<code>read.text ()</code>
<code>read.jdbc ()</code>
<code>read.format("avro")</code>
</p>

<p>
<b>Important:</b>  Note, that your json does have to have one object per
row in the text file. I was keeping my json in pretty-print format
and I lost like a good hour understanding what went wrong when
importing the data.
</p>

<p>
Are also viable options. Moreover, you can as always select
Blockstorage, the Local File System or HDFS as the source of your
data.
</p>
</div>
</div>

<div id="outline-container-org5faa099" class="outline-3">
<h3 id="org5faa099">Schema Inference</h3>
<div class="outline-text-3" id="text-org5faa099">
<p>
An important question that arises is how to specify the schema of
the tabular representation of the data you are
importing. Interesting is that in contrast to SQL there is no need
to create a table and specifying the schema of it when importing
the data.
</p>

<p>
Such a schema is internally inferred by Spark at the time of
the data import. Should you be interested in the available data
types of spark you can consul the <a href="https://spark.apache.org/docs/latest/sql-reference.html">following link</a>. There is even
the option to manually set the data schema for the imported data,
so should you even need that you can go into it.
</p>
</div>
</div>

<div id="outline-container-org7ccf81f" class="outline-3">
<h3 id="org7ccf81f">About the break of first-normal form</h3>
<div class="outline-text-3" id="text-org7ccf81f">
<p>
It is clear that when importing tree-shaped data, your data might
easily break the first-normal form.
</p>

<p>
How are such data saved in the tabular form? 
</p>

<p>
The solution was here to extend the relational base logic by
allowing a futher <code>array datatype</code> as you can see from the link in
the previous section. You can think it as follows:
</p>

<p>
<img src="/images/Bildschirmfoto_2020-05-03_um_11.58.39.png" alt="nil"/>
</p>
</div>
</div>

<div id="outline-container-orgad93b8d" class="outline-3">
<h3 id="orgad93b8d">SQL commands</h3>
<div class="outline-text-3" id="text-orgad93b8d">
<p>
As your data are in tabular form you may now leverage the SQL
syntax for querying your data.
</p>
</div>

<div id="outline-container-org667ad1f" class="outline-4">
<h4 id="org667ad1f">Array Objects</h4>
<div class="outline-text-4" id="text-org667ad1f">
<p>
Notice that it is now clear though that we have to extend the
functions of SQL for dealing with such a cases.
</p>

<p>
This was solved by introducing some new functions such as
<code>EXPLODE</code>.
</p>

<p>
To see that consider the following <code>json</code>.
</p>

<div class="highlight"><pre><span></span>    {
      &quot;First&quot;: &quot;Albert&quot;,
      &quot;Last&quot;: &quot;Einstein&quot;,
      &quot;Countries&quot;: [
	&quot;D&quot;,
	&quot;I&quot;,
	&quot;CH&quot;,
	&quot;A&quot;,
	&quot;BE&quot;,
	&quot;US&quot;
      ]
    }
</pre></div>

<p>
Notice again that the above is in pretty-print format just for
facilitating you to read it but it should be saved as a one liner
for the spark API to correctly work.
</p>

<div class="highlight"><pre><span></span>    df = spark.read.json(&quot;file:///Users/marcohassan/Desktop/my_test.json&quot;)

    df.createOrReplaceTempView(&quot;dataset&quot;)

    ## note that the space at the end of each line is important for the
    ## SQL API
    df2 = spark.sql(&quot;SELECT First,EXPlODE(Countries) &quot;
		    &quot;FROM dataset &quot;)

    df2.show ()
</pre></div>

<pre class="example" id="orge0b43f9">
+------+---+
| First|col|
+------+---+
|Albert|  D|
|Albert|  I|
|Albert| CH|
|Albert|  A|
|Albert| BE|
|Albert| US|
+------+---+

</pre>
</div>
</div>


<div id="outline-container-orgb3b3ebf" class="outline-4">
<h4 id="orgb3b3ebf">Nested Objects</h4>
<div class="outline-text-4" id="text-orgb3b3ebf">
<p>
The same holds for nested entries.
</p>

<div class="highlight"><pre><span></span>   {
   &quot;Name&quot;: {
   &quot;First&quot;: &quot;Albert&quot;,
   &quot;Last&quot;: &quot;Einstein&quot;
   },
   &quot;Countries&quot;: 6
   }
   {
   &quot;Name&quot;: {
   &quot;First&quot;: &quot;Srinivasa&quot;,
   &quot;Last&quot;: &quot;Ramanujan&quot;
   },
   &quot;Countries&quot;: 2
   }
   {
   &quot;Name&quot;: {
   &quot;First&quot;: &quot;Kurt&quot;,
   &quot;Last&quot;: &quot;GÃ¶del&quot;
   },
   &quot;Countries&quot;: 1
   }
</pre></div>

<div class="highlight"><pre><span></span>   df = spark.read.json(&quot;file:///Users/marcohassan/Desktop/my_test.json&quot;)

   df.createOrReplaceTempView(&quot;dataset&quot;)

   ## note that the space at the end of each line is important for the
   ## SQL API
   print (spark.sql(&quot;SELECT * &quot;
		    &quot;FROM dataset &quot;).show ())


   df2 = spark.sql(&quot;SELECT Name.First &quot;
		   &quot;FROM dataset &quot;)

   df2.show ()
</pre></div>

<pre class="example" id="org5b27177">
+---------+--------------------+
|Countries|                Name|
+---------+--------------------+
|        6|  [Albert, Einstein]|
|        2|[Srinivasa, Raman...|
|        1|       [Kurt, Gödel]|
+---------+--------------------+

None
+---------+
|    First|
+---------+
|   Albert|
|Srinivasa|
|     Kurt|
+---------+
</pre>
</div>
</div>
</div>

<div id="outline-container-orgc57934a" class="outline-3">
<h3 id="orgc57934a">The heterogeneity flaw.</h3>
<div class="outline-text-3" id="text-orgc57934a">
<p>
Notice that there is one last fundamental flaw when working with
DataFrames API of spark.
</p>

<p>
Consider the case when your data are heterogeneous as follows:
</p>


<div class="highlight"><pre><span></span>   { &quot;foo&quot; : 1, &quot;bar&quot; : true}
   { &quot;foo&quot; : 2, &quot;bar&quot; : true}
   { &quot;foo&quot; : [3, 4], &quot;bar&quot; : false}
   { &quot;foo&quot; : 4, &quot;bar&quot; : true}
   { &quot;foo&quot; : 5, &quot;bar&quot; : true}
   { &quot;foo&quot; : 6, &quot;bar&quot; : false}
   { &quot;foo&quot; : 7, &quot;bar&quot; : true}
</pre></div>

<div class="highlight"><pre><span></span>   df = spark.read.json(&quot;file:///Users/marcohassan/Desktop/my_test.json&quot;)

   df.createOrReplaceTempView(&quot;dataset&quot;)

   ## note that the space at the end of each line is important for the
   ## SQL API
   df2 = spark.sql(&quot;SELECT *&quot;
		   &quot;FROM dataset &quot;)

   df2.show ()
</pre></div>

<pre class="example" id="org2e5dadb">
+-----+-----+
|  bar|  foo|
+-----+-----+
| true|    1|
| true|    2|
|false|[3,4]|
| true|    4|
| true|    5|
|false|    6|
| true|    7|
+-----+-----+
</pre>

<p>
It follows now that the inferred Schema of foo will be a string as
this is the only way of dealing with the outlier array entry and
not an integer.
</p>
</div>
</div>
</div>

<div id="outline-container-org02f7076" class="outline-2">
<h2 id="org02f7076">Literature</h2>
<div class="outline-text-2" id="text-org02f7076">
<p>
<a href="https://runawayhorse001.github.io/LearningApacheSpark/rdd.html">https://runawayhorse001.github.io/LearningApacheSpark/rdd.html</a>
</p>

<p>
<a href="https://blogs.msdn.microsoft.com/bigdatasupport/2015/09/14/understanding-sparks-sparkconf-sparkcontext-sqlcontext-and-hivecontext/">https://blogs.msdn.microsoft.com/bigdatasupport/2015/09/14/understanding-sparks-sparkconf-sparkcontext-sqlcontext-and-hivecontext/</a>
</p>

<p>
<a href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SparkSession">http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SparkSession</a>
</p>

<p>
<a href="https://www.systems.ethz.ch/courses/spring2020/bigdataforeng/material">https://www.systems.ethz.ch/courses/spring2020/bigdataforeng/material</a>
</p>
</div>
</div>
