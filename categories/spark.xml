<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Bits of Experience (Posts about Spark)</title><link>https://marcohassan.github.io/bits-of-experience/</link><description></description><atom:link href="https://marcohassan.github.io/bits-of-experience/categories/spark.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2022 &lt;a href="mailto:marco.hassan30@gmail.com"&gt;Marco Hassan&lt;/a&gt; </copyright><lastBuildDate>Sat, 16 Apr 2022 12:50:51 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Defining Functions in Spark</title><link>https://marcohassan.github.io/bits-of-experience/posts/defining-functions-in-spark/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;p&gt;
One especially interesting feature that I already used before; but
unfortunately I did not make notes for, is the possibility of
specifying particular functions via the PySpark raw API and make them
available throughout your spark session when working via data-frames -
be it via SparkSQL or via pandas.
&lt;/p&gt;

&lt;p&gt;
Note that this notes are based on &lt;a href="https://jaceklaskowski.gitbooks.io/mastering-spark-sql/content/spark-sql-udfs.html"&gt;this source&lt;/a&gt;. Looks like a very
exhaustive source. So check at it again should this project really
start. There you might make sense of all of that. 
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/defining-functions-in-spark/"&gt;Read more…&lt;/a&gt; (3 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Spark</category><guid>https://marcohassan.github.io/bits-of-experience/posts/defining-functions-in-spark/</guid><pubDate>Thu, 03 Jun 2021 12:34:46 GMT</pubDate></item><item><title>Spark UI</title><link>https://marcohassan.github.io/bits-of-experience/posts/spark-ui/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;style&gt;
img {
display: block;
margin-left: auto;
margin-right: auto;
margin-top: 60px;
margin-bottom: 60px;
}
&lt;/style&gt;




&lt;p&gt;
This post is a reminder for me to use Spark UI when working as it
greatly improves debugging and allows you to better understand of
Spark.
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/spark-ui/"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Spark</category><guid>https://marcohassan.github.io/bits-of-experience/posts/spark-ui/</guid><pubDate>Thu, 16 Jul 2020 15:31:07 GMT</pubDate></item><item><title>Spark Architecture</title><link>https://marcohassan.github.io/bits-of-experience/posts/spark-architecture/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;br&gt;
&lt;br&gt;

&lt;p&gt;
After the different posts written on setting up a spark session and
working with RDDs &lt;a href="https://marcohassan.github.io/bits-of-experience/categories/spark/"&gt;available here&lt;/a&gt;, this post briefly goes a more into
the detail of the spark physical layer.
&lt;/p&gt;

&lt;p&gt;
It will introduce the concept of DAGs and will make a comparison of
spark and the more restrictive &lt;a href="https://marcohassan.github.io/bits-of-experience/posts/mapreduce/"&gt;MapReduce&lt;/a&gt;. 
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/spark-architecture/"&gt;Read more…&lt;/a&gt; (4 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Big Data</category><category>Spark</category><guid>https://marcohassan.github.io/bits-of-experience/posts/spark-architecture/</guid><pubDate>Sun, 24 May 2020 09:44:04 GMT</pubDate></item><item><title>RDDs Transformations and Actions</title><link>https://marcohassan.github.io/bits-of-experience/posts/rdds-transformations-and-actions/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;br&gt;
&lt;br&gt;


&lt;p&gt;
This post continues the discussion started a few times ago on &lt;a href="https://marcohassan.github.io/bits-of-experience/posts/spark-session-initalization/"&gt;RDD and
Spark&lt;/a&gt;.
&lt;/p&gt;

&lt;p&gt;
I will try to go here in the lifecycle of an RDD and will present the
major Transformation and Actions functions. I will moreover touch on
the physical implementation of Spark as this will give you the mental
tools to properly understand how to properly structure your Spark
workflow in order to optimize the performance.   
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/rdds-transformations-and-actions/"&gt;Read more…&lt;/a&gt; (11 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Big Data</category><category>Spark</category><guid>https://marcohassan.github.io/bits-of-experience/posts/rdds-transformations-and-actions/</guid><pubDate>Sun, 03 May 2020 13:51:24 GMT</pubDate></item><item><title>Apache Spark SQL</title><link>https://marcohassan.github.io/bits-of-experience/posts/Apache%20Spark%20SQL/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;br&gt;
&lt;br&gt;

&lt;p&gt;
This posts makes the point for Apache Spark SQL. 
&lt;/p&gt;

&lt;p&gt;
Using RDDs API might be quite annoying, especially if you are used to
the industry standard of RDMS and their SQL sytax.
&lt;/p&gt;

&lt;p&gt;
Here ApacheSparkSQL kicks in, providing a SQL interface to your data.
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/Apache%20Spark%20SQL/"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Big Data</category><category>Spark</category><guid>https://marcohassan.github.io/bits-of-experience/posts/Apache%20Spark%20SQL/</guid><pubDate>Fri, 06 Sep 2019 16:21:43 GMT</pubDate></item><item><title>Spark Session Initialization, RDD and DataFrames</title><link>https://marcohassan.github.io/bits-of-experience/posts/spark-session-initalization/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;br&gt;
&lt;br&gt;


&lt;p&gt;
This second post present the basic set up of a Spark session and goes
over the basic of creating Spark RDDs, the first-class citizens of
Spark. An RDD is a &lt;code&gt;Resilient Distributed Dataset&lt;/code&gt;. It is Resilient as
it can be recomputed if cluster failures happens. It is distributed as
an RDD can be distributed among cores and machines. And it is finally
a dataset.
&lt;/p&gt;

&lt;p&gt;
RDDs are saved in-memory by default. Nonetheless when the memory of
the machine is not sufficient to handle the data, RDDs are splitted to
disk.
&lt;/p&gt;

&lt;p&gt;
Finally this post outlines a bit the difference among RDDs and
DataFrames and tries to clarify that.
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/spark-session-initalization/"&gt;Read more…&lt;/a&gt; (8 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Big Data</category><category>Spark</category><guid>https://marcohassan.github.io/bits-of-experience/posts/spark-session-initalization/</guid><pubDate>Wed, 21 Aug 2019 21:31:02 GMT</pubDate></item><item><title>PySpark Set-Up and Integration with Emacs</title><link>https://marcohassan.github.io/bits-of-experience/posts/pyspark-set-up/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;div class="HTML" id="org26e89f5"&gt;
&lt;p&gt;
&amp;lt;br&amp;gt;
&amp;lt;br&amp;gt;
&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;
This post aims at providing an overview of the necessary steps
required in order to leverage Apache Spark through the Python API and
the PySpark module.
&lt;/p&gt;

&lt;p&gt;
Despite the information is vastly reported over the internet my usual
&lt;i&gt;procedere&lt;/i&gt; when I deal with new tools and software is to write a
short piece of note when learning new software that is intended as an
overview over the tool and serve as a beginner cheat sheet.
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/pyspark-set-up/"&gt;Read more…&lt;/a&gt; (4 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Big Data</category><category>emacs</category><category>Spark</category><guid>https://marcohassan.github.io/bits-of-experience/posts/pyspark-set-up/</guid><pubDate>Mon, 05 Aug 2019 21:51:11 GMT</pubDate></item></channel></rss>