<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Bits of Experience</title><link>https://marcohassan.github.io/bits-of-experience/</link><description>A readable view on my studying adventures.</description><atom:link href="https://marcohassan.github.io/bits-of-experience/rss.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents Â© 2020 &lt;a href="mailto:marco.hassan30@gmail.com"&gt;Marco Hassan&lt;/a&gt; </copyright><lastBuildDate>Sun, 24 May 2020 17:14:25 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>HDFS</title><link>https://marcohassan.github.io/bits-of-experience/posts/hdfs/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;br&gt;
&lt;br&gt;

&lt;p&gt;
After having discussed Object Storage, this posts continues to dig
into the storage layer by briefly introducing HDFS. It will briefly
make the point for the difference between Object Storage and HDFS as a
distributed storage option.
&lt;/p&gt;

&lt;p&gt;
Moreover, it tries to draw a line between Block Storage via Storage
Area Network (SAN), HDFS block storage and the local file system (LFS)
block storage, three topics that highly confused me at first when
writing this posts series.
&lt;/p&gt;

&lt;br&gt;

&lt;div id="outline-container-org60f8929" class="outline-2"&gt;
&lt;h2 id="org60f8929"&gt;HDFS vs. Object Storage&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org60f8929"&gt;
&lt;p&gt;
Important to keep in mind is that in the space of Big Data there are
different kinds of big data all belonging to the same &lt;code&gt;Big Data&lt;/code&gt;
world.
&lt;/p&gt;

&lt;p&gt;
While on the one hand you might have &lt;b&gt;billions of up to TB files&lt;/b&gt; on
the other hand we might have &lt;b&gt;millions of PB files&lt;/b&gt;. Object Storage
fits well for the first case but it has no chance when dealing with
the second case without a horrible manual operation from the side of
one engineer. On the other hand HDFS works by leveraging a file system
structure, keeping therefore a hierarchy of the files and has no
chance to deal with billions of different files. 
&lt;/p&gt;

&lt;p&gt;
Moreover HDFS works with a block storage component and is therefore
more suited for very write intensive application as it is to store
edits in memory and periodically write them down into bigger
Hfiles. This is not possible with big unique objects also because of
th Object Storage Architecture that rather runs via HTTP servers and
there is no big virtualized container structure where intermediary
results / write intensive operations can be stored.
&lt;/p&gt;

&lt;br&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgcbd9f73" class="outline-2"&gt;
&lt;h2 id="orgcbd9f73"&gt;HDFS the Physical layer&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgcbd9f73"&gt;
&lt;p&gt;
At the physical layer HDFS is nothing more than a file system where
each file is splitted in several different blocks that are distributed
among machines.
&lt;/p&gt;

&lt;p&gt;
The important aspect is to understand that the HDFS is not a physical
file system but rather a virtual, &lt;b&gt;logical&lt;/b&gt; file system acting on top
of the distributed machines local file system.
&lt;/p&gt;

&lt;p&gt;
As mentioned above, in the typical HDFS application, files are
huge. They are therefore splitted into different blocks and saved
across different machines.
&lt;/p&gt;

&lt;p&gt;
Such blocks live then on commodity hardware machines and are saved on
a typical local file system that also uses block storage. Each block
that is assigned to a physical machine is therefore further split into
blocks. Important is however to understand that we still talk of HDFS
blocks in the sense that given a HDFS block ID for a local machine to
compose the HDFS through the local file system is very immediate.
&lt;/p&gt;

&lt;p&gt;
The difference between the HDFS and the local file system blocks lies
then simply in the block size difference, where each block size is
chose in order to get the better time performance as the sum of
&lt;b&gt;latency&lt;/b&gt; and &lt;b&gt;throughput&lt;/b&gt;.
&lt;/p&gt;

&lt;p&gt;
In Hadoop the usual size is around 64MB and 128MB. The point is
that this is a good compromise among &lt;i&gt;latency&lt;/i&gt; and
&lt;i&gt;throughput&lt;/i&gt;. If the block was too small then the cost of
accessing a file on a remote machine and prepare it would be too
big as the latency is higher compared to local systems. This means
that the file must be large enough to leverage the I/O
capabilities of the machines and the high throughput.
&lt;/p&gt;

&lt;p&gt;
It should however not be too big as the key of Hadoop is the
&lt;i&gt;distributed&lt;/i&gt; nature of the file system. The size should render
the &lt;i&gt;distribution&lt;/i&gt; manageable.
&lt;/p&gt;

&lt;p&gt;
Further important advantages on relying on larger blocks are: 
&lt;/p&gt;

&lt;ol class="org-ol"&gt;
&lt;li&gt;it &lt;b&gt;minimizes the cost of seeks&lt;/b&gt;. If the block is large enough,
the time it takes to transfer the data from the disk can be
significantly longer than the time to seek to the start of the
block. Thus, transferring a large file made of multiple blocks
operates at the disk transfer rate.&lt;/li&gt;

&lt;li&gt;it &lt;b&gt;reduces clients' need to interact with the master&lt;/b&gt; because
reads and writes on the same chunk require only one initial
request to the master for chunk location information. The
reduction is especially significant for our workloads because
applications mostly read and write large files sequentially.&lt;/li&gt;

&lt;li&gt;since on a large chunk, a client is more likely to perform many
operations on a given chunk, it can &lt;b&gt;reduce network overhead by
keeping a persistent TCP connection&lt;/b&gt; to the chunkserver over an
extended period of time.&lt;/li&gt;

&lt;li&gt;it &lt;b&gt;reduces the size of the metadata stored on the master&lt;/b&gt;. This
allows us to keep the metadata in memory.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;
In the same spirit the local file system uses blocks of 4kB - 32kB.
&lt;/p&gt;

&lt;br&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgc89ff13" class="outline-2"&gt;
&lt;h2 id="orgc89ff13"&gt;HDFS Architecture&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgc89ff13"&gt;
&lt;p&gt;
HDFS is a master-slave architecture. The master, the &lt;code&gt;Namenode&lt;/code&gt; is
responsible for connecting with the client users. It is moreover
responsible for:
&lt;/p&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;keeping an up to date version of the HDFS file system and access
control overview across the cluster.&lt;/li&gt;

&lt;li&gt;keeping a map from the Hfiles to the file blocks&lt;/li&gt;

&lt;li&gt;keeping the record of each Hfile block location on the different
slave nodes&lt;/li&gt;

&lt;li&gt;informing the client of the slave node location and block id such
that the client can directly connect the slave node to get the data
offloading the tasks the Namenode has to perform&lt;/li&gt;

&lt;li&gt;responsible for asynchronous replication&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
On the other hand the slave node, the &lt;code&gt;DataNodes&lt;/code&gt; are responsible for
the following:
&lt;/p&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;keeping a state-ful connection with the &lt;code&gt;Namenode&lt;/code&gt; and sending a
heartbit message to the it so that the &lt;code&gt;Namenode&lt;/code&gt; is informed that
the node is still alive.&lt;/li&gt;

&lt;li&gt;transferring the desired blocks to clients connecting&lt;/li&gt;

&lt;li&gt;sending a BlockReport with the blocks stored every 6h (default;
option configurable)&lt;/li&gt;

&lt;li&gt;replication by pipelining the data to other &lt;code&gt;DataNodes&lt;/code&gt;. Important
is here that the pipeline is sent by the client.&lt;/li&gt;
&lt;/ul&gt;

&lt;br&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgbdfc43e" class="outline-2"&gt;
&lt;h2 id="orgbdfc43e"&gt;Replica default Settings&lt;/h2&gt;
&lt;/div&gt;



&lt;div id="outline-container-org809146b" class="outline-2"&gt;
&lt;h2 id="org809146b"&gt;HDFS and SAN&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org809146b"&gt;
&lt;p&gt;
Interesting is the case of cloud block storage via Storage
Area Network. 
&lt;/p&gt;

&lt;p&gt;
There are essentially two different possible architectures for
HDFS. The first and traditional one is to work with &lt;b&gt;direct attached
storage (DAS)&lt;/b&gt; as presented above. This is an architecture where you
essentially have each &lt;code&gt;DataNode&lt;/code&gt; server attached to a local storage
solution so that each server responsible for the data directly
connected to it.
&lt;/p&gt;

&lt;p&gt;
The second is indeed to work with &lt;b&gt;SAN&lt;/b&gt; and make a clear separation as
regarding the storage and the all of the other operations of slave
nodes servers. You have a SAN between the storage layer and the slave
nodes servers and it is therefore possible to leverage different
benefits in terms of caching etc. However, important is to understand
that much of the content introduced before is gone. Replication is not
task of the slave nodes anymore. This rather focus on the computation
layer which is introduced then with Spark and MapReduce and the YARN.
&lt;/p&gt;

&lt;p&gt;
A good overview for the interested reader is at &lt;a href="https://www.snia.org/sites/default/education/tutorials/2013/spring/big/SamFineberg_Big_Data_Hadoop_Storage_Options_3v9.pdf"&gt;HDFS storage Options&lt;/a&gt;.
&lt;/p&gt;

&lt;br&gt;
&lt;/div&gt;
&lt;/div&gt;


&lt;div id="outline-container-org8179aeb" class="outline-2"&gt;
&lt;h2 id="org8179aeb"&gt;Literature&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org8179aeb"&gt;
&lt;p&gt;
&lt;a href="https://www.systems.ethz.ch/courses/spring2020/bigdataforeng/material"&gt;Big Data for Engineers - ETH course&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;
&lt;a href="https://stackoverflow.com/questions/16811959/hdfs-vs-lfs-how-hadoop-dist-file-system-is-built-over-local-file-system"&gt;StackOverflow - HDFS vs LFS&lt;/a&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>Big Data</category><guid>https://marcohassan.github.io/bits-of-experience/posts/hdfs/</guid><pubDate>Sun, 24 May 2020 15:01:55 GMT</pubDate></item><item><title>Storage Layer - Object Storage</title><link>https://marcohassan.github.io/bits-of-experience/posts/storage-layer-object-storage/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;br&gt;
&lt;br&gt;

&lt;p&gt;
There are essentially three major storage options. The first being
Block Storage, the second being File Storage and the last being Object
Storage. You can find a good introduction to the three different
options at the &lt;a href="https://www.ibm.com/cloud/learn/block-storage"&gt;following link&lt;/a&gt;.
&lt;/p&gt;

&lt;p&gt;
This post briefly introduces the third storage option above
i.e. object storage.
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/storage-layer-object-storage/"&gt;Read moreâ¦&lt;/a&gt; (3 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Big Data</category><guid>https://marcohassan.github.io/bits-of-experience/posts/storage-layer-object-storage/</guid><pubDate>Sun, 24 May 2020 13:31:11 GMT</pubDate></item><item><title>Spark Architecture</title><link>https://marcohassan.github.io/bits-of-experience/posts/spark-architecture/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;br&gt;
&lt;br&gt;

&lt;p&gt;
After the different posts written on setting up a spark session and
working with RDDs &lt;a href="https://marcohassan.github.io/bits-of-experience/categories/spark/"&gt;available here&lt;/a&gt;, this post briefly goes a more into
the detail of the spark physical layer.
&lt;/p&gt;

&lt;p&gt;
It will introduce the concept of DAGs and will make a comparison of
spark and the more restrictive &lt;a href="https://marcohassan.github.io/bits-of-experience/posts/mapreduce/"&gt;MapReduce&lt;/a&gt;. 
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/spark-architecture/"&gt;Read moreâ¦&lt;/a&gt; (4 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Big Data</category><category>Spark</category><guid>https://marcohassan.github.io/bits-of-experience/posts/spark-architecture/</guid><pubDate>Sun, 24 May 2020 09:44:04 GMT</pubDate></item><item><title>MapReduce</title><link>https://marcohassan.github.io/bits-of-experience/posts/mapreduce/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;br&gt;
&lt;br&gt;

&lt;p&gt;
This post go briefly over the key idea of using MapReduce as a way to
parallelize operations over multiple distributed machines. 
&lt;/p&gt;

&lt;p&gt;
This was in fact the first tool that was developed for parallelizing
computation over multiple machines and not simply to use multiple
machines for the storage layer.
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/mapreduce/"&gt;Read moreâ¦&lt;/a&gt; (7 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Big Data</category><guid>https://marcohassan.github.io/bits-of-experience/posts/mapreduce/</guid><pubDate>Sat, 23 May 2020 12:25:10 GMT</pubDate></item><item><title>YARN</title><link>https://marcohassan.github.io/bits-of-experience/posts/yarn/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;br&gt;
&lt;br&gt;

&lt;p&gt;
This post briefly introduces YARN - &lt;b&gt;Yet Another Resource Negotiator&lt;/b&gt;.
It was introduced to overcome the limits of MapReduce v 1.0,
especially the idle resources component, &lt;a href="https://marcohassan.github.io/bits-of-experience/posts/mapreduce/"&gt;discussed in the previous post&lt;/a&gt;.
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/yarn/"&gt;Read moreâ¦&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Big Data</category><guid>https://marcohassan.github.io/bits-of-experience/posts/yarn/</guid><pubDate>Sat, 23 May 2020 12:21:55 GMT</pubDate></item><item><title>NLP text classification</title><link>https://marcohassan.github.io/bits-of-experience/posts/nlp-text-classification/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;br&gt;
&lt;br&gt;

&lt;p&gt;
This post briefly introduce two key concepts for representing NLP
text data. The first is simply based on some frequency metric given a
text corpus, the second &lt;code&gt;word2vec&lt;/code&gt; tries a more elegant and elaborate
approach by trying to extract the semantical representation of words
in a low dimensional space.
&lt;/p&gt;

&lt;p&gt;
Both will be briefly explained. 
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/nlp-text-classification/"&gt;Read moreâ¦&lt;/a&gt; (4 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><guid>https://marcohassan.github.io/bits-of-experience/posts/nlp-text-classification/</guid><pubDate>Wed, 20 May 2020 20:22:37 GMT</pubDate></item><item><title>Slovak Learning</title><link>https://marcohassan.github.io/bits-of-experience/posts/slovak-learning/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;br&gt;
&lt;br&gt;

&lt;p&gt;
Ok. This post is likely not interesting for any visitor. I am learning
the language and after a discussion with my girlfriend it turned out I
rather write down the new vocabulary for learning the language the
fastest.
&lt;/p&gt;

&lt;p&gt;
In this post I write down my progress and I can go back to it at later
stages.
&lt;/p&gt;


&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/slovak-learning/"&gt;Read moreâ¦&lt;/a&gt; (3 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Slovak</category><guid>https://marcohassan.github.io/bits-of-experience/posts/slovak-learning/</guid><pubDate>Sat, 16 May 2020 07:45:34 GMT</pubDate></item><item><title>RDDs Transformations and Actions</title><link>https://marcohassan.github.io/bits-of-experience/posts/rdds-transformations-and-actions/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;br&gt;
&lt;br&gt;


&lt;p&gt;
This post continues the discussion started a few times ago on &lt;a href="https://marcohassan.github.io/bits-of-experience/posts/spark-session-initalization/"&gt;RDD and
Spark&lt;/a&gt;.
&lt;/p&gt;

&lt;p&gt;
I will try to go here in the lifecycle of an RDD and will present the
major Transformation and Actions functions. I will moreover touch on
the physical implementation of Spark as this will give you the mental
tools to properly understand how to properly structure your Spark
workflow in order to optimize the performance.   
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/rdds-transformations-and-actions/"&gt;Read moreâ¦&lt;/a&gt; (10 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Big Data</category><category>Spark</category><guid>https://marcohassan.github.io/bits-of-experience/posts/rdds-transformations-and-actions/</guid><pubDate>Sun, 03 May 2020 13:51:24 GMT</pubDate></item><item><title>On The Brave Browser</title><link>https://marcohassan.github.io/bits-of-experience/posts/how-to-verify-your-github-page-with-brave-rewards/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;p&gt;
So I recently started to work with the Brave browser. I started to see
it on a LinkIn cryptocurrency influencer webpage and I was rather
suspicious. I noticed it but I did not further investigate. I just had
integrated &lt;code&gt;DuckDuckGo&lt;/code&gt; into my Chrome browser and I was feeling ok
with that at the time.
&lt;/p&gt;

&lt;p&gt;
I started to get quite interested in it when I saw that the Prof. of
Communication Networks of the University I am currently visiting was
using it. He, as an expert using the browser. There must be some
goodness in it.
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/how-to-verify-your-github-page-with-brave-rewards/"&gt;Read moreâ¦&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Brave Browser</category><guid>https://marcohassan.github.io/bits-of-experience/posts/how-to-verify-your-github-page-with-brave-rewards/</guid><pubDate>Sat, 02 May 2020 17:05:36 GMT</pubDate></item><item><title>Remotely Login - SSH Key</title><link>https://marcohassan.github.io/bits-of-experience/posts/remotely-login-ssh-key/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;br&gt;
&lt;br&gt;

&lt;p&gt;
This posts outlines the way to save ssh into a virtual machine using a
generated &lt;code&gt;SSH-key&lt;/code&gt;. It is very much in the spirit of the ssh keys
registration for github, for which you are referred to &lt;a href="https://marcohassan.github.io/bits-of-experience/posts/Multiple%20SSH%20Keys%20for%20different%20accounts%20on%20a%20single%20machine/"&gt;this post&lt;/a&gt;.
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/remotely-login-ssh-key/"&gt;Read moreâ¦&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Unix</category><guid>https://marcohassan.github.io/bits-of-experience/posts/remotely-login-ssh-key/</guid><pubDate>Fri, 01 May 2020 09:32:46 GMT</pubDate></item></channel></rss>