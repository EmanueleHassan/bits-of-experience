<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Bits of Experience</title><link>https://marcohassan.github.io/bits-of-experience/</link><description>A readable view on my studying adventures.</description><atom:link href="https://marcohassan.github.io/bits-of-experience/rss.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2019 &lt;a href="mailto:marco.hassan30@gmail.com"&gt;Marco Hassan&lt;/a&gt; </copyright><lastBuildDate>Sun, 22 Sep 2019 08:55:53 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Cloud Storage</title><link>https://marcohassan.github.io/bits-of-experience/posts/Cloud%20Storage/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;br&gt;
&lt;br&gt;

&lt;p&gt;
In the cloud environment, when deploying an application through a
kubernetes cluster a question that naturally arise is how to save
data and general information. 
&lt;/p&gt;

&lt;p&gt;
What you actually want to achieve is to mount a file system on your
containers and read it as it was local.
&lt;/p&gt;

&lt;p&gt;
This posts tries to make the point for the general approached used to
tackle the issue.
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/Cloud%20Storage/"&gt;Read more…&lt;/a&gt; (2 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>IT Architecture</category><category>Storage</category><guid>https://marcohassan.github.io/bits-of-experience/posts/Cloud%20Storage/</guid><pubDate>Wed, 18 Sep 2019 16:21:43 GMT</pubDate></item><item><title>Github - Multiple SSH Keys for different accounts on a single machine</title><link>https://marcohassan.github.io/bits-of-experience/posts/Multiple%20SSH%20Keys%20for%20different%20accounts%20on%20a%20single%20machine/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;br&gt;
&lt;br&gt;

&lt;p&gt;
Last week I set up my working laptop and the SSH key to access my
private working github repository.
&lt;/p&gt;

&lt;p&gt;
A question arise: &lt;i&gt;How to add an additional SSH-key to modify the
content on my public github registry?&lt;/i&gt;
&lt;/p&gt;

&lt;p&gt;
As usual a simple query to my favorite research engine solved the
issue. I refer therefore to the following blog post well outlining the
solution for the issue: &lt;a href="https://code.tutsplus.com/tutorials/quick-tip-how-to-work-with-github-and-multiple-accounts--net-22574"&gt;multiple keys for different accounts&lt;/a&gt;.
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/Multiple%20SSH%20Keys%20for%20different%20accounts%20on%20a%20single%20machine/"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Github</category><guid>https://marcohassan.github.io/bits-of-experience/posts/Multiple%20SSH%20Keys%20for%20different%20accounts%20on%20a%20single%20machine/</guid><pubDate>Sat, 14 Sep 2019 16:21:43 GMT</pubDate></item><item><title>Prompt Costumization</title><link>https://marcohassan.github.io/bits-of-experience/posts/Prompt%20Costumization/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;br&gt;
&lt;br&gt;

&lt;p&gt;
This month I started to work for the first time with a Mac OS. 
&lt;/p&gt;

&lt;p&gt;
As this recently became my primar working OS I immediately started by
downloading &lt;i&gt;emacs&lt;/i&gt; and customize the editor in order to optimize my
workflow.
&lt;/p&gt;

&lt;p&gt;
Annoyingly after pinning the Emacs executable to the Mac Dock and
launching Emacs from there I got troubles with the &lt;code&gt;$PATH&lt;/code&gt;.
&lt;/p&gt;

&lt;p&gt;
Emacs launched in such a way will not inherit the &lt;code&gt;$PATH&lt;/code&gt; specified
within the &lt;code&gt;~/.bash_profile&lt;/code&gt; file nor will the &lt;code&gt;M-x shell&lt;/code&gt; command of
emacs display a nice prompt.
&lt;/p&gt;

&lt;p&gt;
This article goes over my fix and the way I customized my set up to
get a nice informative prompt in the terminal.
&lt;/p&gt;


&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/Prompt%20Costumization/"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Prompt</category><guid>https://marcohassan.github.io/bits-of-experience/posts/Prompt%20Costumization/</guid><pubDate>Sun, 08 Sep 2019 16:21:43 GMT</pubDate></item><item><title>Apache Spark SQL</title><link>https://marcohassan.github.io/bits-of-experience/posts/Apache%20Spark%20SQL/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;br&gt;
&lt;br&gt;

&lt;p&gt;
This posts makes the point for Apache Spark SQL. 
&lt;/p&gt;

&lt;p&gt;
Using RDDs API might be quite annoying, especially if you are used to
the industry standard of RDMS and their SQL sytax.
&lt;/p&gt;

&lt;p&gt;
Here ApacheSparkSQL kicks in, providing a SQL interface to your data.
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/Apache%20Spark%20SQL/"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Big Data</category><guid>https://marcohassan.github.io/bits-of-experience/posts/Apache%20Spark%20SQL/</guid><pubDate>Fri, 06 Sep 2019 16:21:43 GMT</pubDate></item><item><title>Kubernetes</title><link>https://marcohassan.github.io/bits-of-experience/posts/Kubernetes/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;br&gt;
&lt;br&gt;

&lt;p&gt;
A strong orchestrator tool that operates above the container level and
allows to manage a cluster to handle containers.
&lt;/p&gt;

&lt;p&gt;
It is essentially the tool set that lets you manage containers.
&lt;/p&gt;

&lt;p&gt;
Enterprises can use it to manage the life cycle of containerized apps
in a cluster of nodes, which is a collection of worker machines such
as virtual machines (VMs) or physical machines.
&lt;/p&gt;

&lt;p&gt;
In general kubernetes try to leverage clusters in order to avoid
having a single point of failure.
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/Kubernetes/"&gt;Read more…&lt;/a&gt; (2 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>IT Architecture</category><guid>https://marcohassan.github.io/bits-of-experience/posts/Kubernetes/</guid><pubDate>Fri, 06 Sep 2019 16:21:43 GMT</pubDate></item><item><title>Microservices</title><link>https://marcohassan.github.io/bits-of-experience/posts/Microservices/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;br&gt;
&lt;br&gt;

&lt;p&gt;
Microservices refer to an application architectural style that divides
an application into components, where each component is a full, but
miniature, application that is focused on producing a single business
task.
&lt;/p&gt;

&lt;p&gt;
Each microservice has a well-defined interface and dependencies (to
other microservices and to external resources) so that it can run
fairly independently, and the team can develop it fairly
independently.
&lt;/p&gt;

&lt;p&gt;
Microservices enable developers to accomplish meaningful work working
in small teams. Small teams allow developers to be more productive
because they spend less time in meetings and decrease the need for
communication and coordination that is needed with a large team.
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/Microservices/"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>IT Architecture</category><guid>https://marcohassan.github.io/bits-of-experience/posts/Microservices/</guid><pubDate>Fri, 06 Sep 2019 16:21:43 GMT</pubDate></item><item><title>Docker</title><link>https://marcohassan.github.io/bits-of-experience/posts/Docker/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;br&gt;
&lt;br&gt;

&lt;p&gt;
The basic idea of Docker is to allow the possibility to save all of
the configuration of an application in one single image. This should
be considered as a safe environment that once properly set up can be
easily shared among different teams and once images are instantiated
all of the different teams can be sure to operate and leverage the
right configuration for running their application.
&lt;/p&gt;

&lt;p&gt;
A simple and straight forward overview about the advantage of Docker
might be found in this sense at &lt;a href="https://www.tutorialspoint.com/docker/docker_architecture.htm"&gt;docker architecture&lt;/a&gt;.
&lt;/p&gt;

&lt;p&gt;
To sum up before starting the basic idea is to create images and to
create containers based on that, which will run then the application
as defined in the docker image.
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/Docker/"&gt;Read more…&lt;/a&gt; (2 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>IT Architecture</category><guid>https://marcohassan.github.io/bits-of-experience/posts/Docker/</guid><pubDate>Mon, 02 Sep 2019 16:21:43 GMT</pubDate></item><item><title>Spark Session Initialization, RDD: Transformations and Actions</title><link>https://marcohassan.github.io/bits-of-experience/posts/rdd-transformations-and-actions/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;br&gt;
&lt;br&gt;


&lt;p&gt;
This second post present the basic set up of a Spark session and goes
over the basic transformations and actions that applies to Spark
RDDs. These are necessary given the immutability of RDDs.
&lt;/p&gt;

&lt;p&gt;
RDDs are saved in-memory by default. Nonetheless when the memory of
the machine is not sufficient to handle the data, RDDs are splitted to
disk.
&lt;/p&gt;

&lt;p&gt;
Finally, RDDs are lazy. This, means that only if the data is needed
for a certain computation the data is read from the underlying storage
system.
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/rdd-transformations-and-actions/"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Big Data</category><category>Spark</category><guid>https://marcohassan.github.io/bits-of-experience/posts/rdd-transformations-and-actions/</guid><pubDate>Wed, 21 Aug 2019 21:31:02 GMT</pubDate></item><item><title>Paredit</title><link>https://marcohassan.github.io/bits-of-experience/posts/paredit/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;br&gt;
&lt;br&gt;

&lt;p&gt;
Paredit is a nice tool to quickly deal with parenthesis when
coding. Thorough simple commands it lets you easily edit content
wrapped in parenthesis, move quickly the parenthesis themselves 
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/paredit/"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>emacs</category><category>parenthesis</category><guid>https://marcohassan.github.io/bits-of-experience/posts/paredit/</guid><pubDate>Sat, 17 Aug 2019 16:21:43 GMT</pubDate></item><item><title>PySpark Set-Up and Integration with Emacs</title><link>https://marcohassan.github.io/bits-of-experience/posts/pyspark-set-up/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;br&gt;
&lt;br&gt;

&lt;p&gt;
This post aims at providing an overview of the necessary steps
required in order to leverage Apache Spark through the Python API and
the PySpark module.
&lt;/p&gt;

&lt;p&gt;
Despite the information is vastly reported over the internet my usual
&lt;i&gt;procedere&lt;/i&gt; when I deal with new tools and software is to write a
short piece of note when learning new software that is intended as an
overview over the tool and serve as a beginner cheat sheet.
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/pyspark-set-up/"&gt;Read more…&lt;/a&gt; (4 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Big Data</category><category>emacs</category><category>Spark</category><guid>https://marcohassan.github.io/bits-of-experience/posts/pyspark-set-up/</guid><pubDate>Mon, 05 Aug 2019 21:51:11 GMT</pubDate></item></channel></rss>