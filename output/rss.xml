<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Bits of Experience</title><link>https://marcohassan.github.io/bits-of-experience/</link><description>A readable view on my studying adventures.</description><atom:link href="https://marcohassan.github.io/bits-of-experience/rss.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2020 &lt;a href="mailto:marco.hassan30@gmail.com"&gt;Marco Hassan&lt;/a&gt; </copyright><lastBuildDate>Tue, 26 May 2020 13:57:32 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>HDFS</title><link>https://marcohassan.github.io/bits-of-experience/posts/hdfs/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;br&gt;
&lt;br&gt;

&lt;p&gt;
After having discussed Object Storage, this posts continues to dig
into the storage layer by briefly introducing HDFS. It will briefly
make the point for the difference between Object Storage and HDFS as a
distributed storage option.
&lt;/p&gt;

&lt;p&gt;
Moreover, it tries to draw a line between Block Storage via Storage
Area Network (SAN), HDFS block storage and the local file system (LFS)
block storage, three topics that highly confused me at first when
writing this posts series.
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/hdfs/"&gt;Read more…&lt;/a&gt; (5 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Big Data</category><guid>https://marcohassan.github.io/bits-of-experience/posts/hdfs/</guid><pubDate>Sun, 24 May 2020 15:01:55 GMT</pubDate></item><item><title>Storage Layer - Object Storage</title><link>https://marcohassan.github.io/bits-of-experience/posts/storage-layer-object-storage/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;br&gt;
&lt;br&gt;

&lt;p&gt;
There are essentially three major storage options. The first being
Block Storage, the second being File Storage and the last being Object
Storage. You can find a good introduction to the three different
options at the &lt;a href="https://www.ibm.com/cloud/learn/block-storage"&gt;following link&lt;/a&gt;.
&lt;/p&gt;

&lt;p&gt;
This post briefly introduces the third storage option above
i.e. object storage.
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/storage-layer-object-storage/"&gt;Read more…&lt;/a&gt; (3 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Big Data</category><guid>https://marcohassan.github.io/bits-of-experience/posts/storage-layer-object-storage/</guid><pubDate>Sun, 24 May 2020 13:31:11 GMT</pubDate></item><item><title>Spark Architecture</title><link>https://marcohassan.github.io/bits-of-experience/posts/spark-architecture/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;br&gt;
&lt;br&gt;

&lt;p&gt;
After the different posts written on setting up a spark session and
working with RDDs &lt;a href="https://marcohassan.github.io/bits-of-experience/categories/spark/"&gt;available here&lt;/a&gt;, this post briefly goes a more into
the detail of the spark physical layer.
&lt;/p&gt;

&lt;p&gt;
It will introduce the concept of DAGs and will make a comparison of
spark and the more restrictive &lt;a href="https://marcohassan.github.io/bits-of-experience/posts/mapreduce/"&gt;MapReduce&lt;/a&gt;. 
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/spark-architecture/"&gt;Read more…&lt;/a&gt; (4 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Big Data</category><category>Spark</category><guid>https://marcohassan.github.io/bits-of-experience/posts/spark-architecture/</guid><pubDate>Sun, 24 May 2020 09:44:04 GMT</pubDate></item><item><title>MapReduce</title><link>https://marcohassan.github.io/bits-of-experience/posts/mapreduce/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;br&gt;
&lt;br&gt;

&lt;p&gt;
This post go briefly over the key idea of using MapReduce as a way to
parallelize operations over multiple distributed machines. 
&lt;/p&gt;

&lt;p&gt;
This was in fact the first tool that was developed for parallelizing
computation over multiple machines and not simply to use multiple
machines for the storage layer.
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/mapreduce/"&gt;Read more…&lt;/a&gt; (7 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Big Data</category><guid>https://marcohassan.github.io/bits-of-experience/posts/mapreduce/</guid><pubDate>Sat, 23 May 2020 12:25:10 GMT</pubDate></item><item><title>YARN</title><link>https://marcohassan.github.io/bits-of-experience/posts/yarn/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;br&gt;
&lt;br&gt;

&lt;p&gt;
This post briefly introduces YARN - &lt;b&gt;Yet Another Resource Negotiator&lt;/b&gt;.
It was introduced to overcome the limits of MapReduce v 1.0,
especially the idle resources component, &lt;a href="https://marcohassan.github.io/bits-of-experience/posts/mapreduce/"&gt;discussed in the previous post&lt;/a&gt;.
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/yarn/"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Big Data</category><guid>https://marcohassan.github.io/bits-of-experience/posts/yarn/</guid><pubDate>Sat, 23 May 2020 12:21:55 GMT</pubDate></item><item><title>NLP text classification</title><link>https://marcohassan.github.io/bits-of-experience/posts/nlp-text-classification/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;br&gt;
&lt;br&gt;

&lt;p&gt;
This post briefly introduce two key concepts for representing NLP
text data. The first is simply based on some frequency metric given a
text corpus, the second &lt;code&gt;word2vec&lt;/code&gt; tries a more elegant and elaborate
approach by trying to extract the semantical representation of words
in a low dimensional space.
&lt;/p&gt;

&lt;p&gt;
Both will be briefly explained. 
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/nlp-text-classification/"&gt;Read more…&lt;/a&gt; (4 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><guid>https://marcohassan.github.io/bits-of-experience/posts/nlp-text-classification/</guid><pubDate>Wed, 20 May 2020 20:22:37 GMT</pubDate></item><item><title>Slovak Learning</title><link>https://marcohassan.github.io/bits-of-experience/posts/slovak-learning/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;br&gt;
&lt;br&gt;

&lt;p&gt;
Ok. This post is likely not interesting for any visitor. I am learning
the language and after a discussion with my girlfriend it turned out I
rather write down the new vocabulary for learning the language the
fastest.
&lt;/p&gt;

&lt;p&gt;
In this post I write down my progress and I can go back to it at later
stages.
&lt;/p&gt;


&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/slovak-learning/"&gt;Read more…&lt;/a&gt; (4 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Slovak</category><guid>https://marcohassan.github.io/bits-of-experience/posts/slovak-learning/</guid><pubDate>Sat, 16 May 2020 07:45:34 GMT</pubDate></item><item><title>RDDs Transformations and Actions</title><link>https://marcohassan.github.io/bits-of-experience/posts/rdds-transformations-and-actions/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;br&gt;
&lt;br&gt;


&lt;p&gt;
This post continues the discussion started a few times ago on &lt;a href="https://marcohassan.github.io/bits-of-experience/posts/spark-session-initalization/"&gt;RDD and
Spark&lt;/a&gt;.
&lt;/p&gt;

&lt;p&gt;
I will try to go here in the lifecycle of an RDD and will present the
major Transformation and Actions functions. I will moreover touch on
the physical implementation of Spark as this will give you the mental
tools to properly understand how to properly structure your Spark
workflow in order to optimize the performance.   
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/rdds-transformations-and-actions/"&gt;Read more…&lt;/a&gt; (10 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Big Data</category><category>Spark</category><guid>https://marcohassan.github.io/bits-of-experience/posts/rdds-transformations-and-actions/</guid><pubDate>Sun, 03 May 2020 13:51:24 GMT</pubDate></item><item><title>On The Brave Browser</title><link>https://marcohassan.github.io/bits-of-experience/posts/how-to-verify-your-github-page-with-brave-rewards/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;p&gt;
So I recently started to work with the Brave browser. I started to see
it on a LinkIn cryptocurrency influencer webpage and I was rather
suspicious. I noticed it but I did not further investigate. I just had
integrated &lt;code&gt;DuckDuckGo&lt;/code&gt; into my Chrome browser and I was feeling ok
with that at the time.
&lt;/p&gt;

&lt;p&gt;
I started to get quite interested in it when I saw that the Prof. of
Communication Networks of the University I am currently visiting was
using it. He, as an expert using the browser. There must be some
goodness in it.
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/how-to-verify-your-github-page-with-brave-rewards/"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Brave Browser</category><guid>https://marcohassan.github.io/bits-of-experience/posts/how-to-verify-your-github-page-with-brave-rewards/</guid><pubDate>Sat, 02 May 2020 17:05:36 GMT</pubDate></item><item><title>Remotely Login - SSH Key</title><link>https://marcohassan.github.io/bits-of-experience/posts/remotely-login-ssh-key/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;br&gt;
&lt;br&gt;

&lt;p&gt;
This posts outlines the way to save ssh into a virtual machine using a
generated &lt;code&gt;SSH-key&lt;/code&gt;. It is very much in the spirit of the ssh keys
registration for github, for which you are referred to &lt;a href="https://marcohassan.github.io/bits-of-experience/posts/Multiple%20SSH%20Keys%20for%20different%20accounts%20on%20a%20single%20machine/"&gt;this post&lt;/a&gt;.
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/remotely-login-ssh-key/"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Unix</category><guid>https://marcohassan.github.io/bits-of-experience/posts/remotely-login-ssh-key/</guid><pubDate>Fri, 01 May 2020 09:32:46 GMT</pubDate></item></channel></rss>