<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Bits of Experience</title><link>https://marcohassan.github.io/bits-of-experience/</link><description>A readable view on my studying adventures.</description><atom:link href="https://marcohassan.github.io/bits-of-experience/rss.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2019 &lt;a href="mailto:marco.hassan30@gmail.com"&gt;Marco Hassan&lt;/a&gt; </copyright><lastBuildDate>Tue, 08 Oct 2019 20:26:22 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>One-way Analysis of Variance</title><link>https://marcohassan.github.io/bits-of-experience/posts/one-way-analysis-of-variance/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;br&gt;
&lt;br&gt;

&lt;p&gt;
This post discusses the comparison of different effects among
different groups. It will start with a refresh of the &lt;i&gt;t-test&lt;/i&gt; 
used for &lt;i&gt;pair-wise&lt;/i&gt; comparison among different groups and it extends
the concept for situations with \(groups \geq 2\) in order to check
whether results are statistically significant among them.
&lt;/p&gt;

&lt;div id="outline-container-sec-1" class="outline-2"&gt;
&lt;h2 id="sec-1"&gt;T-test&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-1"&gt;
&lt;p&gt;
As a brief reminder it is possible to compare the means of two
independent groups using a &lt;i&gt;two sample t-test&lt;/i&gt;. 
&lt;/p&gt;

&lt;p&gt;
The basic idea of the test is that assuming i.i.d. observation from
two different groups \(X_1\) and \(X_2\) it is possible to compute summary
statistics for the two.
&lt;/p&gt;

&lt;p&gt;
It is then possible to prove, that the computed mean from the
i.i.d. observation converges in probability to the normal distribution
due to the &lt;i&gt;Law of the Large Numbers&lt;/i&gt; and the &lt;i&gt;Central Limit
Theorem&lt;/i&gt;. 
&lt;/p&gt;

&lt;p&gt;
\[ \bar{X} \overset{p}{\to} N (\mu, \sigma^2) \]
&lt;/p&gt;

&lt;p&gt;
Given the further fact that the linear combination of normal
distributed variables is normal distributed it follows that the
difference among the mean value of the groups of choice \(X_1\) and
\(X_2\) is as well normal distributed with
&lt;/p&gt;

&lt;p&gt;
\[ \bar{X}_1 - \bar{X}_2 \overset{p}{\to} N (\mu_1 - \mu_2, \sigma_1^2 + \sigma_2^2) \]
&lt;/p&gt;

&lt;p&gt;
where the variance does not display the covariance between the two
group observation as these were assumed to be independent.
&lt;/p&gt;

&lt;p&gt;
This is essentially the basic theory on which the t-test relies. The
difference in the mean of two groups of choice is then simply
counterpoised to the Null \(H_0\) and the normal distribution is
replaced by the t-distribution due to the final sample of the
observations.
&lt;/p&gt;

&lt;br&gt;
&lt;/div&gt;
&lt;/div&gt;


&lt;div id="outline-container-sec-2" class="outline-2"&gt;
&lt;h2 id="sec-2"&gt;The \(n \geq 2\) case&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-2"&gt;
&lt;p&gt;
In case of existing groups \(n \geq 2\) it is not possible to directly
apply the t-test outlined above. It is therefore necessary to expand
the theory developed so far.
&lt;/p&gt;

&lt;p&gt;
Consider therefore &lt;i&gt;m&lt;/i&gt; different groups containing each &lt;i&gt;n&lt;/i&gt;
observations. Assuming moreover that for each of the &lt;i&gt;m&lt;/i&gt; different
groups we observed a \(N(\mu_i, \sigma)\) response variable it is clear
that we observe &lt;i&gt;m + 1&lt;/i&gt; parameters, because of the &lt;i&gt;m&lt;/i&gt; different means
and the common variance shared among the groups.
&lt;/p&gt;

&lt;p&gt;
Notice now that albeit the different groups will display different
means it is possible to rewrite their response variable disentangling
the different effects affecting the response variables; namely in:
&lt;/p&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;a common component μ shared among all the response variables in
the groups different groups
&lt;/li&gt;

&lt;li&gt;a group specific α&lt;sub&gt;i&lt;/sub&gt; component adjusting the effect of the
common component μ to reflect the information of the group
specific mean μ&lt;sub&gt;i&lt;/sub&gt;
&lt;/li&gt;

&lt;li&gt;a stochastic component ε&lt;sub&gt;ij&lt;/sub&gt; ~ N (0, σ&lt;sup&gt;2&lt;/sup&gt;). 
&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
It follows that 
&lt;/p&gt;

&lt;p&gt;
Notice however how an additional parameter was introduced. &lt;i&gt;m + 2&lt;/i&gt;
parameters result from the above equation. &lt;i&gt;We silently introduced an
additional parameter&lt;/i&gt; with the result that the problem is not
&lt;i&gt;identifiable&lt;/i&gt; anymore given that we currently have &lt;i&gt;m+1&lt;/i&gt; parameters
to mode &lt;i&gt;m&lt;/i&gt; means. 
&lt;/p&gt;

&lt;p&gt;
In order to address the issue it is therefore necessary to add a side
constant in order to add a further equality that will restore the
&lt;i&gt;well specification&lt;/i&gt; of the problem. 
&lt;/p&gt;

&lt;p&gt;
It is common to apply one of three major &lt;i&gt;side-constrains&lt;/i&gt;:
&lt;/p&gt;

&lt;p&gt;
(notice that the table below is not well rendered in nikola. read the
&lt;a href="http://eyesfreelinux.ninja/posts/nikola-plugins.html"&gt;following post&lt;/a&gt; or consider using latex tables.)
&lt;/p&gt;


&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="left"&gt;

&lt;col class="left"&gt;

&lt;col class="left"&gt;

&lt;col class="left"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="left"&gt;Name&lt;/th&gt;
&lt;th scope="col" class="left"&gt;Side-Constant&lt;/th&gt;
&lt;th scope="col" class="left"&gt;Interpretation of μ&lt;/th&gt;
&lt;th scope="col" class="left"&gt;&lt;code&gt;R-code&lt;/code&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="left"&gt;sum-to-zero&lt;/td&gt;
&lt;td class="left"&gt;∑ α&lt;sub&gt;i&lt;/sub&gt;&lt;sup&gt;m&lt;/sup&gt;&lt;sub&gt;i=1&lt;/sub&gt;  = 0&lt;/td&gt;
&lt;td class="left"&gt;μ = \(frac{1}{m}\) ∑ μ&lt;sub&gt;i&lt;/sub&gt;&lt;/td&gt;
&lt;td class="left"&gt; &lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="left"&gt; &lt;/td&gt;
&lt;td class="left"&gt; &lt;/td&gt;
&lt;td class="left"&gt; &lt;/td&gt;
&lt;td class="left"&gt; &lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="left"&gt;reference group&lt;/td&gt;
&lt;td class="left"&gt;α&lt;sub&gt;1&lt;/sub&gt; = 0&lt;/td&gt;
&lt;td class="left"&gt;μ = μ&lt;sub&gt;1&lt;/sub&gt;&lt;/td&gt;
&lt;td class="left"&gt;&lt;code&gt;contr.sum&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="left"&gt; &lt;/td&gt;
&lt;td class="left"&gt; &lt;/td&gt;
&lt;td class="left"&gt; &lt;/td&gt;
&lt;td class="left"&gt; &lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="left"&gt;weigthed sum-to-zero&lt;/td&gt;
&lt;td class="left"&gt;∑&lt;sup&gt;m&lt;/sup&gt;&lt;sub&gt;i=1&lt;/sub&gt; n&lt;sub&gt;i&lt;/sub&gt; α&lt;sub&gt;i&lt;/sub&gt; = 0&lt;/td&gt;
&lt;td class="left"&gt;μ = \(\frac{1}{N}\) ∑ n&lt;sub&gt;i&lt;/sub&gt; μ&lt;sub&gt;i&lt;/sub&gt;&lt;/td&gt;
&lt;td class="left"&gt;&lt;code&gt;contr.tratment&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;
Notice therefore that after adding the side-constant only &lt;i&gt;g-1&lt;/i&gt;
elements of the groups are allowed to vary freely and therefore we
have &lt;i&gt;g-1&lt;/i&gt; degrees of freedom.
&lt;/p&gt;

&lt;br&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-sec-3" class="outline-2"&gt;
&lt;h2 id="sec-3"&gt;Variance Decomposition&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-3"&gt;
&lt;p&gt;
Given the decomposition above it is possible to set up the general
framework for comparing the different effect of different treatments
groups.
&lt;/p&gt;

&lt;p&gt;
First of all we notice that we estimate the &lt;i&gt;g-1&lt;/i&gt; parameters of
interest by the &lt;b&gt;least square method&lt;/b&gt;.
&lt;/p&gt;

&lt;p&gt;
It follows therefore 
&lt;/p&gt;

&lt;p&gt;
\[ \hat{\mu} \textasciicircum{\alpha_i} = argmin_{\mu, \alpha_i} \sum^{m}_{i=1} \sum^{n}_{j=1} (y_{ij} - \mu - \alpha_i)^2   \]
&lt;/p&gt;




&lt;br&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-sec-4" class="outline-2"&gt;
&lt;h2 id="sec-4"&gt;Literature&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-4"&gt;
&lt;p&gt;
&lt;a href="https://stat.ethz.ch/lectures/as19/anova.php#course_materials"&gt;Applied Anaysis of Variance - ETH Lecture Notes - Autumn 2019&lt;/a&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>ANOVA</category><guid>https://marcohassan.github.io/bits-of-experience/posts/one-way-analysis-of-variance/</guid><pubDate>Sun, 22 Sep 2019 13:22:52 GMT</pubDate></item><item><title>Analysis of Variance - Terminology</title><link>https://marcohassan.github.io/bits-of-experience/posts/analysis-of-variance-terminology/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;br&gt;
&lt;br&gt;

&lt;p&gt;
This is the first post making notes on the ANOVA - &lt;i&gt;analysis of
variance&lt;/i&gt;.
&lt;/p&gt;

&lt;p&gt;
The basic research question of the study of the ANOVA is how to
compare different research outputs and how to assess whether the
difference among two or more group outcomes are statistically
significant or rather the result of randomness. In this sense a
special attention is given to the &lt;b&gt;experimental error&lt;/b&gt;, which
represent the natural variation and error observed in an untouched
environment from the unconditioned underlying data generating
process. Just with this notion of "true/underlying" randomness the
interpretation of the results is sensible.
&lt;/p&gt;

&lt;p&gt;
This first post, starts to explore the above by making the point of
the difficulties underlying such studies and the importance of the
experimental settings which the researcher is exposed to. 
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/analysis-of-variance-terminology/"&gt;Read more…&lt;/a&gt; (3 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>ANOVA</category><guid>https://marcohassan.github.io/bits-of-experience/posts/analysis-of-variance-terminology/</guid><pubDate>Sun, 22 Sep 2019 12:24:02 GMT</pubDate></item><item><title>Cloud Storage</title><link>https://marcohassan.github.io/bits-of-experience/posts/Cloud%20Storage/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;br&gt;
&lt;br&gt;

&lt;p&gt;
In the cloud environment, when deploying an application through a
kubernetes cluster a question that naturally arise is how to save
data and general information. 
&lt;/p&gt;

&lt;p&gt;
What you actually want to achieve is to mount a file system on your
containers and read it as it was local.
&lt;/p&gt;

&lt;p&gt;
This posts tries to make the point for the general approached used to
tackle the issue.
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/Cloud%20Storage/"&gt;Read more…&lt;/a&gt; (2 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>IT Architecture</category><category>Storage</category><guid>https://marcohassan.github.io/bits-of-experience/posts/Cloud%20Storage/</guid><pubDate>Wed, 18 Sep 2019 16:21:43 GMT</pubDate></item><item><title>Github - Multiple SSH Keys for different accounts on a single machine</title><link>https://marcohassan.github.io/bits-of-experience/posts/Multiple%20SSH%20Keys%20for%20different%20accounts%20on%20a%20single%20machine/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;br&gt;
&lt;br&gt;

&lt;p&gt;
Last week I set up my working laptop and the SSH key to access my
private working github repository.
&lt;/p&gt;

&lt;p&gt;
A question arise: &lt;i&gt;How to add an additional SSH-key to modify the
content on my public github registry?&lt;/i&gt;
&lt;/p&gt;

&lt;p&gt;
As usual a simple query to my favorite research engine solved the
issue. I refer therefore to the following blog post well outlining the
solution for the issue: &lt;a href="https://code.tutsplus.com/tutorials/quick-tip-how-to-work-with-github-and-multiple-accounts--net-22574"&gt;multiple keys for different accounts&lt;/a&gt;.
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/Multiple%20SSH%20Keys%20for%20different%20accounts%20on%20a%20single%20machine/"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Github</category><guid>https://marcohassan.github.io/bits-of-experience/posts/Multiple%20SSH%20Keys%20for%20different%20accounts%20on%20a%20single%20machine/</guid><pubDate>Sat, 14 Sep 2019 16:21:43 GMT</pubDate></item><item><title>Prompt Costumization</title><link>https://marcohassan.github.io/bits-of-experience/posts/Prompt%20Costumization/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;br&gt;
&lt;br&gt;

&lt;p&gt;
This month I started to work for the first time with a Mac OS. 
&lt;/p&gt;

&lt;p&gt;
As this recently became my primar working OS I immediately started by
downloading &lt;i&gt;emacs&lt;/i&gt; and customize the editor in order to optimize my
workflow.
&lt;/p&gt;

&lt;p&gt;
Annoyingly after pinning the Emacs executable to the Mac Dock and
launching Emacs from there I got troubles with the &lt;code&gt;$PATH&lt;/code&gt;.
&lt;/p&gt;

&lt;p&gt;
Emacs launched in such a way will not inherit the &lt;code&gt;$PATH&lt;/code&gt; specified
within the &lt;code&gt;~/.bash_profile&lt;/code&gt; file nor will the &lt;code&gt;M-x shell&lt;/code&gt; command of
emacs display a nice prompt.
&lt;/p&gt;

&lt;p&gt;
This article goes over my fix and the way I customized my set up to
get a nice informative prompt in the terminal.
&lt;/p&gt;


&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/Prompt%20Costumization/"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Prompt</category><guid>https://marcohassan.github.io/bits-of-experience/posts/Prompt%20Costumization/</guid><pubDate>Sun, 08 Sep 2019 16:21:43 GMT</pubDate></item><item><title>Apache Spark SQL</title><link>https://marcohassan.github.io/bits-of-experience/posts/Apache%20Spark%20SQL/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;br&gt;
&lt;br&gt;

&lt;p&gt;
This posts makes the point for Apache Spark SQL. 
&lt;/p&gt;

&lt;p&gt;
Using RDDs API might be quite annoying, especially if you are used to
the industry standard of RDMS and their SQL sytax.
&lt;/p&gt;

&lt;p&gt;
Here ApacheSparkSQL kicks in, providing a SQL interface to your data.
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/Apache%20Spark%20SQL/"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Big Data</category><category>Spark</category><guid>https://marcohassan.github.io/bits-of-experience/posts/Apache%20Spark%20SQL/</guid><pubDate>Fri, 06 Sep 2019 16:21:43 GMT</pubDate></item><item><title>Kubernetes</title><link>https://marcohassan.github.io/bits-of-experience/posts/Kubernetes/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;br&gt;
&lt;br&gt;

&lt;p&gt;
A strong orchestrator tool that operates above the container level and
allows to manage a cluster to handle containers.
&lt;/p&gt;

&lt;p&gt;
It is essentially the tool set that lets you manage containers.
&lt;/p&gt;

&lt;p&gt;
Enterprises can use it to manage the life cycle of containerized apps
in a cluster of nodes, which is a collection of worker machines such
as virtual machines (VMs) or physical machines.
&lt;/p&gt;

&lt;p&gt;
In general kubernetes try to leverage clusters in order to avoid
having a single point of failure.
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/Kubernetes/"&gt;Read more…&lt;/a&gt; (2 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>IT Architecture</category><guid>https://marcohassan.github.io/bits-of-experience/posts/Kubernetes/</guid><pubDate>Fri, 06 Sep 2019 16:21:43 GMT</pubDate></item><item><title>Microservices</title><link>https://marcohassan.github.io/bits-of-experience/posts/Microservices/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;br&gt;
&lt;br&gt;

&lt;p&gt;
Microservices refer to an application architectural style that divides
an application into components, where each component is a full, but
miniature, application that is focused on producing a single business
task.
&lt;/p&gt;

&lt;p&gt;
Each microservice has a well-defined interface and dependencies (to
other microservices and to external resources) so that it can run
fairly independently, and the team can develop it fairly
independently.
&lt;/p&gt;

&lt;p&gt;
Microservices enable developers to accomplish meaningful work working
in small teams. Small teams allow developers to be more productive
because they spend less time in meetings and decrease the need for
communication and coordination that is needed with a large team.
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/Microservices/"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>IT Architecture</category><guid>https://marcohassan.github.io/bits-of-experience/posts/Microservices/</guid><pubDate>Fri, 06 Sep 2019 16:21:43 GMT</pubDate></item><item><title>Docker</title><link>https://marcohassan.github.io/bits-of-experience/posts/Docker/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;br&gt;
&lt;br&gt;

&lt;p&gt;
The basic idea of Docker is to allow the possibility to save all of
the configuration of an application in one single image. This should
be considered as a safe environment that once properly set up can be
easily shared among different teams and once images are instantiated
all of the different teams can be sure to operate and leverage the
right configuration for running their application.
&lt;/p&gt;

&lt;p&gt;
A simple and straight forward overview about the advantage of Docker
might be found in this sense at &lt;a href="https://www.tutorialspoint.com/docker/docker_architecture.htm"&gt;docker architecture&lt;/a&gt;.
&lt;/p&gt;

&lt;p&gt;
To sum up before starting the basic idea is to create images and to
create containers based on that, which will run then the application
as defined in the docker image.
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/Docker/"&gt;Read more…&lt;/a&gt; (2 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>IT Architecture</category><guid>https://marcohassan.github.io/bits-of-experience/posts/Docker/</guid><pubDate>Mon, 02 Sep 2019 16:21:43 GMT</pubDate></item><item><title>Spark Session Initialization, RDD: Transformations and Actions</title><link>https://marcohassan.github.io/bits-of-experience/posts/rdd-transformations-and-actions/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;br&gt;
&lt;br&gt;


&lt;p&gt;
This second post present the basic set up of a Spark session and goes
over the basic transformations and actions that applies to Spark
RDDs. These are necessary given the immutability of RDDs.
&lt;/p&gt;

&lt;p&gt;
RDDs are saved in-memory by default. Nonetheless when the memory of
the machine is not sufficient to handle the data, RDDs are splitted to
disk.
&lt;/p&gt;

&lt;p&gt;
Finally, RDDs are lazy. This, means that only if the data is needed
for a certain computation the data is read from the underlying storage
system.
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/rdd-transformations-and-actions/"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Big Data</category><category>Spark</category><guid>https://marcohassan.github.io/bits-of-experience/posts/rdd-transformations-and-actions/</guid><pubDate>Wed, 21 Aug 2019 21:31:02 GMT</pubDate></item></channel></rss>