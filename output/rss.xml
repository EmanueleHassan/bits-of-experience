<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Bits of Experience</title><link>https://marcohassan.github.io/bits-of-experience/</link><description>A readable view on my studying adventures.</description><atom:link href="https://marcohassan.github.io/bits-of-experience/rss.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2020 &lt;a href="mailto:marco.hassan30@gmail.com"&gt;Marco Hassan&lt;/a&gt; </copyright><lastBuildDate>Fri, 17 Jul 2020 07:14:10 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>On Scalability</title><link>https://marcohassan.github.io/bits-of-experience/posts/on-scalability/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;p&gt;
This post quickly examines two different laws of scalability: Amdahl's
law and Gustafson's law.
&lt;/p&gt;

&lt;p&gt;
Both laws addresses the extent to which a problem can be speed up
through scaling out. 
&lt;/p&gt;

&lt;p&gt;
The premise of setting under which the laws operate is
different. Amdahl's law assumes a &lt;b&gt;fix problem size&lt;/b&gt;, while
Gustafson's law assumes an &lt;b&gt;an expanding problem size in the number of
machines scaling out&lt;/b&gt;.
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/on-scalability/"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>IT Architecture</category><guid>https://marcohassan.github.io/bits-of-experience/posts/on-scalability/</guid><pubDate>Fri, 17 Jul 2020 06:43:49 GMT</pubDate></item><item><title>Spark UI</title><link>https://marcohassan.github.io/bits-of-experience/posts/spark-ui/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;style&gt;
img {
display: block;
margin-left: auto;
margin-right: auto;
margin-top: 60px;
margin-bottom: 60px;
}
&lt;/style&gt;




&lt;p&gt;
This post is a reminder for me to use Spark UI when working as it
greatly improves debugging and allows you to better understand of
Spark.
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/spark-ui/"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Spark</category><guid>https://marcohassan.github.io/bits-of-experience/posts/spark-ui/</guid><pubDate>Thu, 16 Jul 2020 15:31:07 GMT</pubDate></item><item><title>Python Pipelines</title><link>https://marcohassan.github.io/bits-of-experience/posts/python-pipelines/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;br&gt;

&lt;p&gt;
Pipelines are important when working on ML projects in python. They
first appeared through the implementation of ML tasks sklearn. There
the design setting of the maintainer of the library was to design a
set of APIs through which to govern the entire ML process. The three
major class of APIs are &lt;code&gt;tranformers&lt;/code&gt;, i.e. transformations to the
feature and dependent variables, &lt;code&gt;estimators&lt;/code&gt;, i.e. the ML model that
you fit on your transformed features matrix, and finally &lt;code&gt;predictors&lt;/code&gt;,
i.e. the API method used for predicting your desired variable based on
your fitted estimator.
&lt;/p&gt;

&lt;p&gt;
The above logic became so widespread that further important
libraries - such as the spark ML modules for using ML models
leveraging the spark engine for distributed computation - decided to
keep the same design setting and keep the above API structure.
&lt;/p&gt;

&lt;p&gt;
This posts aims at keeping an overview of some important transformers,
estimators and predictors as well as to set up the general framework
to combine the three together in easy to use Pipelines. 
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/python-pipelines/"&gt;Read more…&lt;/a&gt; (5 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Machine Learning</category><category>Python</category><guid>https://marcohassan.github.io/bits-of-experience/posts/python-pipelines/</guid><pubDate>Thu, 02 Jul 2020 10:44:32 GMT</pubDate></item><item><title>On the Architecture of Query Engines - Batch vs Streaming</title><link>https://marcohassan.github.io/bits-of-experience/posts/on-the-architecture-of-query-engines-batch-vs-streaming/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;p&gt;
In the previous post we addressed the Jsoniq declarative and
functional language for querying data in tree shape. It was mentioned
that due to data independence it is possible to run such Java Library
in combination to many Big Data frameworks and in general in
combination with the Hadoop framework - MapReduce, Spark, HDFS etc -.
&lt;/p&gt;

&lt;p&gt;
This post addresses the importance of understanding how a query engine
operates despite the data independence. 
&lt;/p&gt;

&lt;p&gt;
I.e. you might well be able to run and retrieve your desired data
without understanding how different engines work under the hood;
however you might then not write meaningful queries or run into issue
if your hardware infrastructure is not suited to fit your desired data
size and query engine operations.
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/on-the-architecture-of-query-engines-batch-vs-streaming/"&gt;Read more…&lt;/a&gt; (4 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Big Data</category><guid>https://marcohassan.github.io/bits-of-experience/posts/on-the-architecture-of-query-engines-batch-vs-streaming/</guid><pubDate>Sat, 27 Jun 2020 09:46:43 GMT</pubDate></item><item><title>On Data Idependence - Querying Trees. </title><link>https://marcohassan.github.io/bits-of-experience/posts/on-data-idependence-querying-trees/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;style&gt;
img {
display: block;
margin-top: 60px;
margin-bottom: 60px;
margin-left: auto;
margin-right: auto;
}
&lt;/style&gt;

&lt;p&gt;
So far we introduced data objects in their tree format. We saw that
being HDFS agnostic you can save data in their tree shape - be it
&lt;code&gt;JSON&lt;/code&gt; or &lt;code&gt;XML&lt;/code&gt; - in &lt;code&gt;HDFS&lt;/code&gt;. We see how it was possible to operate on
such tree data directly via the pyspark RDD low level API. Moreover,
we briefly tackled the idea of compressing the information in the tree
data model into dataframes and on how to handle these via &lt;code&gt;sparkSQL&lt;/code&gt;
with its &lt;a href="https://marcohassan.github.io/bits-of-experience/posts/spark-session-initalization/"&gt;explode operators&lt;/a&gt;.
&lt;/p&gt;

&lt;p&gt;
Finally, we addressed document stores, &lt;code&gt;NoSQL&lt;/code&gt; databases that are
especially designed to deal with data in their tree shape. We saw the
extent to which document stores rely on indices and the consequent
benefit over HDFS when it comes to find indexed data as no scan over
the entire dataset has to be executed.
&lt;/p&gt;

&lt;p&gt;
Given the interesting case for document store but the limited
scalability the question is on whether we could leverage data
independence for using a &lt;b&gt;declarative language&lt;/b&gt; in a similar spirit of
the &lt;b&gt;SQL&lt;/b&gt; that is appositely created to deal with tree-modeled data. 
&lt;/p&gt;

&lt;p&gt;
This posts confronts with the above case and introduces a &lt;b&gt;Jsoniq&lt;/b&gt; one
of the many languages that are emerging in this Big Data decade to
deal with tree-modeled data in a declarative way. This will allow to
deal with tree-data without having to use the low-level RDD API or the
approximative mapping of the tree structure into dataframes.
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/on-data-idependence-querying-trees/"&gt;Read more…&lt;/a&gt; (3 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Big Data</category><guid>https://marcohassan.github.io/bits-of-experience/posts/on-data-idependence-querying-trees/</guid><pubDate>Sat, 27 Jun 2020 07:23:56 GMT</pubDate></item><item><title>Document Stores</title><link>https://marcohassan.github.io/bits-of-experience/posts/document-stores/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;style&gt;
img{
  display: block;
  margin-left: auto;
  margin-right: auto;
}
&lt;/style&gt;


&lt;p&gt;
This post briefly goes over the idea of document stores. These are
databases that were appositely creating for dealing with data in its
tree shape. Some act on top of &lt;code&gt;json&lt;/code&gt; documents some on top of &lt;code&gt;XML&lt;/code&gt;.
&lt;/p&gt;

&lt;p&gt;
The idea is that albeit you might find some tweaks for working with
tree data shapes (think for instance at the &lt;code&gt;EXPLODE&lt;/code&gt; function of
sparkSQL) you will always have difficulties in storing a tree data
model into a relational table.
&lt;/p&gt;

&lt;p&gt;
This is mainly due to the two properties of trees that are not
available in relational tables being:
&lt;/p&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;heterogeneity&lt;/li&gt;

&lt;li&gt;nestedness&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
In this sense document stores are schemaless. You might save the
document however you want and validate them ex-post.
&lt;/p&gt;

&lt;p&gt;
To overcome data impedance of storing tree modeled data into
relational tables new databases are emerging that allows to store and
query directly json documents. However, notice that such databases are
at their infancy and to this stage they don't have the scaling force
of HDFS. I.e. you they scale just up to handful of machines.
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/document-stores/"&gt;Read more…&lt;/a&gt; (9 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Big Data</category><guid>https://marcohassan.github.io/bits-of-experience/posts/document-stores/</guid><pubDate>Fri, 26 Jun 2020 15:46:36 GMT</pubDate></item><item><title>Wide Column Store - HBase</title><link>https://marcohassan.github.io/bits-of-experience/posts/wide-column-store-hbase/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;style&gt;
img {
  display: block;
  margin-left: auto;
  margin-right: auto;
}
&lt;/style&gt;


&lt;p&gt;
The idea of wide column store is to extend the idea of RDBMS,
i.e. storing and managing data in tabular format, and to bring it into
the BigData world. 
&lt;/p&gt;

&lt;p&gt;
This post will briefly address the different characteristics of
Hbase. I.e. it will first look at the logical idea for keeping data in
a tabular form in the big data world. It will then look at the
architecture behind Hbase and as you will see the basis for it is the
good old &lt;i&gt;master-slave&lt;/i&gt; in parallel to HDFS that will be here used as
the storage layer.
&lt;/p&gt;

&lt;p&gt;
Finally it will briefly address the physical storage layer.
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/wide-column-store-hbase/"&gt;Read more…&lt;/a&gt; (11 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Big Data</category><guid>https://marcohassan.github.io/bits-of-experience/posts/wide-column-store-hbase/</guid><pubDate>Fri, 26 Jun 2020 10:55:39 GMT</pubDate></item><item><title>Working with Trees - Json And XML Sytanx and Validation</title><link>https://marcohassan.github.io/bits-of-experience/posts/working-with-trees-json-and-xml-sytanx-and-validation/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;style&gt;
img {
  display: block;
  margin-left: auto;
  margin-right: auto;
}
&lt;/style&gt;

&lt;p&gt;
This posts goes over the syntax of XML and Json; two of the most
widely used formats for storing data in tree form. 
&lt;/p&gt;

&lt;p&gt;
It also goes over the validation of the data in the two
formats. Recall that today other formats are more used in the Big Data
world. Think for instance at &lt;code&gt;parquet&lt;/code&gt;. With a basic understanding of
the formats above it will however be easy for you to get on going with
the other formats.
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/working-with-trees-json-and-xml-sytanx-and-validation/"&gt;Read more…&lt;/a&gt; (17 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Big Data</category><guid>https://marcohassan.github.io/bits-of-experience/posts/working-with-trees-json-and-xml-sytanx-and-validation/</guid><pubDate>Thu, 25 Jun 2020 08:10:01 GMT</pubDate></item><item><title>Reinforcement Learning</title><link>https://marcohassan.github.io/bits-of-experience/posts/reinforcement-learning/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;style&gt;
img {
  display: block;
  margin-left: auto;
  margin-right: auto;
}
&lt;/style&gt;

&lt;p&gt;
Here are some notes based on the &lt;i&gt;Artificial Intelligence:
Reinforcement Learning in Python&lt;/i&gt; Udemy course.
&lt;/p&gt;

&lt;p&gt;
This are very personal notes that do not intend to substitute the
course. The guy is good. I recommend his courses. I am enjoying and
the way he teaches with minor exercises that makes you well think is
good. 
&lt;/p&gt;

&lt;p&gt;
Note that the code presented is open sourced and can be found &lt;a href="https://github.com/lazyprogrammer/machine_learning_examples/tree/master/rl"&gt;here&lt;/a&gt;.
&lt;/p&gt;

&lt;p&gt;
Notice that this course follows pretty much the structure and the
content of &lt;i&gt;Reinforcement Learning - An introduction&lt;/i&gt; of (Sutton and
Barto). Check at it if you want to go &lt;a href="http://incompleteideas.net/book/RLbook2018.pdf"&gt;over the book&lt;/a&gt; and if you want
to expand on the section on approximation methods.
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/reinforcement-learning/"&gt;Read more…&lt;/a&gt; (30 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Reinforcement Learning</category><guid>https://marcohassan.github.io/bits-of-experience/posts/reinforcement-learning/</guid><pubDate>Mon, 22 Jun 2020 13:25:17 GMT</pubDate></item><item><title>Ein - Ipython Notebooks in Emacs</title><link>https://marcohassan.github.io/bits-of-experience/posts/ein-ipython-notebooks-in-emacs/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;p&gt;
I recently decided to switch from using &lt;code&gt;ob-ipython&lt;/code&gt; to &lt;code&gt;ein&lt;/code&gt; for
working with the ipython kernel on Emacs. I was quite satisfied with
the first but I noticed it was not maintained anymore, while there
seems to be quite a lot of activity around &lt;code&gt;ein&lt;/code&gt;. I therefore decided
to stay up to date in order to benefit from the development in the
second package.
&lt;/p&gt;

&lt;p&gt;
This post summarizes the major things that makes it possible and the
key workflow to operate through &lt;code&gt;EIN&lt;/code&gt; properly.
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/ein-ipython-notebooks-in-emacs/"&gt;Read more…&lt;/a&gt; (2 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>emacs</category><guid>https://marcohassan.github.io/bits-of-experience/posts/ein-ipython-notebooks-in-emacs/</guid><pubDate>Sun, 21 Jun 2020 19:15:24 GMT</pubDate></item></channel></rss>