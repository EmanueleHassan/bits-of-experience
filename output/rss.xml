<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Bits of Experience</title><link>https://marcohassan.github.io/bits-of-experience/</link><description>A readable view on my studying adventures.</description><atom:link href="https://marcohassan.github.io/bits-of-experience/rss.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2020 &lt;a href="mailto:marco.hassan30@gmail.com"&gt;Marco Hassan&lt;/a&gt; </copyright><lastBuildDate>Tue, 25 Aug 2020 08:43:52 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>On Multicloud Ghosts</title><link>https://marcohassan.github.io/bits-of-experience/posts/on-multicloud-ghosts/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;br&gt;
&lt;br&gt;
&lt;br&gt;

&lt;p&gt;
Funny world. 
&lt;/p&gt;

&lt;p&gt;
I was reading a bit on the multi-cloud developments and the offer of
AWS with JUKE and Microsoft with its Azure Arc.
&lt;/p&gt;

&lt;p&gt;
I already imagine squads of sales people of the companies trying to
sell the two as multi-cloud vendors. I wonder how many people will
really buy it.
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/on-multicloud-ghosts/"&gt;Read more…&lt;/a&gt; (2 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><guid>https://marcohassan.github.io/bits-of-experience/posts/on-multicloud-ghosts/</guid><pubDate>Wed, 12 Aug 2020 10:34:39 GMT</pubDate></item><item><title>A list of interesting Posts On ML techniques</title><link>https://marcohassan.github.io/bits-of-experience/posts/interesting-posts-on-ml/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;p&gt;
This is a list of good material I found on various ML techniques. Some
you might want to explore them deeper in later posts. Some will just
stay as they are here.
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/interesting-posts-on-ml/"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Machine Learning</category><guid>https://marcohassan.github.io/bits-of-experience/posts/interesting-posts-on-ml/</guid><pubDate>Tue, 21 Jul 2020 19:40:24 GMT</pubDate></item><item><title>Indexing Data Structures</title><link>https://marcohassan.github.io/bits-of-experience/posts/indexing-data-structures/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;style&gt;
.container {
  position: relative;
  left: 15%;
  margin-top: 60px;
  margin-bottom: 60px;
  width: 70%;
  overflow: hidden;
  padding-top: 56.25%; /* 16:9 Aspect Ratio */
  display:block;
  overflow-y: hidden;
}

.responsive-iframe {
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  right: 0;
  width: 100%;
  height: 100%;
  border: none;
  display:block;
  overflow-y: hidden;
}
&lt;/style&gt;



&lt;p&gt;
While going deeper into data stores and HBase I got interested in the
indexing mechanism on which the database rely for an optimal
read/write performance. 
&lt;/p&gt;

&lt;p&gt;
I watched some videos that I put on this post. The first one gives a
very good overview of Block Storage and the effectiveness of multi 
-indexing putting at the end the focus on B-trees and B+-trees.
&lt;/p&gt;

&lt;p&gt;
The second one briefly goes Bloom Filters, it is not as good as the
first but it makes the point for Bloom Filter without going too much
in the nitty-gritty. These are very interesting data structure that is
widely used in different fields - from fraud detection to check
whether a transaction is unlikely to have been performed by the legal
owner; to databases for skipping a query all-together if the bloom
filter gives negative output (see for instance in Hbase where a query
given the size of the table might be especially expensive) etc. 
&lt;/p&gt;

&lt;p&gt;
The third does not discuss any data structure and assumes on top of it
that you are comfortable with the topic of log structured tree and
merge-sort algorithms. It goes in to the research field and analyzes
possible optimizations for write intensive key-value stores, which as
seen in the Big Data class are by now possibly one of the most
important store class.
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/indexing-data-structures/"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Data Structures</category><guid>https://marcohassan.github.io/bits-of-experience/posts/indexing-data-structures/</guid><pubDate>Sat, 18 Jul 2020 08:54:22 GMT</pubDate></item><item><title>On Scalability</title><link>https://marcohassan.github.io/bits-of-experience/posts/on-scalability/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;p&gt;
This post quickly examines two different laws of scalability: Amdahl's
law and Gustafson's law.
&lt;/p&gt;

&lt;p&gt;
Both laws addresses the extent to which a problem can be speed up
through scaling out. 
&lt;/p&gt;

&lt;p&gt;
The premise of setting under which the laws operate is
different. Amdahl's law assumes a &lt;b&gt;fix problem size&lt;/b&gt;, while
Gustafson's law assumes an &lt;b&gt;an expanding problem size in the number of
machines scaling out&lt;/b&gt;.
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/on-scalability/"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>IT Architecture</category><guid>https://marcohassan.github.io/bits-of-experience/posts/on-scalability/</guid><pubDate>Fri, 17 Jul 2020 06:43:49 GMT</pubDate></item><item><title>Spark UI</title><link>https://marcohassan.github.io/bits-of-experience/posts/spark-ui/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;style&gt;
img {
display: block;
margin-left: auto;
margin-right: auto;
margin-top: 60px;
margin-bottom: 60px;
}
&lt;/style&gt;




&lt;p&gt;
This post is a reminder for me to use Spark UI when working as it
greatly improves debugging and allows you to better understand of
Spark.
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/spark-ui/"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Spark</category><guid>https://marcohassan.github.io/bits-of-experience/posts/spark-ui/</guid><pubDate>Thu, 16 Jul 2020 15:31:07 GMT</pubDate></item><item><title>Python Pipelines</title><link>https://marcohassan.github.io/bits-of-experience/posts/python-pipelines/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;br&gt;

&lt;p&gt;
Pipelines are important when working on ML projects in python. They
first appeared through the implementation of ML tasks sklearn. There
the design setting of the maintainer of the library was to design a
set of APIs through which to govern the entire ML process. The three
major class of APIs are &lt;code&gt;tranformers&lt;/code&gt;, i.e. transformations to the
feature and dependent variables, &lt;code&gt;estimators&lt;/code&gt;, i.e. the ML model that
you fit on your transformed features matrix, and finally &lt;code&gt;predictors&lt;/code&gt;,
i.e. the API method used for predicting your desired variable based on
your fitted estimator.
&lt;/p&gt;

&lt;p&gt;
The above logic became so widespread that further important
libraries - such as the spark ML modules for using ML models
leveraging the spark engine for distributed computation - decided to
keep the same design setting and keep the above API structure.
&lt;/p&gt;

&lt;p&gt;
This posts aims at keeping an overview of some important transformers,
estimators and predictors as well as to set up the general framework
to combine the three together in easy to use Pipelines. 
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/python-pipelines/"&gt;Read more…&lt;/a&gt; (6 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Machine Learning</category><category>Python</category><guid>https://marcohassan.github.io/bits-of-experience/posts/python-pipelines/</guid><pubDate>Thu, 02 Jul 2020 10:44:32 GMT</pubDate></item><item><title>On the Architecture of Query Engines - Batch vs Streaming</title><link>https://marcohassan.github.io/bits-of-experience/posts/on-the-architecture-of-query-engines-batch-vs-streaming/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;p&gt;
In the previous post we addressed the Jsoniq declarative and
functional language for querying data in tree shape. It was mentioned
that due to data independence it is possible to run such Java Library
in combination to many Big Data frameworks and in general in
combination with the Hadoop framework - MapReduce, Spark, HDFS etc -.
&lt;/p&gt;

&lt;p&gt;
This post addresses the importance of understanding how a query engine
operates despite the data independence. 
&lt;/p&gt;

&lt;p&gt;
I.e. you might well be able to run and retrieve your desired data
without understanding how different engines work under the hood;
however you might then not write meaningful queries or run into issue
if your hardware infrastructure is not suited to fit your desired data
size and query engine operations.
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/on-the-architecture-of-query-engines-batch-vs-streaming/"&gt;Read more…&lt;/a&gt; (4 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Big Data</category><guid>https://marcohassan.github.io/bits-of-experience/posts/on-the-architecture-of-query-engines-batch-vs-streaming/</guid><pubDate>Sat, 27 Jun 2020 09:46:43 GMT</pubDate></item><item><title>On Data Idependence - Querying Trees. </title><link>https://marcohassan.github.io/bits-of-experience/posts/on-data-idependence-querying-trees/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;style&gt;
img {
display: block;
margin-top: 60px;
margin-bottom: 60px;
margin-left: auto;
margin-right: auto;
}
&lt;/style&gt;

&lt;p&gt;
So far we introduced data objects in their tree format. We saw that
being HDFS agnostic you can save data in their tree shape - be it
&lt;code&gt;JSON&lt;/code&gt; or &lt;code&gt;XML&lt;/code&gt; - in &lt;code&gt;HDFS&lt;/code&gt;. We see how it was possible to operate on
such tree data directly via the pyspark RDD low level API. Moreover,
we briefly tackled the idea of compressing the information in the tree
data model into dataframes and on how to handle these via &lt;code&gt;sparkSQL&lt;/code&gt;
with its &lt;a href="https://marcohassan.github.io/bits-of-experience/posts/spark-session-initalization/"&gt;explode operators&lt;/a&gt;.
&lt;/p&gt;

&lt;p&gt;
Finally, we addressed document stores, &lt;code&gt;NoSQL&lt;/code&gt; databases that are
especially designed to deal with data in their tree shape. We saw the
extent to which document stores rely on indices and the consequent
benefit over HDFS when it comes to find indexed data as no scan over
the entire dataset has to be executed.
&lt;/p&gt;

&lt;p&gt;
Given the interesting case for document store but the limited
scalability the question is on whether we could leverage data
independence for using a &lt;b&gt;declarative language&lt;/b&gt; in a similar spirit of
the &lt;b&gt;SQL&lt;/b&gt; that is appositely created to deal with tree-modeled data. 
&lt;/p&gt;

&lt;p&gt;
This posts confronts with the above case and introduces a &lt;b&gt;Jsoniq&lt;/b&gt; one
of the many languages that are emerging in this Big Data decade to
deal with tree-modeled data in a declarative way. This will allow to
deal with tree-data without having to use the low-level RDD API or the
approximative mapping of the tree structure into dataframes.
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/on-data-idependence-querying-trees/"&gt;Read more…&lt;/a&gt; (3 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Big Data</category><guid>https://marcohassan.github.io/bits-of-experience/posts/on-data-idependence-querying-trees/</guid><pubDate>Sat, 27 Jun 2020 07:23:56 GMT</pubDate></item><item><title>Document Stores</title><link>https://marcohassan.github.io/bits-of-experience/posts/document-stores/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;style&gt;
img{
  display: block;
  margin-left: auto;
  margin-right: auto;
}
&lt;/style&gt;


&lt;p&gt;
This post briefly goes over the idea of document stores. These are
databases that were appositely creating for dealing with data in its
tree shape. Some act on top of &lt;code&gt;json&lt;/code&gt; documents some on top of &lt;code&gt;XML&lt;/code&gt;.
&lt;/p&gt;

&lt;p&gt;
The idea is that albeit you might find some tweaks for working with
tree data shapes (think for instance at the &lt;code&gt;EXPLODE&lt;/code&gt; function of
sparkSQL) you will always have difficulties in storing a tree data
model into a relational table.
&lt;/p&gt;

&lt;p&gt;
This is mainly due to the two properties of trees that are not
available in relational tables being:
&lt;/p&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;heterogeneity&lt;/li&gt;

&lt;li&gt;nestedness&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
In this sense document stores are schemaless. You might save the
document however you want and validate them ex-post.
&lt;/p&gt;

&lt;p&gt;
To overcome data impedance of storing tree modeled data into
relational tables new databases are emerging that allows to store and
query directly json documents. However, notice that such databases are
at their infancy and to this stage they don't have the scaling force
of HDFS. I.e. you they scale just up to handful of machines.
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/document-stores/"&gt;Read more…&lt;/a&gt; (9 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Big Data</category><guid>https://marcohassan.github.io/bits-of-experience/posts/document-stores/</guid><pubDate>Fri, 26 Jun 2020 15:46:36 GMT</pubDate></item><item><title>Wide Column Store - HBase</title><link>https://marcohassan.github.io/bits-of-experience/posts/wide-column-store-hbase/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;style&gt;
img {
  display: block;
  margin-left: auto;
  margin-right: auto;
}
&lt;/style&gt;


&lt;p&gt;
The idea of wide column store is to extend the idea of RDBMS,
i.e. storing and managing data in tabular format, and to bring it into
the BigData world. 
&lt;/p&gt;

&lt;p&gt;
This post will briefly address the different characteristics of
Hbase. I.e. it will first look at the logical idea for keeping data in
a tabular form in the big data world. It will then look at the
architecture behind Hbase and as you will see the basis for it is the
good old &lt;i&gt;master-slave&lt;/i&gt; in parallel to HDFS that will be here used as
the storage layer.
&lt;/p&gt;

&lt;p&gt;
Finally it will briefly address the physical storage layer.
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/wide-column-store-hbase/"&gt;Read more…&lt;/a&gt; (11 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Big Data</category><guid>https://marcohassan.github.io/bits-of-experience/posts/wide-column-store-hbase/</guid><pubDate>Fri, 26 Jun 2020 10:55:39 GMT</pubDate></item></channel></rss>