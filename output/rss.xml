<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Bits of Experience</title><link>https://marcohassan.github.io/bits-of-experience/</link><description>A readable view on my studying adventures.</description><atom:link href="https://marcohassan.github.io/bits-of-experience/rss.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2019 &lt;a href="mailto:marco.hassan30@gmail.com"&gt;Marco Hassan&lt;/a&gt; </copyright><lastBuildDate>Sat, 07 Sep 2019 09:28:00 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Apache Spark SQL</title><link>https://marcohassan.github.io/bits-of-experience/posts/Apache%20Spark%20SQL/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;p&gt;
This posts makes the point for Apache Spark SQL. 
&lt;/p&gt;

&lt;p&gt;
Using RDDs API might be quite annoying, especially if you are used to
the industry standard of RDMS and their SQL sytax.
&lt;/p&gt;

&lt;p&gt;
Here ApacheSparkSQL kicks in, providing a SQL interface to your data.
&lt;/p&gt;



&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/Apache%20Spark%20SQL/"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Big Data</category><guid>https://marcohassan.github.io/bits-of-experience/posts/Apache%20Spark%20SQL/</guid><pubDate>Fri, 06 Sep 2019 16:21:43 GMT</pubDate></item><item><title>Kubernetes</title><link>https://marcohassan.github.io/bits-of-experience/posts/Kubernetes/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;p&gt;
A strong orchestrator tool that operates above the container level and
allows to manage a cluster to handle containers.
&lt;/p&gt;

&lt;p&gt;
It is essentially the toolset that lets you manage containers.
&lt;/p&gt;

&lt;p&gt;
Enterprises can use it to manage the lifecycle of containerized apps
in a cluster of nodes, which is a collection of worker machines such
as virtual machines (VMs) or physical machines.
&lt;/p&gt;

&lt;p&gt;
In general kubernetes try to leverage clusters in order to avoid
having a single point of failure.
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/Kubernetes/"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>IT Architecture</category><guid>https://marcohassan.github.io/bits-of-experience/posts/Kubernetes/</guid><pubDate>Fri, 06 Sep 2019 16:21:43 GMT</pubDate></item><item><title>Docker</title><link>https://marcohassan.github.io/bits-of-experience/posts/Docker/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;br&gt;
&lt;br&gt;

&lt;p&gt;
The basic idea of Docker is to allow the possibility to save all of
the configuration of an application in one single image. This should
be considered as a safe environment that once properly set up can be
easily shared among different teams and once images are instantiated
all of the different teams can be sure to operate and leverage the
right configuration for running their application.
&lt;/p&gt;

&lt;p&gt;
A simple and straight forward overview about the advantage of Docker
might be found in this sense at &lt;a href="https://www.tutorialspoint.com/docker/docker_architecture.htm"&gt;docker architecture&lt;/a&gt;.
&lt;/p&gt;

&lt;p&gt;
To sum up before starting the basic idea is to create images and to
create containers based on that, which will run then the application
as defined in the docker image.
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/Docker/"&gt;Read more…&lt;/a&gt; (2 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>IT Architecture</category><guid>https://marcohassan.github.io/bits-of-experience/posts/Docker/</guid><pubDate>Mon, 02 Sep 2019 16:21:43 GMT</pubDate></item><item><title>Spark Session Initialization, RDD: Transformations and Actions</title><link>https://marcohassan.github.io/bits-of-experience/posts/rdd-transformations-and-actions/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;br&gt;
&lt;br&gt;


&lt;p&gt;
This second post present the basic set up of a Spark session and goes
over the basic transformations and actions that applies to Spark
RDDs. These are necessary given the immutability of RDDs.
&lt;/p&gt;

&lt;p&gt;
RDDs are saved in-memory by default. Nonetheless when the memory of
the machine is not sufficient to handle the data, RDDs are splitted to
disk.
&lt;/p&gt;

&lt;p&gt;
Finally, RDDs are lazy. This, means that only if the data is needed
for a certain computation the data is read from the underlying storage
system.
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/rdd-transformations-and-actions/"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Big Data</category><category>Spark</category><guid>https://marcohassan.github.io/bits-of-experience/posts/rdd-transformations-and-actions/</guid><pubDate>Wed, 21 Aug 2019 21:31:02 GMT</pubDate></item><item><title>Paredit</title><link>https://marcohassan.github.io/bits-of-experience/posts/paredit/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;p&gt;
Paredit is a nice tool to quickly deal with parenthesis when
coding. Thorough simple commands it lets you easily edit content
wrapped in parenthesis, move quickly the parenthesis themselves 
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/paredit/"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>emacs</category><category>parenthesis</category><guid>https://marcohassan.github.io/bits-of-experience/posts/paredit/</guid><pubDate>Sat, 17 Aug 2019 16:21:43 GMT</pubDate></item><item><title>PySpark Set-Up and Integration with Emacs</title><link>https://marcohassan.github.io/bits-of-experience/posts/pyspark-set-up/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;br&gt;
&lt;br&gt;

&lt;p&gt;
This post aims at providing an overview of the necessary steps
required in order to leverage Apache Spark through the Python API and
the PySpark module.
&lt;/p&gt;

&lt;p&gt;
Despite the information is vastly reported over the internet my usual
&lt;i&gt;procedere&lt;/i&gt; when I deal with new tools and software is to write a
short piece of note when learning new software that is intended as an
overview over the tool and serve as a beginner cheat sheet.
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/pyspark-set-up/"&gt;Read more…&lt;/a&gt; (4 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Big Data</category><category>emacs</category><category>Spark</category><guid>https://marcohassan.github.io/bits-of-experience/posts/pyspark-set-up/</guid><pubDate>Mon, 05 Aug 2019 21:51:11 GMT</pubDate></item></channel></rss>