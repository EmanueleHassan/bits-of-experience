<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Bits of Experience (Posts about Big Data)</title><link>https://marcohassan.github.io/bits-of-experience/</link><description></description><atom:link href="https://marcohassan.github.io/bits-of-experience/categories/big-data.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2020 &lt;a href="mailto:marco.hassan30@gmail.com"&gt;Marco Hassan&lt;/a&gt; </copyright><lastBuildDate>Wed, 29 Jan 2020 15:56:15 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Apache Spark SQL</title><link>https://marcohassan.github.io/bits-of-experience/posts/Apache%20Spark%20SQL/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;br&gt;
&lt;br&gt;

&lt;p&gt;
This posts makes the point for Apache Spark SQL. 
&lt;/p&gt;

&lt;p&gt;
Using RDDs API might be quite annoying, especially if you are used to
the industry standard of RDMS and their SQL sytax.
&lt;/p&gt;

&lt;p&gt;
Here ApacheSparkSQL kicks in, providing a SQL interface to your data.
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/Apache%20Spark%20SQL/"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Big Data</category><category>Spark</category><guid>https://marcohassan.github.io/bits-of-experience/posts/Apache%20Spark%20SQL/</guid><pubDate>Fri, 06 Sep 2019 16:21:43 GMT</pubDate></item><item><title>Spark Session Initialization, RDD: Transformations and Actions</title><link>https://marcohassan.github.io/bits-of-experience/posts/rdd-transformations-and-actions/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;br&gt;
&lt;br&gt;


&lt;p&gt;
This second post present the basic set up of a Spark session and goes
over the basic transformations and actions that applies to Spark
RDDs. These are necessary given the immutability of RDDs.
&lt;/p&gt;

&lt;p&gt;
RDDs are saved in-memory by default. Nonetheless when the memory of
the machine is not sufficient to handle the data, RDDs are splitted to
disk.
&lt;/p&gt;

&lt;p&gt;
Finally, RDDs are lazy. This, means that only if the data is needed
for a certain computation the data is read from the underlying storage
system.
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/rdd-transformations-and-actions/"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Big Data</category><category>Spark</category><guid>https://marcohassan.github.io/bits-of-experience/posts/rdd-transformations-and-actions/</guid><pubDate>Wed, 21 Aug 2019 21:31:02 GMT</pubDate></item><item><title>PySpark Set-Up and Integration with Emacs</title><link>https://marcohassan.github.io/bits-of-experience/posts/pyspark-set-up/</link><dc:creator>Marco Hassan</dc:creator><description>&lt;div&gt;&lt;br&gt;
&lt;br&gt;

&lt;p&gt;
This post aims at providing an overview of the necessary steps
required in order to leverage Apache Spark through the Python API and
the PySpark module.
&lt;/p&gt;

&lt;p&gt;
Despite the information is vastly reported over the internet my usual
&lt;i&gt;procedere&lt;/i&gt; when I deal with new tools and software is to write a
short piece of note when learning new software that is intended as an
overview over the tool and serve as a beginner cheat sheet.
&lt;/p&gt;

&lt;p&gt;&lt;a href="https://marcohassan.github.io/bits-of-experience/posts/pyspark-set-up/"&gt;Read more…&lt;/a&gt; (4 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Big Data</category><category>emacs</category><category>Spark</category><guid>https://marcohassan.github.io/bits-of-experience/posts/pyspark-set-up/</guid><pubDate>Mon, 05 Aug 2019 21:51:11 GMT</pubDate></item></channel></rss>