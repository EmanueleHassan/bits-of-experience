#+BEGIN_COMMENT
.. title: Spark UI
.. slug: spark-ui
.. date: 2020-07-16 17:31:07 UTC+02:00
.. tags: Spark
.. category: 
.. link: 
.. description: 
.. type: text

#+END_COMMENT

#+begin_export html
<style>
img {
display: block;
margin-left: auto;
margin-right: auto;
margin-top: 60px;
margin-bottom: 60px;
}
</style>
#+end_export




This post is a reminder for me to use Spark UI when working as it
greatly improves debugging and allows you to better understand of
Spark.

{{{TEASER_END}}}

In fact Spark UI is possibly the centerpiece when working with Spark
it you want to truly understand it.

It helps to understand how many jobs, stages etc. are created out of
your application code. It can help you to visualize the DAG for your
code and this might even help you in assessing if RDD should be
cached. It can help you to visualize how spark SQL is
converted in an execution plan and its corresponding DAG. Finally it
will help you to have an overview of what executors are connected to
the Spark driver, how many cores and memory is available on each and
generally get a glimpse on your spark cluster infrastructure.

This is important for bringing the level of Spark understanding to a
higher level. One thing is to use Spark, one thing is to understand
its processing engine and operations and this is a good place to start
with.

To get a feeling of the Spark-UI check the following image.

#+begin_export html
 <img width="70%" height="100%" src="../../images/Bildschirmfoto_2020-07-16_um_17.29.19.png" class="center">
#+end_export

I noticed that it is even possible to integrate some of the dashboards
available in the Spark-UI in jupyter notebooks. This was for instance
the default option for Azure Jupyter Notebooks. Recently I discovered
this is a Jupyter Notebook Integration written in Javascript called
=SparkMonitor=. You find some documentation for it [[https://krishnan-r.github.io/sparkmonitor/how.html][here]].

I found it useful. Nonetheless due to my preference to code on Emacs
rather than through browsers I prefer to keep the coding and the Spark
Dashboard splitted. In this sense I prefer to access the Spark UI via
browser. 

As a last remark, notice that the default port where your Spark-UI is
running is =4040=. This is particularly useful when running Spark via
the =findspark= tool on the local PC making use of all of your virtual
cores. Notice that when launching spark from the shell as in the most
standard application there should be written where exactly the
Spark-UI is running.








