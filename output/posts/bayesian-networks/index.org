#+BEGIN_COMMENT
.. title: Bayesian Networks
.. slug: bayesian-networks
.. date: 2021-02-15 11:23:13 UTC+01:00
.. tags: Bayesian Networks
.. category: 
.. link: 
.. description: 
.. type: text
.. has_math: yes
#+END_COMMENT

#+begin_export html
<style>
img {
display: block;
margin-top: 60px;
margin-bottom: 60px;
margin-left: auto;
margin-right: auto;
width: 70%;
height: 100%;
class: center;
}

.container {
  position: relative;
  left: 15%;
  margin-top: 60px;
  margin-bottom: 60px;
  width: 70%;
  overflow: hidden;
  padding-top: 56.25%; /* 16:9 Aspect Ratio */
  display:block;
  overflow-y: hidden;
}

.responsive-iframe {
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  right: 0;
  width: 100%;
  height: 100%;
  border: none;
  display:block;
  overflow-y: hidden;
}
</style>
#+end_export

Here are some Notes about the topic of my Master Thesis - Bayesian
Networks.

Note that most of these Notes are based on /Probabilistic Graphical
Models - Principles and Techniques/ ([[https://www.amazon.de/Probabilistic-Graphical-Models-Principles-Computation/dp/0262013193][Koller and Friedman]]). The book is
outstanding and would recommend it. Note that here all of the pictures
you have inside stem from this book. Read them as a word by word
quotation. 

{{{TEASER_END}}}

* Bayesian Networks
  :LOGBOOK:
  CLOCK: [2021-02-15 Mon 14:27]--[2021-02-15 Mon 14:52] =>  0:25
  CLOCK: [2021-02-15 Mon 12:50]--[2021-02-15 Mon 13:15] =>  0:25
  CLOCK: [2021-02-15 Mon 11:27]--[2021-02-15 Mon 11:52] =>  0:25
  :END:
  
The general outlook.

So recall that in general you have three elements in Bayesian
Networks:

- Representation

  - how do you represent the joint probability of the events as a
    network (i.e. as a graph data structure)? Can such structure
    represent the joint in a compact way due to the conditional
    independence relations?

    *Note 1:* that such compact formulation is one of the key benefits of
    Bayesian Networks as it really gives the possibility of shrinking
    the amount of parameters needed to describe the full joint
    probability leveraging the independence structure among the RVs.

    *Note 2:* this formulation is /transparent/, i.e. highly
    understandable also to non-AI experts. It is so to say highly
    explainable and in this new buzz of /explainable AI/ a solid
    option.
  
- Inference

  - given some information about some Parent variables, how can I
    infer/compute the distribution of the children in the Network?
  
- Learning

  - given some observed data, how can I use such information to
    construct / (infer) / learn the  /structure/ of the network?

  - given some observed data, how can I learn the /parameters/ of the
    network? I.e. how can I use the information content of the data to
    derive some plausible parameterization of the network.


So these are the main tasks you have to deal with in Bayesian
Networks. Basically you can do all of the three in a very simple way,
which is from a theoretical standpoint very concrete and
straightforward or you can start to consider all the aspects of the
problem going quickly towards more complex situations.

** Representation

   #+BEGIN_EXPORT html
   <br>
   <br>
   #+END_EXPORT

   #+begin_src plantuml :file ~/Desktop/Blog/images/bayesNet1.svg :exports none
   @startuml
   circle A
   circle B
   circle C
   circle D


   A --> B
   A --> C

   B --> D
   C --> D
   @enduml
   #+end_src

   #+RESULTS:
   [[file:~/Desktop/Blog/images/bayesNet1.svg]]

   #+begin_export html
<style>
.bg-svg {
  width: 40%;
  background-image: url(../../images/bayesNet1.svg);
  background-size: cover;
  height: 0;
  padding: 0; /* reset */
  padding-bottom: 92%;
  border: thin dotted darkgrey;
  float:right;
  margin-left: 5%;
}
.content p{
    display: block;
    margin: 2px 0 0 0;
}
</style>

<div style="width: 100%">
   <div style= "width: 70%; margin-left = 5%;">
      <div class="bg-svg">
   </div>
   <p>

   <br/>
   <br/>

   As mentioned bayesian networks allow us to express the joint
   through less parameters.

   <br/>
   <br/>

   The idea is that you factorize the joint as a product of the
   conditionals and given the parameterization of the conditionals you
   fully specify the joint. Given the independence structures the
   number of factorization of conditional terms is limited and the
   overall necessary parameters to specify the joint small.

   <br/>
   <br/>
   
   For instance if a Variable D is fully determined by its parents B,
   C in this graph:

   <br/>   
   <br/>
   
   Then you might well understand that given B, C you do not need
   P(D | A, B, C) parameters as P(D | B, C) suffices.
  </p>
  <br style="clear: both;" />
</div>
    #+end_export

   #+BEGIN_EXPORT html
   <br>
   <br>
   <br>
   <br>   
   #+END_EXPORT
    
   A concrete example is the following:

#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-02-15_um_13.21.25.png">
#+end_export

   Notice there that instead of needing 2 (Diff) * 2 (Int) * 3
   (Grade) * 2 (Sat) * 2 (Let) = 48 parameters to describe the joint
   you simply need 2 + 2 + 12 + 6 = 22.

   Given this understanding it is immediate to see that Bayesian
   Networks are defined as above, i.e. as a graph data structure to
   which /local probabilities/ are applied. In the specific each RV in
   the graph is associated with /conditional probability distributions
   (CPD)/ that specify the distribution given each possible joint
   assignment of values to its parents. And the graph structure
   together with the CPD specifies the Bayesian Network.

   A *second* representation/ definition of Bayesian Networks is to
   define it via a /global probability P/ together with the independence
   relations determined by the graph.

   To determine independence relations in graphs you can use standard
   logic where the argument is essentially the following:

   #+begin_quote
    Our intuition tells us that the parents of a variable “shield” it
    from probabilistic influence that is causal in nature. In other
    words, once I know the value of the parents, no information
    relating directly or indirectly to its parents or other ancestors
    can influence my beliefs about it. However, information about its
    descendants can change my beliefs about it, via an evidential
    reasoning process. (Koller and Friedman)
   #+end_quote

   Such that you would have the following /local independence
   structures/:

   $$ For each variable X_i : (X_i \perp NonDescendants X_i | Parents
   X_i) $$

   Notice that such set of independence is called an I-map for a
   probability distribution /P/. You then say that a graph /G/ is an
   I-map for /P/ if it satisfies the I-map relations specified /I(P)/.

   And you would ultimately have the following definition:

#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-02-15_um_14.50.38.png">
#+end_export

   So that you basically take here the opposite direction, from a
   joint distribution /P/ and the local independence structure you
   have a fully specified Bayesian Network.

   *Note* that you can go from one representation to the other and the
   BN is defined *if and only if* you can from one to the other.

*** On Graph Dependencies and D-separation

    Given the above discussion and the fact that it is possible to
    determine the BN given a joint density and a Graph structure, the
    question now is on how to extract the conditional independence
    structures implied by a graph, i.e. to extract the I-map
    relations.

    In order to do that a simple algorithm exists the /d-separation
    algorithm/.

    The idea here is the following. You know that for three nodes X,
    Y, Z there exists a dependence structure between X and Y if one of
    the following conditions *hold*:

#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-02-15_um_15.18.31.png">
#+end_export

     This is quite intuitive.

     It follows now that we can quickly assess whether two variables
     are generally conditionally independent by making reasonings
     leveraging the active trails as above.

     I.e. for two variables to be *dependent* there must be an active
     trail as defined by the conditions above.

     Notice that for instance in the student BN you can investigate
     the conditional independence between SAT and Difficulty as
     follows:

#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-02-15_um_15.25.30.png">
#+end_export


     Generally it holds:

#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-02-15_um_15.26.42.png">
#+end_export

     You can then find in the book an algorithm for checking
     d-separation, if interested at any point in time. Notice that
     there is are also reasonings about /completeness/ and /soundness/
     of d-separation. I.e. how well that covers and fully specifies
     independence structures of /P/.

     I write in here the definition of /completeness/ and /soundness/
     should it be of interest at any point at a later stage:

     /Soundness/:

     If two nodes X and Y are d-separated given some Z, then we are
     guaranteed that they are, conditionally independent given Z.

     /Completeness/:

     D-separation is complete if it detects all of the possible
     independencies. I.e. if two variables X and Y are independent
     given Z, then they are d-separated.

     Formally:

#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-02-19_um_09.32.33.png">
#+end_export

*** On CPD

    So far we discussed the possibility of representing the
    high-dimensional joint distribution into a product of
    lower-dimensional CPDs or factors, i.e. a product of local
    probabilities models.

    In this section we explore more into the detail the possibility of
    representing such CPDs.

    
**** Tabular CPD

     This is the most basics form of CPD. It works for spaces composed
     solely of *discrete* valued RV.

     It consists in expressing the $P(X | PA_X)$ as a table that
     contains the joint probability of $X \and PA_X$.

     This is essentially what was given in the example above.

     *Note:* it is important to realize that the number of joint
     probabilities that you have to express is given by

     $$|Val(PA_X)| * |Val(X)|$$

     I.e. it grows /exponentially/ in the number of parents. This is a
     serious problem in many settings. You can also not ask an expert
     to express all such CPDs. He will loose patient at some point.

     So the idea is to find a mechanism to express each and every
     $P(X | PA_X)$ for each X and $PA_X$ but without doing the
     exercise explicitly.

     I.e. you should find a /functional formula CPD = f(X, PA_X)/ such
     that you can leverage some structures represented by the
     functional formula and do not have to express all of the
     probabilities individually.

     You can then read in the book some forms of such deterministic
     CPDs. The general idea is quite simple. There might be
     deterministic structures that naturally arise due to the
     structure of the modeled phenomena.

     Moreover for deterministic networks you might have the notion of
     =context specific independence=. Here the idea is that given some
     particular configuration $X \cup Y \cup Z$ you might have
     independence of X and Y given Z in this particular configuration.

     
**** Context Specific CPDs for non-deterministic dependecies

     Structure in CPDs might not just arise in the case of
     context-specific CPDs.

     The idea is that often there is some structure such that for
     certain realizations a RV X given some partial assignment to some
     subset of parents $ U \subset PA_X$ the probability is fully
     specified and does not depend on the remaining parents.

     Two ways to capture such structure is through Tree-CPDs and
     rule-based CPDs.

***** Tree-CPDs

      This is a very intuitive structure for every human. In fact
      trees are used continuously. There is a natural tendencies for
      such structures in engineering so nothing new. You saw them 100s
      time.

      However, what is interesting is the example. In fact it is easy
      to see that by leveraging the tree structure, i.e. the context
      specific structure and the resulting independencies you can
      highly reduce the total number of parameters.

      To understand that think of the following example:

      #+begin_export html
       <img src="../../images/Bildschirmfoto_2021-02-19_um_12.01.18.png" class="center">
      #+end_export

      It is immediate then to see that the above highly reduces the
      number of parameters.

      #+begin_export html
       <img src="../../images/Bildschirmfoto_2021-02-19_um_12.03.41.png" class="center">
      #+end_export


      Notice that when we talk we say that the Tree-CPDs represent the
      network context specific information. This is immediate to see
      as you do not in fact consider the full structure of the network, but
      you already factor out some of the independencies.

      To see that consider, the following case where you would have
      two recommendation letters and are applying for a job. You have
      to choose among the two. Then you can represent the case in the
      following ways:

      #+begin_export html
       <img src="../../images/Bildschirmfoto_2021-02-20_um_19.08.25.png" class="center">
      #+end_export

      It is clear that on the left you work at the network structure
      not leveraging context specific information while on (b) you
      already start to pack that in.

***** Rule-based

      Another possibility to pack information of the network structure
      by leveraging context specific information is via =rule-CPDs=.

      They are defined in the following way:
      
#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-02-20_um_19.31.28.png" class="center">
#+end_export


#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-02-20_um_19.50.22.png" class="center">
#+end_export

      It follows immediately that it basically consists in sets joint
      co-occurrences of RV and assigns probabilities to such cases.

      With it you can then basically express all sorts of CPDs
      structures that are based on some partitioning.

      It is in fact immediate to see that tree-CPDs can be easily
      expressed via rule-based CPDs but the converse is not true.      

**** Independence of Causal Influence

     Here the idea is the case where you have a set of variables X_i
     influencing Y, such that X_i can influence Y in an arbitrary
     way. I.e. you assume that X_i can interact with each other in
     complex ways making the *effect of each combination unrelated to
     any other possible combination*.

     Two such models that fulfill such characteristics are

      - the noisy-or model

      - the generalized linear models.

***** Noisy Or Model

      This is a very simple model. If an event occurs then you have no
      100% guarantee that the usual reaction will occur. That is there
      is some noise in the model and some side reaction might happen.

      Think for instance at working hard at work. Then with 90% you
      might have a successful project. However, due to some random
      factor, say sudden cut of budget or company restructuring, your
      project might fail. This is the /noisy part/ and the noisy or
      model.

      This is the general setting. It is then possible to express such
      a noisy model through a graphical representation.

      Think of the following:

   #+begin_src plantuml :file ~/Desktop/Blog/images/bayesNet2.png :exports none
   @startuml
   circle W
   circle W_1
   circle S

   W -right-> W_1
   W_1 -right-> S
   @enduml
   #+end_src

   #+RESULTS:
   [[file:~/Desktop/Blog/images/bayesNet2.png]]

#+begin_export html
 <img src="../../images/bayesNet2.png"  style = "width: 30% !important;">
#+end_export

      It follows then that W_1 expresses the probability of the noisy
      factor taking place - i.e. budget restriction. Such that
      \lambda_W = P(W_1 | W) = 0.9. Where W = work hard and W_1 = normal condition.
      Notice now, the case where independently on your hard work the
      team mate hard work also affects the result. Then you could be
      in a situation as the following
      

   #+begin_src plantuml :file ~/Desktop/Blog/images/bayesNet3.png :exports none
   @startuml
   circle W
   circle W_1
   circle TW
   circle TW_1

   circle S

   W --> W_1
   TW --> TW_1
   W_1 --> S
   TW_1 --> S
   @enduml
   #+end_src

   #+RESULTS:

#+begin_export html
<div  style ="height: 40%; width: 50%; margin:0 auto;">
   <img src="../../images/bayesNet3.png">
</div>
#+end_export

      Again also the TW hard work induces a probability of success of
      95%, i.e. \lambda__TW = P(TW_1 | TW) = 95%, and there is a 5%
      prob of failure due to restructuring and budget cut.

      This is essentially the Noisy-or model. You have a deterministic
      or relation influencing the project success - i.e. either your
      work or your team members work. You have noise, i.e. despite the
      factors you might have project failures due to some
      unpredictable conditions - noise. Overall the probability of
      success is given by products of lambdas. I.e. if both team work
      and individual work multiply both lambdas. If just one, then
      take the respective lambda etc.

      More formally such model is defined as:
      
#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-02-21_um_09.48.03.png" class="center">
#+end_export

      Notice that the /leak probability/ was not discussed that
      far. It consists of the probability of project success even in
      the case that no hard work - for neither myself nor the team
      members was put in the project.

      *Note* that in such a models the parameters would be represented
      by the estimation of the /different lambdas/.

***** Generalized Linear Models

      These are networks where the interaction among the variables is
      represented by generalized linear models you saw a couple of
      times in your studies.

      Recall that in generalized linear models you would have a linear
      model

      $$ f(X_1, ..., X_p) = \sum_{i}^{p} w_i * X_i  $$

      That would represent the load that the parents sets on the
      system. Where the load of each individual variable might be
      higher or lower and is therefore weighted.

      Then basically you would transform such a load into a
      probability by applying a sensible transformation that could
      well reflect the system work. I.e. a very wide used example is
      the S-shaped structure that can be modeled via logit or probit
      models.

      You can also start to make inference on what happens
      if... cases. For instance in the book it is discussed on how, in
      the case of a binary model, the log-odd probability changes
      w.r.t. a change in one of the independent binary RV. This gives
      you an idea of some possible structures and relations that could
      occur in such models so that if representative of some real
      world situation you can leverage on this.

      *Note* that here once the transformation is defined the only
      parameters left are the weights/loads entering the linear part
      of the model. You should therefore specify these under this
      setting.

      
**** Continuous Variables

     These are not discussed here. Have to move on. The idea is always
     the same. You now have some continuous variables, say Y and
     X. You would then have for instance a relation governed by a
     normal distribution where $Y \sim N( \beta * X, \sigma^2)$.

     That would actually be the case when

     $$ Y = \beta_0 + \sum_{i = 1}^{P} \beta_i X_i + \epsilon $$

     where \epsilon is gaussian N(0, \sigma^2). So again the usual
     stuff.

**** Hybrid Models

     Here the basic idea is that you have a network where you have a
     mixture of continuous and discrete variables affecting other
     variables.

     Then one possibility to model such hybrid situation is the
     following

     
#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-02-21_um_10.34.49.png" class="center">
#+end_export

    Notice that such CLG model induces a mixture on the continuous
    parents Y. Moreover it does not allow to have /discrete
    children/. Notice moreover that the number of parameters here is
    exponential in the number of discrete variables.

    Another possibility to model hybrid models is via threshold
    models, where you would easily go from continuous parents to
    discrete children.

    Notice that these are just very basic possibilities and the idea -
    both here and in the book I guess - is to start to make you reason
    about how to model such situations. The possibilities are however
    uncountable and therefore it is up to you then on a project to
    spend some time at the beginning to engineer the entire model and
    decide on the setting.

    
*** On Conditional Bayesian Networks

    Recall that no matter the CPD definition resulting from the
    network structure before jumping straight into the modeling of the
    CPDs for the entire network it might well make sense to consider
    to reduce the problem.

    In some case you might have a general problem that could be split
    into submodules. Each submodules would then be generally defined -
    say exhaustive - over the entire network if *conditioning* on some
    elements =X= and upon some output =Y= it's entire dependency with
    the network would be sufficiently specified. All of the other
    elements of the sub-module would be *encapsulated* in between.

    An example could be for instance the one of expressing the
    failures for a PC.

    Then you might well start with determining the CPDs for each
    component given the parents over the entire network. On the other
    hand you might consider to decompose the problem, leveraging
    /conditional Bayesian Network/.


    Consider for instance the /hard drive/. Although the hard drive
    has a rich internal state, the only aspects of its state that
    influence objects outside the hard drive are whether it is working
    properly and whether it is full. The Temperature input of the hard
    drive in a computer is outside the probabilistic model and will be
    mapped to the Temperature parent of the Hard-Drive variable in the
    computer model.

    You might then use the following Conditional CPDs to express the
    system:

#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-02-21_um_12.19.59.png" class="center">
#+end_export

    More formally than what previously described, albeit a bit clumsy
    ad definition in my opinion:

    
#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-02-21_um_12.21.17.png" class="center">
#+end_export


*** TODO Template Based Representations

    Skipped and not even read to this stage. Here also temporal dependent models.

    
*** TODO Gaussian Network Models

    Skipped and not even read to this stage.
    
*** TODO Exponential Families

    Skipped and not even read to this stage. I guess it is simply the
    generalization of gaussian Network Models to the different
    exponential family distributions.

                
** Inference

   An important exercise for inference is to query
   distributions. I.e. as said the task is to compute the probability
   of the occurrence of some RV given some evidence /E/, i.e. a subset
   of RVs that is observed.

   So in general the task is to determine:

   $$ P (Y | E = e) $$

   where =Y = query variable= and =E = evidence=.

   Given such definition of probability queries it is possible to
   introduce the *first type* of query: /MAP queries/.

   $$ MAP (W| e) = \operatorname*{argmax}_w P (w,e)$$

   where W = all non-observed RV.

   #+begin_quote
   I.e. in MAP queries you are interested in finding the most likely
   joint assignment of the non-observed variables given the evidence.

   If you perform MAP queries for a single RV Y then you are basically
   computing a probability query for all of the possible realizations
   y and selecting the most probable one.

   Notice that the joint prob. maximizing the likelihood might well
   differ from the individual RV maximizing realization.
   #+end_quote


   A *second type of query* is: /Marginal MAP Query/:

   The idea of this is well explained in the book via example.

   Imagine you have a class of disease. You want to find the most
   likely disease given your evidence. Assume that you observe a
   subset of symptoms E = e. You want to find the MAP assignment of
   the disease Y.

   The issue is now that you have non-observed symptoms: Z.

   If you now have a disease that has just a small number of
   associated symptoms with high probability, and you observe such
   symptoms, then your MAP query will likely select this realization
   as most likely.

   In reality there might well be a more likely realization - i.e. a
   different RV that is associated with a lot of symptoms with small
   probability. The result is that when taking that into account and
   therefore considering the possible influence of non-observed
   symptoms the conclusion might be well different.

   For this it makes sense to consider /marginal MAP/ that tries in
   fact to adjust for the presence of the other *non-observed RVs
   influencing the outcome*.

   $$ marginal MAP (Y | e) = \operatorname*{argmax}_Y  \sum_{Z}{P (Y,
   Z | e)} $$

   The issue when doing inference and performing the MAP and marginal
   MAP exercises as mentioned above is that the exercise complexity
   increases in an exponential way in the number of parents.

   We will show now that there are ways to deal with it. I.e. we will
   show that there are cases where you can reduce the problem by
   *factorizing terms out*. On the other hand we will show that
   sometimes it makes sense to circumventing the hardness of exact
   inference compromising, to some extent, on the accuracies of our
   answers. This will be addressed through *approximate inference
   analysis*.

   Finally note that no matter whether you are using exact or
   approximate methods both were shown to be *NP-hard
   problems*. Despite of this in practice approximate methods seems to
   work fine.

*** TODO Exact Inference

    Note the difficulty of doing exact inference without reducing the
    problem.

    Check at the following network:

   #+begin_src plantuml :file ~/Desktop/Blog/images/bayesNet6.png :exports none
   @startuml
   circle A
   circle B
   circle C
   circle D

   A -right-> B
   B -right-> C
   C -right-> D
   @enduml
   #+end_src

   #+RESULTS:
   [[file:~/Desktop/Blog/images/bayesNet6.png]]

#+begin_export html
 <img src="../../images/bayesNet6.png"  style = "width: 40% !important;">
#+end_export

     Then it is immediate to see why the problem increases in an
     exponential way:

#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-15_um_12.34.21.png" class="center">
#+end_export

      It is then immediate to see how such an approach explodes in
      longer chains. Consider the general problem.

#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-15_um_12.38.58.png" class="center">
#+end_export

      Note that in contrast to this if you do not leverage the
      conditional probabilistic structure you would have to generate
      $k^n$ probabilities for the different events $x_1, . . . , x_n$.

      Finally note that using /dynamic programming/ we can make the
      above exercise for expressing the joint distribution via
      conditional distributions smart. In the sense that we do not
      compute:
      
#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-15_um_12.56.48.png" class="center">
#+end_export

      as such. We rather leverage the structure to reduce the number
      of computations in the following way:

#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-15_um_12.57.35.png" class="center">
#+end_export

      So essentially the above is an exercise of factorizing the joint
      distribution and *marginalizing out variables from a
      distribution*.

      This factor marginalization exercise and will be used throughout
      the book so that I define here:

#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-15_um_15.37.58.png" class="center">
#+end_export

      Where a factor is defined as follows:
      
#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-15_um_16.00.32.png" class="center">
#+end_export

      Moreover, note that for factors both the commutative as well as
      the associative rules apply.

      This means:

      $$ \sum_X (\phi_1 * \phi_2) = \phi_1 * \sum_X \phi_2 $$

      It is now immediate given the discussion above that it is
      possible to rewrite the joint distribution in factor notation:

      $$ P(A, B, C, D) = P(A) P(B|A) * P(C|B) * P(D|C) $$

      such that it becomes:

      $$ P(A, B, C, D) = \phi_A * \phi_B * \phi_C * \phi_D $$
      
      and due to the properties of the factors:

      $$ P(D) = \sum_C \sum_B \sum_A (\phi_A * \phi_B * \phi_C *
      \phi_D) $$

      $$ P(D) = \sum_C \sum_B * \phi_C * \phi_D (\sum_A \phi_A *
      \phi_B) $$

      $$ P(D) = \sum_C  * \phi_D * \sum_B( \phi_C (\sum_A \phi_A *
      \phi_B)) $$

      where the different transformations are justified by the limited
      scope of the CPD factors; for example, the second equality is
      justified by the fact that the scope of \phi_C and \phi_D does not
      contain A.

      *Important - Important - Important:*

      #+begin_quote
      In general, any marginal probability computation involves taking
      the product of all the CPDs, and doing a summation on all the
      variables except the query variables. We can do these steps in
      any order we want, as long as we only do a summation on a
      variable X after multiplying in all of the factors that involve
      X.
      #+end_quote

      In mathematical terms you can express this condition as follows:

      $$ \sum_Z \prod_{\phi \in \Phi} $$

      We call this the *sum-product* for inference. And by now it
      should be clear to you that the advantage is the fact that due
      to the conditional structure and the fact that not every node
      depends on all of the other nodes - i.e. the fact that the scope
      of the factors is limited - you can “push in” some of the
      summations, performing them over the product of only a subset of
      variable factors. This reduces the complexity.

      So when doing exact inference you can essentially work by
      keeping *sum-product variable elimination* keeping the
      complexity low. The basic idea in the algorithm is that we /sum
      out variables one at a time/. When we sum out any variable, we
      /multiply all the factors that mention that variable/, generating
      a product factor.

      Now that the algorithm should be clear you can formulate and
      express it in its more general way:

#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-15_um_17.03.45.png" class="center">
#+end_export

      So now you have a way to compute the marginal distribution for a
      Bayesian Network. The question is on how to leverage such work
      in order to make a query of the form: $P (Y| e) $. 

      In order to answer such a challenge you can use the following reasoning.

      First note the Gibbs Distribution being defined as follows:

#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-15_um_20.05.38.png" class="center">
#+end_export

      I.e. in this setting you can express the Gibbs distribution as
      the product of factors normalized by the /partition function/.

      Moreover, given this definition it is possible to see that a
      Bayesian network conditioned on evidence E = e also induces a
      Gibbs distribution: the one defined by the original factors
      reduced to the context E = e.

      Or in mathematical terms:

#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-15_um_20.14.05.png" class="center">
#+end_export

      Where the proof follows immediately from the definitions.

      Based on this it follows immediately that we can perform
      immediately the *same sum-product variable elimination*
      algorithm to the task of computing P(Y , e).

      I.e. we would apply the sum-product variable elimination to all
      of the variables in the network excluding the query variables
      and the /evidence variables/. What you would obtain then is an
      unnormalized distribution \phi^{*}(Y). You would then normalize
      this in a last step by multiplying with the inverse of the
      partition function, which is obtained simply by marginalizing
      all of the evidence terms $E = e$ from the obtained \phi^{*}(Y).

      You can then summarize all of the above in the following
      algorithm.

#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-15_um_20.24.39.png" class="center">
#+end_export

      It should be straightforward to understand now that such a
      sum-product variable elimination algorithm is computationally
      much more efficient then a full enumeration of the joint as done
      in the introduction of this section. If you want to go into the
      asymptotics and analyze the time performance of such an
      algorithm check 9.4 of the book. There is an entire sub-chapter
      dedicated to it. This is not too much of interest to me to this
      stage so that I will skip it now.

      
*** TODO Clique Tree - Exact Inference

    This is a second technique to do exact inference. I skipped it
    here as if it will not pop up continuously in my research it is
    fine. It is not the core for my research and I want to go to the
    approximate inference task of the next section.

*** TODO Approximate Inference

    The idea is that despite the techniques mentioned before, i.e. the
    *sum-product variable elimination* and the *clique-tree* the
    complexity of such algorithm might still be too large for big
    networks.

    In such cases it might make sense to abandon the idea of making
    an exact inference and to trade-off exact results against
    computational complexity. This is the interest of this section on
    approximate inference. 

    To analyze the approximate inference task formally, we must first
    define a metric for evaluating the quality of our approximation.

    There are essentially /two perspectives/:

    - The first perspective goes as follows. In /conditional
      probability queries/ the problem is to compute the probability
      $P(Y | e)$, i.e. for some query variable $Y$ and evidence $e$.

      Hence you are essentially computing a probability function
      $P(Y | e)$, such that as metric for evaluating the approximation
      you would ultimately choose probability distance metrics - such
      as entropy measures.

    - The other perspective consists in analyzing the problem with
      different eyes.

      Consider a /specific/ query: $P(Y = y| E = e)$ such that we are
      interested in the specific assignment $y$.

      The approximate answer to this query is a number \rho, whose
      accuracy we wish to evaluate relative to the correct
      probability.

      In this sense it makes sense to check the relative error as it
      takes into account that an event might have a large or low
      probability such that the error is taken wrt to this baseline:

#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-15_um_12.06.45.png" class="center">
#+end_export

    So given such definitions it is clear that the goal of approximate
    inference is to fulfill a good approximation in the sense of the
    measures above and at the same time to reduce complexity at most.

    In this sense the general approach is the following:

    #+begin_quote
   The methods considered consists in defining a target class Q of
   “easy” (i.e. easy to query) distributions Q and then search for an
   instance within that class that is the “best” approximation to
   P_{\Phi}. Queries can then be answered using inference on Q rather
   than on P_{\Phi}.
    #+end_quote

    So what is important to understand at this point is that you move
    from an unconstrained optimization problem to a *constrained
    optimization problem*. One way you know well to deal with such
    optimization problem is the one of the Lagrange method. However,
    in the following we will use a different approach leveraging a set
    of fixed point equations derived from the /constrained energy
    optimization/, and we will work through such structure in order to
    find a solution.

    There are essentially three major ways of performing such an
    approximate inference task:

    - simplify the inference task by using an /approximate energy
      functional/.
      In this category falls the famous *loopy belief propagation*
      method. 

    - simplify the inference task by using an /use message passing
      algorithm maximizing the exact energy functional but with relaxed consistency on the representation of Q/.
      This is called the *expectation propagation* method.

    - simplify the inference task by using /the exact energy functional, but they restrict attention to a class $\mathscr{Q}$ consisting of distributions
      Q that have a particular simple factorization/. This is falls
      into the category of /mean field/ 
    

*** TODO Inference as Optimization
*** TODO Particle Based Approximate Inference

    These are essentially the methods you saw in stochastic simulation
    course. 

*** TODO Map Inference

*** TODO Inference in Hybrid Models and Temporal Models


** Learning

   I will do now some brief notes on Learning. This will likely be the
   matter of my Thesis.

   It makes sense therefore to focus now on this, given the little
   time I have now and as I have to push a bit in order to set things
   correctly into the pipeline.

   Recall that the idea of Learning, is to learn, either (i) the network
   structure, or (ii) the parameters of the model or (iii) both, from
   the data.

   In some domains, the amount of knowledge required is just too large
   or the expert’s time is too valuable to ask one to set up and
   construct all of the network.  In others, there are simply no
   experts who have sufficient understanding of the domain.  In many
   domains, the properties of the distribution change from one
   application site to another or over time, and we cannot expect an
   expert to sit and redesign the network every few weeks.

   For all of these reasons learning model parameters and structure
   from the data is particularly important.

   Formally, we have a distribution P^* that is induced by a network
   M^* = (K^*, \theta^*). Given a dataset D = (d[1], ..., d[m]) of M
   samples of P^*. Notice that such data samples are i.i.d. P^*
   distributed. Then given a some model family $\tilde{M}$ that
   defines a probability $P_{\tilde{M}}$ (or $\tilde{P}$ when
   $\tilde{M}$ is clear). I.e. we may want to learn only model
   parameters for a fixed structure, or some or all of the structure
   of the model.

*** Goals of Learning

    Notice that we want to construct a $\tilde{M}$ that precisely
    represents the distribution P^*.

    Because of the limited amount of data and the fact that we might
    possibly have to estimate a very high-dimensional distribution it
    is clear that in practice we must select an $\tilde{M}$ that is
    just a *best* approximation of M^*.

    To define what a *best* approximation is, we have to specify the
    goals of learning such that we can quantify how well a
    distribution approximates.

    
**** On a Precise Density Estimation

     It is clear that if the goal of setting up a bayesian network is
     the one of performing /inference/, then you might want to
     *estimate the density at best* such that your inference will be the
     most precise as possible.

     I.e. you try to construct a model $\tilde{M}$ such that
     $\tilde{P}$ is "close" to the generating distribution P^*.

     In order to measure how close the two densities lie to each other
     you can use the /relative entropy distance/:

#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-02-22_um_12.01.01.png" class="center" style = "width: 30% !important;">
#+end_export

     Notice however that in the above you implicitly assume that P^*
     is known. Obviously this is not the case in many practical cases
     and it is in fact what we aim to achieve.

     A solution for this is the following:

#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-02-22_um_12.21.43.png" class="center">
#+end_export

     Continuing the sentence in the above, the -H_p term is the
     entropy above and the second is the /expected log-likelihood/. It
     is immediate to see that the second term is higher, the higher
     the probability that $\tilde{M}$ gives to points, sampled from
     the true distribution.

     As a consequence of that, it holds however that the
     log-likelihood as a metric for comparing one learned model to
     another, we cannot evaluate a particular $\tilde{M}$ in how close
     it is to the unknown optimum as we have lost in the above the
     baseline $E_{P^{*}(\xi)}(log (P^{*}(\xi)) - i.e. the first term
     that we ignore as not depending on $\tilde{P}$.

     Notice moreover that in our discussion we will be interested in
     the /likelihood/ of the data given the model M - i.e. on $l(D :
     M)$. (recall this notation).

     Another option for comparing how well a model fits a distribution
     is through the notion of /loss functions/ $loss(\xi : M)$. This
     measures the loss a model $M$ makes on a particular data sample,
     i.e. on an instance \xi.

     Assume that you take loss function is expressed as the /negative
     log-likelihood/, i.e. $loss(\xi : M) = -
     \sum^{M}_{m=1}log(P(\xi[m]) : M)$.

     Then it holds for the /expected loss/:

#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-02-22_um_12.54.30.png" class="center">
#+end_export

     
**** Specific Prediction Tasks
     :LOGBOOK:
     CLOCK: [2021-02-22 Mon 14:15]--[2021-02-22 Mon 14:40] =>  0:25
     :END:

     Notice that when assuming that you want to learn the model to
     perform probabilistic inference, you implicitly state that your
     aim is to make conclusions on the overall distribution /P^*/.

     I.e. in such a case you are interested in evaluating the
     probability of a full instance \xi, i.e. the probability of an
     occurrence/sample over/of the entire network.

     In contrast to this setting in many situations we might be
     interested in answering a whole range of queries of the form
     P(Y | X).

     For instance in a classification task we might be interested in
     selecting an Y given X. We can then work in such a case with a
     MAP assignment to Y, i.e.

     $$h_{\tilde{P}} = \operatorname*{argmax}_y \tilde{P} (y | x)$$

     We might then act similarly for other cases.

     We might even use classification errors such as the standard =0/1
     loss=.

     Another option is to focus on the general extent to which our
     learned model is able to predict data generated from the
     distribution.

#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-02-22_um_14.20.47.png" class="center" style = "width: 30% !important;">
#+end_export

     Notice that it is immediate to see that if we /negate/ the above
     we immediately obtain a loss function to compute an empirical
     estimate by taking the average relative to a data set.


**** Knowledge Discovery
     :LOGBOOK:
     CLOCK: [2021-02-22 Mon 14:45]--[2021-02-22 Mon 15:10] =>  0:25
     :END:

     This is another possible goal in comparison to probabilistic
     inference. Here the idea is that you want to understand important
     properties of the domain by observing P^*.

     I.e. what are the direct and indirect dependencies, what
     characterizes the nature of the dependencies and so forth.

     Of course, simpler statistical methods can be used to explore
     the data, for example, by highlighting the most significant
     correlations between pairs of variables. However, a learned
     network model can provide parameters that have direct causal
     interpretation and can also reveal much finer structure, for
     example, by distinguishing between direct and indirect
     dependencies, both of which lead to correlations in the
     resulting distribution.

     Notice that such a task requires a very different approach in
     comparison to the prediction task.

     In this setting, we really do care about reconstructing the
     *correct model* $M^*$. While before we could well have distorted
     reconstructed model $\tilde{M}$ as long as we would induce a
     *distribution* similar to the one induced by $M^*$.

     So in this task we are not interested in some metric stating the
     difference in the distributions defined by the models but rather
     as a measure of success we should take directly something
     representing the distance between $\tilde{M}$ and $M^*$.

     This is however *not always achievable*. Even, with a large
     amounts of data, the true model might not be
     /identifiable/. Recall in fact that for instance the =network
     structure= itself $K^*$, might not be well identifiable due to
     the I-map discussion of the representation chapter. The best we
     can achieve in this sense is to recover an I-equivalent
     structure.

     Such problems are exacerbated when data is limited. It might be
     difficult to detect the correlation of two nodes that are in fact
     related in the true model and distinguish it from some spurious
     correlation in the data. Note that such a limit is less prominent
     in a *density estimation task*. The reasoning is that as if the
     correlation does not appear in the data than it is likely to be a
     weak one.

     The relatively high probability of making model identification
     errors can be significant if the goal is to discover the correct
     structure of the underlying distribution. So here it is important
     to make some *confidence* statement about the inferred
     relationship.

     Thus, in a knowledge discovery application, it is far more
     critical to assess the confidence in a prediction, taking into
     account the extent to which it can be identified given the
     available data and the number of hypotheses that would give rise
     to similar observed behavior. On how to deal with it will be
     analyzed in the next sections.
     

**** On Learning as an Optimization Task

     Notice that in the above sections we defined some numerical
     criterion to define the extent to which the distributions are
     comparable to each other.

     Given such numerical measures that we wish to mini- or maximize,
     it follows immediately that learning can be generally seen as an
     /optimization/ exercise.

     We have in fact a /hypotheses space/, that is a set of candidate
     models and an objective function that we aim to optimize. So the
     learning task essentially amounts to find a high-scoring model
     within our model class.

     In the next section we go a bit deeper and analyze the
     ramifications of choosing one objective function over the other.

     
***** Empirical Risk and Overfitting

      As said one of the objective functions we might have is the
      expected loss.

      I.e. we are interested in minimizing $E_{\xi \sim
      P^*}[loss(\xi : M)]$.

      Notice, that as mentioned before as we do not know the
      distribution P^* we work with the empirical distribution
      \^{P_D}, this is given for an event /A/ as follows:

      $$\^{P_D}(A) =  \frac{1}{M} \sum_m 1_{\xi{m} \in A} $$

      i.e. the empirical distribution is the count of instances that
      are elements of the event /A/ over all of the instances sampled
      M. It is therefore the usual frequency.

      Notice that as the number of training samples grows the
      empirical distribution approaches the true distribution. This
      due to the LLN.

      So as you have not have no knowledge about the true P^*, you use
      \^{P_D} as your true distribution and compute the empirical loss
      over it.

      However, note that there are important limits in such
      approach. The dimension of Bayesian Networks distribution
      increases exponentially in the number of nodes - i.e. especially
      when nodes have multiple parents.

      Consider for instance the following:

#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-02-26_um_11.03.43.png" class="center">
#+end_export

      Moreover, recall when using the empirical distribution the usual
      issue of overfitting. I.e. it might be easy to get very high
      accuracy given a possibly large number of parameters. Notice
      however that the empirical dist does not have to be 100%
      representative for the true distribution as discussed above. So
      recall that.


      Recall the standard *bias-variance trade off* in this sense. We
      are in the following /standard statistical dilemma/.

#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-02-26_um_11.21.58.png" class="center">
#+end_export

      So generally we must take care *not to allow a too rich class of
      possible models*. As in fact the error introduced by variance
      may be larger than the potential error introduced by bias.

      So in general recall that *restricting the space of possible
      models* $\tilde{M}$ we might have a worse performance on the
      training objective, but a better distance to P^* - our true
      objective.

      The issue however, is that you are not sure in which case you
      are. It is therefore best practice to use a *regularization*
      technique to counter-balance such effects. You saw examples of
      these multiple times in your courses: LASSO, Ridge, Information
      Criteria etc. etc.

      In the case of Bayesian Network it is common practice *not to
      just use one of these self-determining* regularization
      techniques but rather also setting ex-ante *hard-constraints* on
      the model class.

      So in general when performing our Maximization exercise we use a
      combination of the two, i.e.

      (i) on the one hand, you set hard constraints on the model class

      (ii) on the other hand, we use an optimization objective
      function that /leads us away from overfitting/.

      
***** Validation (Interesting formalization of PAC bounds)

      You can then test how well you achieved your results using
      cross-validation and holdout standard techniques.

      Notice that another interesting technique to determine how well
      the learned model is performing is to compute *PAC bounds*. This
      is something I did not encounter in my memory under this name in
      my Stat courses.

      Recall that we cannot guarantee with certainty the quality of
      the learned model. Recall that the data set D is sampled
      stochastically from P^*. There is always chance that we would
      have "bad luck" and sample a very unrepresentative data set so
      that our empirical distribution will be very off. So with *PAC
      bounds* that is probably approximately correct bounds, we want
      to compute the probability of such very bad outcome and make
      sure it is small enough.

      I am pretty sure that this is what we did in the class of
      distribution system without stating this explicitly under this
      name. There we also saw computed the probability of very bad
      outcomes when implementing shared coin algorithms.

      So formally our goal is to prove that our learning procedure is
      probably approximately correct, that is for *most training sets
      D* the learning procedure will return a model whose error is
      low.

      I.e. assume the /relative entropy/ as the loss function. Let
      P^{*}_{M} be the distribution over the data sets D of size M
      sampled IID from P^{*}. Now assume that a /given learning
      procedure L/ returns a model M_{L(D)}. We search for *PAC bounds*
      a results of the forms:
      
#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-02-26_um_16.18.45.png" class="center">
#+end_export

      The number of samples M required to achieve such a bound is then
      called the *sample complexity*.

      *Note* - Important when computing PAC bounds is the following,
      and that has well to be kept in mind:

      
#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-02-26_um_16.21.47.png" class="center">
#+end_export


***** Discriminative vs Generative Training

      This goes back to the previous analysis. We discussed how we
      might well be interested in a precise density estimation of the
      *entire distribution* P^*. However, we might also be interested
      in some more *specific prediction task*.

      Depending on it we have the different notion of discriminative
      vs generative training. Consider in fact the case where we
      want the model to perform well on a particular task, such as
      predicting $X$ from $Y$.

      Then, on the one hand, when we want to estimate the /entire
      distribution/ we want a /training regime/ that would aim to get
      the model $\tilde{M}$ close to the overall joint distribution
      P^{*}(X, Y). This type of training is termed *generative
      training*. This because we use use the trained model to
      /generate/ all of the variables.

      Alternatively we can train the model /discriminatively/. Our
      goal then is to get $\tilde{P}(Y|X)$ to be close to
      $P^{*}(Y|X)$.

      Note that a model that is trained generatively can still be
      used for the specific prediction tasks. However, the converse
      does not hold true.

      Note, that on top of it discriminative training is less
      appealing in Bayesian Networks as most of the computational
      properties that facilitate Bayesian network learning do not
      carry through to discriminative training. For this reason this
      form of training is usually *performed in the context of
      undirected models*.

      Notice finally that there is a series of trade-off to consider
      when deciding among /discriminative/ and /generative/ models.

      - Generally, generative models have /higher bias/. Make more
        assumptions about the form of the distribution. They encode
        independence assumptions about the feature variables X. In
        contrast discriminative models just make independence
        assumptions on Y and about their dependence with X.

	Notice that this additional bias may not have to be strictly
        bad. It might help for standard bias-variance trade
        off. I.e. it might help to regularize and constrain the
        learned model. On the other hand you might get hurt due to
        the additional constraint you would set estimating the entire
        model when your interest is on some conditional setting.

	Note, moreover, that as the amount of data grows the bias
        effect would dominate as the variance would decrease.

        Therefore as discriminative tend to be less affected by
        incorrect model assumptions and because they will often
        outperform the generatively trained models for large data
        sets.


**** On Learning Tasks - Flavors determining different class of tasks

     We will look here in greater detail to the different variants of
     the learning tasks.

     The input of the learning procedure is the following:

     - Some prior knowledge or constraints about $\tilde{M}$

     - A set D of data instances {d[1], ..., d[M]}. Note again that as
       used throughout these notes a data instance is a data sample
       over the entire network.

     The output is then the model $\tilde{M}$, that might include
     structure, parameters or both.

     There are many variants of this fairly abstract learning
     problem. These depends on:

     - the extent of the constraints hat we are given about $\tilde{M}$

     - the extent to which the data are fully observed

***** On Model Constraints

      A non-reducing analysis is impossible as there is theoretically
      an unbounded set of options here.

      The most common constraints on our model are $\tilde{M}$ are
      the following:

      - we are given a graph structure and we only have to learn some
        of the parameters. Here we do not assume that the graph
        structure corresponds to the correct one, i.e. to K^*.

      - we do not know the structure and have to learn both parameters
        and structure.

      - we may not know the *set of variables* over which the
        distribution P^* is defined. I.e. we might observe just a
        /subset/ of these variables.

      Notice that no matter the case the less prior knowledge we are
      given, the /larger the hypotheses space/ and the more
      possibilities we need to consider when choosing the model.

      Recall that for larger models you face the statistical trade off
      of /variance-bias/ discussed before.

      Moreover, on top of that it is important to see that there is a
      /computational trade-off/ as well: the richer the hypotheses
      space, the more difficult the search to find a high-scoring
      model.

***** On Data Observability

      Notice that in the thesis you will move in this direction. Here
      the case is whether we are always able to observe the
      realization of a RV of the bayesian network or not.

      I.e. there are essentially /two cases/:

      1) complete data: in this case data are /fully observed/. I.e. for
         each of our training instances d[m] is a full instantiation in
         all of the variables spanning our bayesian network.

      2) incomplete data: in this case data are /partially
         observed/. In each training instance d[m], some variables are not
         observed.

      3) hidden variables: here the values of such variables is *never
         observed* in any training instance d[m]. This can represent
         both the case of some RV that is not modeled in the bayesian
         network as we do not know of its existence, as well as the
         case when we know of the existence of such RV but we never
         have the chance to observe it.

      Note that as we will see the usual technique to deal with
      unobservable data is the one of /hypothesize possible values/
      for these variables.

      The greater the extent to which *data are missing*, the less we
      are able to hypothesize reliably values for the missing entries.

      Note, moreover, that modeling hidden variables is important for
      knowledge discovery. These are important to understand and
      properly map the relation among variables. Think for instance in
      a medical application, you want to make sure that you ascribe
      the correct relation among two RV.

      Note, furthermore, that modeling hidden variables might highly
      reduce the complexity of the model. It is in fact possible to
      leverage conditional independencies given hidden variables. To
      make that visual consider the following:
      
#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-01_um_11.05.36.png" class="center">
#+end_export

***** Taxonomy of Learning task

      Depending on the dimension of the axis described above - i.e. on
      the specifics of the learning task we might end up in different
      situations.

      I.e.

      - in the case of parameter learning *with known structure*, the
        problem is of the class of *numerical optimization*.

      - in the case of bayesian learning with *fixed structure and
        with complete data*, the parameter learning is easily solved
        and it is sometimes even possible to find closed form
        solutions. I.e. no need to go in the numerical optimization
        dimension. Even when this is not the case the optimization
        problem is convex and can be solved easily by iterated
        numerical optimization algorithms. Note, however that such
        algorithm often requires /inference over the entire network/.

      - in the case the *structure is not given*, the problem
        incorporates an additional level of complexity. The hypotheses
        space contains an enormous - generally super-exponentially
        large - set of possible structures. here the problem of
        *structure selection* is generally also formulated as an
        optimization problem. I.e. we have different structures and we
        assign a score to them w.r.t. how well they manage to map the
        information of interest. We aim then to find the network whose
        score is the highest and this is essentially the /optimization
        task/. Note that the /score/ assigned to each network can be
        computed in /closed form/.

      - in the case of *incomplete data* the problem is nasty. Here
        multiple hypotheses regarding the values of unobserved
        variables give rise to a *combinatorial range of different
        alternative models*. This induce a non-convex, *multimodal
        optimization problem*. In order to solve this it is common
        practice to use the EM-algorithm. This suffers from the fact
        that requires usually *multiple calls to inference as a
        subroutine* making the process expensive for large
        networks. Note, that if the *structure on the top of it is not
        known*, it is even harder to come to a solution since we need
        to *combine a discrete search over the network structure with a
        non-convex optimization problem over the parameter space*.


      Given this general outline it is now possible to go in the
      different outlined cases in turn and see how to deal with them.


*** Parameter Estimation with Complete Data

    So we start here with the first learning case. I.e. we start with
    parameter learning in the case of *known network structure* and
    *fully observed instances* $D = {\xi[1], ..., \xi[m]}$.

    Note, that this is also the building block for both structure
    learning as well as learning from incomplete data.

    There are in general two approaches to dealing with such parameter
    estimation tasks:

    1) Maximum Likelihood Estimation

    2) Bayesian Appraoch

    We will explore the two next the two cases. We will start easy,
    that is with the case of a bayes network with a single
    variable. We will then expand and discuss about the generalization
    of the theory to arbitrary network structures. We will work with
    parameter estimation in the context of structured CPDs.
    
**** Maximum Likelihood - Parameter Estimation

     So consider for the simple network with one random variable
     consider the case of throwing a thumbtack - see below.

     This can either fall head or tail.

     The goal is to predict the probability through which the object
     falls heads \theta or tails (1 - \theta). So in general it is to
     estimate the probability of \theta.

#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-01_um_16.18.52.png" class="center">
#+end_export

     As you know then the MLE consists on taking the parameter for
     which the likelihood is maximized. Consider for instance M = 100,
     and head = 0.3, then it is straightforward to set \theta = 0.3
     and that is in fact the MLE. You can then calculate the
     confidence interval via standard asymptotic theory. See the
     section on asymptotic theory of your fundamentals of mathematics
     statistics if interested. Or your notes on econometrics II.

     To formalize the above straightforward. Consider a set of tosses
     x[1], ..., x[M] that are IID. Each of these tosses is H (heads),
     T (tails) with prob. \theta, (1 - \theta) respectively.

     We define the /hypotheses space/ \Theta - i.e. the set of
     parameterization we are considering - and an /objective function/
     that tells us how good different hypotheses are relative to the
     data D.

     In MLE we then take as the objective function the likelihood of
     the observations given the parameterization.

     It follows immediately that given the independence property of
     the realizations such objective function, i.e. the /likelihood/
     is given by the following:

#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-01_um_16.41.02.png" class="center">
#+end_export

     So that basically that is very standard intro statistical
     material.

     Let's leave this 101 example and let's start to consider MLE and
     set it into the bayes networks frame.

***** MLE in Bayesian Networks

      Consider the case of a set $\mathscr{X}$ of random variables, with an
      unknown distribution $P^{*}(X)$. We know the sample space,
      i.e. the random variables and their domain.

      We denote the /training set/  of samples as $D$ and assume that
      it consists of $M$ instances (i.e. recall 1 instance = 1 sample
      over the entire network; i.e. 1 realization for the entire
      network) of $\mathscr{X}$, that is \xi[1], ..., \xi[M].

      Next we assume a /parametric model/ which is defined by a
      function $P (\xi : \theta)$, i.e. read by a probability function
      of an instance realization /given/ a set of parameters \theta.

      We require than that for each choice of \theta:

      $$ \sum_{\xi} P(\xi : \theta)  = 1 $$

      The parameter space \Theta excludes parameterizations that /do
      not satisfy/ the above.

      Then depending on your modeling of the space of your X you might
      come up with different parametric models and possible
      \Theta. Again up to know nothing new. These are well known
      things to you.

      Note now that since in MLE we maximize the /objective
      function/ we ideally want it to be continuous - and possibly
      smooth - over \theta.

      To ensure these properties, most of the theory usually require
      the likelihood function to be continuous and differentiable in
      \theta.

      Recall, that given the definitions of above the likelihood
      function is defined as:


      $$ L(\theta : D) = \prod_m P(\xi[m] : \theta)  $$


      Such that the MLE is defined as:

      $$ L(\hat{\theta} : D) = \operatorname*{max}_{\theta \in \Theta}
      L(\theta : D) $$

      Given the proper definition for the problem, the question that
      remains open is simply on how to specify the above properties in
      the case of *Bayesian Networks*.

      In order to give the idea we will consider the most easy
      possible network. We consider the case of the set $\mathscr{X}$
      of variables containing two random variables $X$ and $Y$.

      Among the variables there is the following network structure:

   #+begin_src plantuml :file ~/Desktop/Blog/images/bayesNet4.png :exports none
   @startuml
   circle X
   circle Y

   X -right-> Y
   @enduml
   #+end_src

   #+RESULTS:
   [[file:~/Desktop/Blog/images/bayesNet4.png]]

#+begin_export html
 <img src="../../images/bayesNet4.png"  style = "width: 30% !important;">
#+end_export
      
       So recall that essentially our problem amounts in finding the
       formula for the /objective function/, i.e. the /likelihood
       function/ in our network and to maximize it.

       Note, that in the case of bayesian networks, our network is
       parameterized by a vector containing all of the probabilities
       in the different CPDs. So in our case \theta is a vector
       containing such probabilities.

       In the above toy example with $X, Y$ being a binary RVs, the
       parameterization would consist of:

       - \theta_{x^1} , \theta_{x^0} that specify the two
         probabilities of X

       - the conditional probabilities of the RV $Y$,
         i.e. \theta_{y^{1} | x^{1}}, \theta_{y^{0} | x^{1}},
         \theta_{y^{1} | x^{0}}, \theta_{y^{0} | x^{0}}

       Note that in the following we us \theta_{Y | x^{1}} to refer to
       the set {\theta_{y^{1} | x^{1}}, \theta_{y^{0} | x^{1}}} and
       \theta_{Y | x^{0}} to refer to the set {\theta_{y^{1} | x^{0}},
       \theta_{y^{0} | x^{0}}}. The union of these sets represents
       then \theta_{Y | X}.

       Given this and the conditional joint probability factorization
       it follows that it is possible to write the probability in the
       following form:

       
#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-05_um_10.26.08.png" class="center">
#+end_export

       *Important* is then to see that the likelihood decomposes into
       /two separate terms/. Each of this terms is a *local likelihood
       function* that measures how will it predicts given the
       parents.

       Given this, we can quickly argue for the first very important
       property of the MLE in Bayesian Networks, i.e. the
       *decomposability property* of the likelihood function.

       In order to see this consider the above local likelihoods. It
       is then immediate to understand that each of the local
       likelihood terms just depends on the local parameters for that
       CPD entry. That is, for the P(x[m] : \theta) entries the
       parameter of interest is \theta_X and for the P(y[m] | x[m] :
       \theta) entries the parameter of interest are \theta_{Y | X}.

       Given this realization it follows that 


#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-05_um_10.46.20.png" class="center">
#+end_export

       Note now the following notation that will be used throughout
       the book. Given the above the question is on how it is possible
       to express

       $$ \prod_{m : x[m] = x^0} P(y[m] | x[m] : \theta_{Y | x^0} ) $$

       The idea is then to count both the occasions where x[m] = 0 and
       y[m] = 1; and represent such number by M[x^0, y^1]. As well to
       count the occasions where x[m] = 0 and y[m] = 0 and represent
       such number as M[x^0, y^0].

       It follows then immediately that the above reduces to:

       $$ \prod_{m : x[m] = x^0} P(y[m] | x[m] : \theta_{Y | x^0} ) =
       \theta_{y^1 | x^0}^{M[x^0, y^1]} * \theta_{y^0 | x^0}^{M[x^0,
       y^0]}$$

       It is then immediate to see that the above takes the form of a
       multinomial likelihood function and we know that for it the
       maximizing parameter solution takes the form of:


#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-05_um_11.08.11.png" class="center">
#+end_export

       Note now that despite the fact that the above was a very toy
       model, such *decomposability* property holds in general for
       Bayesian Networks such that it is easy to work with the above.

       I.e. in general the following holds:

       Take a network structure $\mathscr{G}$ with parameter \theta.

       Take on top of it a dataset $\mathscr{D}$ consisting of sample
       instances $\xi[1], ..., \xi[M]$.

       Given that, we can work again as in the above case, that is
       first decompose the likelihood of an instance leveraging the
       conditional factorization of the joint likelihood of the
       instance.

       Then, in a second step, exchange the order of multiplication,
       such that you have the product of *local likelihoods*

       
#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-05_um_11.24.04.png" class="center">
#+end_export


#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-05_um_11.24.20.png" class="center">
#+end_export


       *Important* is therefore to understand that the likelihood
       decomposes as a product of *independent* terms, one for each
       CPD in the network. This is an important property called
       *global decomposition* property.

       Note moreover, that if the \theta_{X_i | Pa_{X_i}} are *disjoint*
       then the parameter vector maximizing the global likelihood can
       be easily computed:

       
#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-05_um_11.31.45.png" class="center">
#+end_export

       You can then read an example in the book for the case of table
       CPD if you are interested. Note that the above holds in general
       for multiple CPD specifications.
       
***** On Non-parameteric Models

       Note finally that although the chapter in the book deals with
       parametric CPDs; in their multinomial form - as above - and as
       linear Gaussians, a growing interest in the use of
       *nonparameteric* Bayesian estimation methods arose.

       Here, the (conditional) distribution is not defined to be in
       some particular parametric class with a fixed number of
       parameters, but rather the complexity of the representation is
       allowed to grow as we get more data instances.

       Note that in the case of discrete variables, any CPD can be
       described as a table - maybe a very large one as discussed in
       the representation section. In this sense, there is less need
       for a non-parametric representation. In contrast in the
       continuous case there is not a general "universal" parametric
       distribution that is able to cover all of the possible
       continuous distributions.

       In this sense, instead of representing some continuous
       distribution with a parameteric distribution, that might fit
       well it might be sensible to use a non-parameteric
       approach. If interested check in the book for more.


**** Bayesian - Parameter Estimation

     Note that here, we will start again with a 101 bayesian example.
     I.e. we will look again at the Thumbtack example. We will then
     generalize on that and look at the general case for bayesian
     parameter estimation.

     Recall that in Bayesian Stat you encode the prior knowledge about
     \theta with a probability distribution. This distribution represents
     how likely we are a priori to believe the different choices of
     parameters.

     Once we quantify our knowledge (or lack thereof) about possible
     values of \theta, we can create a *joint distribution* over the
     parameter \theta and the data cases that we are about to observe X[1],
     . . . , X[M]. This joint distribution captures our assumptions
     about the experiment.

     Let's go back to the thumbtack. Say that \theta is the
     probability of head. Recall that previously we assumed that
     tosses are independent of each other. Note, however, that this
     assumption was made when \theta was fixed. If we do not know
     \theta, then the tosses are not marginally independent: Each toss
     tells us something about the parameter \theta, and thereby about
     the probability of the next toss.

     Note however that given \theta, we cannot learn about the outcome
     of one toss from observing the results of others. So we say that
     the tosses are /conditionally independent/ given \theta.

     
#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-05_um_13.06.13.png" class="center">
#+end_export

     Given this specification, our joint distribution is specified up
     to P(\theta). This is the /prior/ distribution we assign to the
     RV \theta.

     Given all of the above it follows that we have the following
     /network structure and joint probability factorization/:

#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-05_um_13.09.34.png" class="center">
#+end_export

     In practice we can then apply the Bayes Formula to get to the
     posterior distribution of our parameter of interest given some
     sample x[1], ..., x[M].

     This is then the major difference with MLE. We use the posterior
     /distribution/ to compute the parameterization for our network
     instead of selecting a single value for the parameter \theta.

     In order to see this, consider the case where you want to
     estimate the probability of the next coin x[M+1] being head. Then
     you can *integrate* over the entire distribution of \theta instead
     of relying on a single value.

     The idea is that $ P(a | b) = \int P(a, c |b) dc = \int P(a |
     c,b) * P(c | b) dc$ 

     I.e. you can compute it by:

#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-05_um_15.02.02.png" class="center">
#+end_export

     Note that the last equality above uses the fact that for the
     network described conditioning on \theta we cover the information
     deriving from x[1], ..., x[M].

     Going back to the thumbtack example this translates into the
     following:

#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-05_um_15.10.51.png" class="center">
#+end_export

     Note then that it follows a section in the book where you make
     the same exercise for different priors. In particular with a Beta
     distribution.

     I skip it here as you had extensive training in bayesian in your
     master and you should be able to replicate that without big
     issues.

     Note, moreover that there is some heuristic reasoning on how you
     should choose the hyperparameters for your beta. Go read if
     interested.

     We turn now to a generalization of the above for bayesian
     networks leaving the 101 example. 
     
***** General Case and a Sufficient Statistics Example

      Assume a general learning problem with training set
      $\mathscr{D}$, that contains iid M samples of a set tof random
      variable $\mathscr{X}$ from an unknown distribution /P^*(X)/.

      Also assume a parametric model $P(\xi | \theta)$, where the
      parameter belong to the space \Theta.

      Recall that MLE search the point estimate $\hat{\theta}$ in
      \Theta maximizing the likelihood of the data. Recall that in
      contrast in bayesian we include our prior belief in the model
      treating the parameter itself as a RV.

      We therefore do not simply have a likelihood of observing some
      data given a fixed parameterization but we rather have a joint
      probability of observing some data and parameterization.

      So here the joint probability specifying the model is given by
      $P(D, \theta) = P(D | \theta) * P(\theta)$.

      Given this we can compute the marginal likelihood of the data -
      i.e. integrating the parameter out - as

      $$ P(D) = \int_{\Theta} P(D | \theta) P(\theta) d\theta $$.

      So far therefore nothing new in comparison to the 101 case. Just
      written under more general form.

      We turn next to a particularly useful parameterization for the
      bayesian network. I.e. we will show that when working under
      particular prior settings we might end up with sufficient
      statistics for which it is possible to *compactly represent
      posterior distribution*.[fn:1] This will ultimately lead to the
      prediction task of P(x[M+1] | D) which is especially easy to
      compute.

      In order to see this consider the following learning problem -
      i.e. the one of /uncertainty about the parameters of a
      multinomial distribution/. The parameter space \Theta is the one
      of all nonnegative vectors \theta = (\theta_1, ..., \theta_K)
      such that $\sum_k \theta_k = 1$.
      
      The likelihood in this model has the form

      $$ L(\theta : D) = \prod_k \theta_k^{M[k]}  $$

      One conjugate prior to such multinomial likelihood is the
      /Dirichlet/ distribution.

      
#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-05_um_16.47.14.png" class="center">
#+end_export

      Given such model specification it follows now the following.
      
#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-05_um_16.54.47.png" class="center">
#+end_export

      This prediction is similar to prediction with the MLE
      parameters. The only difference is that we added the
      hyperparameters to our counts when making the prediction. For
      this reason the Dirichlet hyperparameters are often called
      /pseudo-counts/. We can think of these as the number of times we
      have seen the different outcomes in our prior experience before
      conducting our current experiment.

      Note moreover the following interesting interpretation for
      model:
      
#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-05_um_17.07.11.png" class="center">
#+end_export

      Note; you should read the /mean prediction/ above, simply as:
      $\frac{\alpha_k}{\alpha} $.

      Note moreover that the prediction in the above case is therefore
      nothing more than a weighted average (convex combination) of the
      prior mean and the MLE estimate.

      The combination weights are determined by the relative magnitude
      of \alpha — the confidence of the prior (or total weight of the
      pseudo-counts) — and M — the number of observed samples. Note
      finally that there is convergence to MLE when the sample size M
      grows to infinity.

***** Bayesian Parameter Estimation in Bayesian Networks

      Note one final time that in the case of bayesian parameter
      estimation we must specify a join distribution the data
      instances and the unknown parameters.

      Again as in the case of MLE we will start from a very basic
      trivial network and we will generalize then to a global
      Decomposition.

      *The simple Case*
      
      Again, we consider the case of the set $\mathscr{X}$ of
      variables containing two random variables $X$ and $Y$.

      Among the variables there is the following network structure:

      #+begin_export html
<img src="../../images/bayesNet4.png"  style = "width: 30% !important;">
      #+end_export

      We have training observations X[m] and Y[m] for m = 1, ...,
      M. In addition we have unknown parameter vectors \theta_X and
      \theta_{Y|X}.

      Note that the meta-network might be represented as follows:

#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-05_um_17.28.09.png" class="center">
#+end_export

      Note that, as in our simple thumbtack example, the instances are
      independent given the unknown parameters.  examination of active
      trails shows that X[m] and Y [m] are d-separated from X[m'] and
      Y[m'] once we observe the parameter variables.

      Moreover, we assume that that the individual parameters *are a
      priori independent*. We believe that knowing the value of one
      parameter tells us nothing about another one.

      Formally:

#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-05_um_17.39.17.png" class="center">
#+end_export

      Thus, although we use the global parameter independence in much
      of our discussion, it is not always appropriate.

      Once you accept global parameter independence an important
      property emerges:

      *Important - Important - Important:*

#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-05_um_17.48.55.png" class="center">
#+end_export

      So this has important ramifications as it means that given the
      data $\mathscr{D}$ we can determine the posterior over \theta_X
      independently of the posterior over \theta_{Y|X}. I.e. you can
      solve each problem separately and then combine the
      results.[fn:2] Finally, in the case of bayesian parameter
      estimation another important property arise. It tells us that
      the posterior can be represented in a compact factorized form.

      Note, that the global parameter independence together with the
      local likelihood decomposition arising from the conditional
      expression of the likelihood as done in the previous section on
      MLE yields the following result:
      
#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-06_um_10.20.50.png" class="center">
#+end_export

      Don't be confused by how the likelihood is defined above. Recall
      that the likelihood always express P(data | parameter).

      So note that we proved the claim above: /we can work locally and
      solve each problem separately and then combine the results/.

      Note that such *local decomposition* property extends also to the
      *prediction task*. It is in fact possible to see that leveraging
      the property above we have for the prediction task of x[m+1],
      y[m+1]:
      
#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-06_um_10.30.11.png" class="center">
#+end_export

      That through the d-separation of \theta arising from the
      complete observations, as well from the posterior probability
      decomposition it is possible to be expressed as:

#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-06_um_10.32.39.png" class="center">
#+end_export

      Note that the first equality in the pic above is not the second
      equality. The first is the first term of the second equality -
      the one of interest.

      Note that such local decomposition holds in general for the case
      of prediction task with complete data, such that it holds in
      general the following formulation:
      
#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-06_um_10.35.38.png" class="center">
#+end_export

      We stress that the discussion so far was based on the assumption
      that the priors over parameters for different CPDs are
      independent; i.e. the *global parameter independence
      property*. We see that, when learning from complete data, this
      assumption alone suffices to get a decomposition of the learning
      problem to several “local” problems, each one involving one CPD.

      /Example in Practice - Table CPD with Dirichlet Distribution/

      Note now that in the book it follows a brief section
      demonstrating all of the above general arguments to the case of
      table-CPDs. Everything is looked again through the lenses of
      /Dirichlet/ priors and multinomial distribution for the
      individual nodes.

      You can read more into the details in the book should you be
      interested in an application as that. You can also quickly go
      through it should you be in need to refresh quickly the applied
      approach, in order to work then with a different case.

      Note that in this sense in the book, the discussion in this
      chapter focuses solely on Bayesian estimation for multinomial
      CPDs. Here, we have a closed form solution for the integral
      required for Bayesian prediction, and thus we can perform it
      efficiently. In many other representations, the situation is not
      so simple. In some cases, such as the noisy-or model or the
      logistic CPDs of section 5.4.2, we do not have a conjugate prior
      or a closed-form solution for the Bayesian integral. In those
      cases, Bayesian prediction requires numerical solutions for
      high-dimensional integrals. Then check your notes. Studied
      extensively how to deal with such cases.

      Note that alternatively on using the more fine-grained
      mathematical models for dealing with posterior integration you
      can also take the short cut and work with =MAP= estimation of
      the parameters, i.e. you do not integrate over the distribution
      but compute the /maximum a posteriori/:
      
#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-06_um_12.16.14.png" class="center">
#+end_export


#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-06_um_12.17.41.png" class="center">
#+end_export

****  On Parameter Estimation with shared Parameter

      Here the difference is likely that the key property of global
      parameter independence fades away. I.e. we do not assume it and
      the complexity increases.

      That is correct, in fact in the Bayesian case, we also assumed
      that the priors on these distributions are independent. This
      assumption is a very strong one, which often does not shared
      hold in practice. We worked also for the MLE under similar
      structure. In fact when working with table-CPDs we assumed that
      global independence of the parameters.

      In real-life systems, we often have shared parameters:
      parameters that occur in multiple places across the network.

      In this section, we discuss how to perform parameter estimation
      in networks where the same parameters are /used multiple times/.

      There are essentially two types of parameter sharing:

      + global parameter sharing
	
      + local parameter sharing

      We will explore the two in turn next.

      *Global Parameter Sharing:*

      Consider a network structure $\mathscr{G}$ over a set of
      variables X = {X1, . . . , Xn}, parameterized by a set of
      parameters \theta. Each variable Xi is associated with a CPD
      P(X_i | U_i , \theta). Now, rather than assume that each such
      CPD has its own parameterization \theta_{X_i|U_i} , we assume
      that we have a certain set of shared parameters that are used by
      multiple variables in the network. Thus, the sharing of
      parameters global parameter is global, over the entire network.

      Note now the following notation to express the above. Do not
      take it too seriously. Not the best expression of the above in
      the below formulation in my opinion but notation that will be
      used later is this so stick to it.
      
#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-06_um_13.07.38.png" class="center">
#+end_export

      So the above means that you have sets \theta^k containing a subset
      parameters and you assign to it the variables that use such
      parameters. As parameters are shared and multiple might use the
      same parameters you assign sets of variables $\mathscr{V}^k$ to
      such sets \theta^k.

      Note that with the above the notation you have immediately a *new
      likelihood decomposition*.

#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-06_um_13.15.26.png" class="center">
#+end_export

      From this it follows immediately the following likelihood
      function - i.e. it is simply a plug-in exercise:
       
#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-06_um_13.35.20.png" class="center" style = "width: 30% !important;>
#+end_export

      Given that you can quickly examine the new behaviour of the
      likelihood under this global parameter sharing setting:

#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-06_um_13.35.20.png" class="center">
#+end_export

       And through this you would get a different MLE taking this
       global structure into account

       $$ \hat{\theta^{k}_{y_k | w_k}}  = \frac{M_k[y_k,
       w_k]}{M_k[w_k]}$$

       This aggregation of sufficient statistics applies not only to
       multinomial distributions. Indeed, for any distribution in the
       linear exponential family, we can perform precisely the same
       aggregation of sufficient statistics over the variables in
       $\mathscr{V}^k$. 

       Note finally that the above has important implications. It
       means that a new observation influences the parameter of
       possibly another CPD that shares the same global
       parameter. This has both positive as well as negative
       influence: that can be summarized in the usual concept of
       /bias-variance/ trade off.

       Otherwise check the following paragraph for the length
       verbalization of that:

       When we share parameters, multiple observations from within the same
       network contribute to the same sufficient statistic, and thereby help
       estimate the same parameter. Reducing the number of parameters allows
       us to obtain /parameter estimates that are less noisy/ and closer to the
       actual generating parameters. This benefit comes at a price, since it
       requires us to make an assumption about the domain. If the two
       distributions with shared parameters are actually different, the
       estimated parameters will be a (weighted) average of the estimate we
       would have had for each of them separately. When we have a small
       number of instances, that approximation may still be beneficial, since
       each of the separate estimates may be far from its generating
       parameters, owing to sample noise. When we have more data, however,
       the shared parameters estimate will be worse than the individual
       ones. We return to this issue in section 17.5.4, where we provide a
       solution that allows us to gradually move away from the shared
       parameter assumption as we get more data.
   

       *Local Parameter Sharing:*

       I skip it here. The reasoning is similar to the previous
       setting. Go read it in the book if interested at any point.

       *Bayesian Inference with Shared Parameters:*

       To perform Bayesian estimation, we need to put a prior on the
       parameters. In the case without parameter sharing, we had a
       separate (independent) prior for each parameter. This model is
       clearly in violation of the assumptions made by parameter
       sharing. If two parameters are shared, we want them to be
       identical, and thus it is inconsistent to assume they have
       *independent prior*.

       Consider the /global parameter sharing/ discussion above. Then
       for Bayesian networks the strategy is usually to introduce a
       prior over each /set of parameters/ \theta^k . If we impose an
       independence assumption for the priors of the different sets,
       we obtain a shared-parameter version of the global parameter
       independence assumption. Hence, this is the general strategy
       for dealing with shared parameters in Bayesian settings.

       Note, however that some of the implications previously
       discussed do not hold anymore under this setting.

       If on the one hand it still holds that it is possible to
       decompose the posterior as a product of the subsets priors and
       likelihoods, i.e. to write:
       
#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-07_um_10.06.55.png" class="center" style = "width: 30% !important;>
#+end_export

       On the other hand when computing the likelihood of a new
       observation it is different. While the previous section
       treating the case for bayesian inference leveraged some
       independence property this is not possible anymore here due to
       the effect of shared parameters.

       I.e. in the previous section we could operate as follows:
       
#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-07_um_10.19.24.png" class="center">
#+end_export

       When we work with shared parameters the above does not
       hold. I.e. the posterior parameters are not independent we
       cannot write their expectation as a product of expectations.

       Then you should pay more care when dealing with the above.

       This concludes this section. Note that many things were not
       discussed too much into the detail but you have to learn to
       move fast in life. You can go back to the book should you want
       more info. Note also that there is a final section discussing
       /hierarchical priors/ that was not treated here.


*** Parameter Estimation under Uncertainty - Partially observed Data

    There are essentially three cases on why partially observed Data
    might arise:

    - missing data by accident

    - observations not made - for instance in medical setting - some
      patients some measurements others not

    - hidden variables - values are never observed. forgot/not
      included in the model

    We will see how incomplete data poses both /foundational and
    computational/ problems.

    The /foundational problem/ concern in formulating an appropriate
    learning task and /specifying what we can expect to learn/ from such
    data.

    The /computational problems/ arise from the complications incurred
    by incomplete data and the construction of algorithms that address
    these complication.

    
**** The Learning Problem with Incomplete Data

     A central concept in the discussion of learning so far was the
     likelihood function that measures the probability of the data
     induced by different choices of models and parameters.

     Note that the likelihood is central both to the bayesian
     procedure - and for computing the posterior - as well to the MLE
     procedure.

     The question is then on how to frame the likelihood in the case
     of incomplete data. This is done through the /marginal concept/.

     That means that the likelihood of an incomplete instance is
     simply the marginal probability given our model.

     In an example you would then have the following:

     Suppose the domain consists of two random variables X and Y , and
     in one particular instance we observed only the value of X to be
     x[m], but not the value of Y . Then, it seems natural to assign     
     the instance the probability P(x[m]).

     This approach is very intuitive and might at first seems
     flawless. However, the approach embodies some rather strong
     assumptions about the nature of our data, and especially the data
     generating process.

     In fact when dealing with missing data you can think of the data
     generating process as a two folded process:

     1) generate the data from the model. 
	
     2) determine - possible through a probabilistic model - which
        values we get to observe and which ones are hidden from us.

     As for the second case a basic example is the one of playing
     /risiko/ with friends. You roll the dices. Sometimes they fall
     out of the table. You do not observe the result. This is possibly
     a hiding mechanism. Note now that in this particular situation we
     do not have a learning process. Consider however the same setting
     for the case of the thumbtack example discussed above. Then as
     the result of a thumbtack out of the table looks quite different
     than the one falling on the table - i.e. we might have a
     different data generating process - we ignore the samples falling
     out of the table. We will see next that /ignoring/ some samples
     might indeed be the right way to deal with the issue. However,
     this might not always be the case. Think for instance when you
     have a reporting person that tells you the thumbtack got out of
     the table in every case where the thumbtack actually falls on
     tail. Then ignoring such observations would lead you to
     misleading conclusions.

     What you can do in the false reporting case is to use the
     knowledge that every missing value is “tails” and count it as
     such. Note that this leads to *very different likelihood function*
     (and hence estimated parameters) from the strategy that we used
     in the previous case.

     This is an example of why it is important to understand and
     determine the data generating process in order to properly
     specify the likelihood function and come to your desired
     solution.

     To define the problem and set it into mathematical notation in
     this section we will hence work with two variables for each
     experiment. I.e. a $X$ variable for the /actual/ flip outcome and
     an $O_X$ for the observed outcome that tells us whether the flip
     was observed or not.

#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-07_um_13.26.33.png" class="center">
#+end_export

     Let $X = {X_1, ..., X_n}$ be some flip outcome sample. Let $O_X =
     {O_{X_1}, ..., O_{X_n}}$ be the observation realization.

     Note that the /observability model/ is a joint distribution
     $P_{missing} (X,O_X) = P(X) * P(O_X | X)$ with parameters \theta
     for P(X) and \phi for P(O_X | X).

     We /define/ a new RV $Y$ summing up the information from $X$ and
     $O_X$. I.e. the variable Y takes the following shape:

#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-07_um_18.35.42.png" class="center">
#+end_export

     Given such definitions it is straightforward to see that for the
     likelihood of our model for the case of /randomly/ non-observed
     variables it holds that:

     $$P(Y = 1) = \theta\phi \\ P(Y = 0) = (1 − \theta)\phi \\ P(Y =
     ?) = (1 − \phi)$$

     such that the overall likelihood function becomes:
     
#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-07_um_18.56.51.png" class="center">
#+end_export

     *Important - Important - Important:* is to understand that in
     this case the likelihood decomposes as in the case of complete
     data seen in the previous chapter. The idea is that above there
     is a clear function depending on \theta and the realization of X,
     i.e. be it M[0] or M[1] and a clear function depending on \phi
     depending both on the realization of X and Y. You can then solve
     for the maximum of these two functions separately and achieve the
     global optimum. This is in fact what is written above in an
     implicit way and this the way to solve this problem.

     Consider, in contrast, the case of /voluntarily/ removing some
     observations.

     Now the parameter \phi determining the observation of the flip
     realization does depend on the outcome of the coin flip.

     So we would ultimately have two different possible
     parameterization: \phi_{O_X|x = heads} , \phi_{O_X|x =
     tails}. Both express the probability of observing the
     flip (o = o^1) in the case that is specified in the conditional
     part. Note that this represented by the meta-network above. You
     can then observe the dependency of the RV O_X on X in the
     above. This represent in fact such described conditional
     dependency.

     Note that in this case when we get an observation Y = ?, we
     essentially observe the value of O_X but not of X. In this case,
     due to the direct edge between X and O_X, the context-specific
     independence properties of Y do not help: X and O_X are
     correlated, and therefore so are their associated
     parameters. Thus, we cannot conclude that the likelihood
     decomposes. In poor words, we cannot say that observing Y = ?
     there is no relation among O_X and X as was the case before when
     observing Y = ?, and hence o = o^0 did not given any information
     for the relation among X and Y.

     This is straightforward to see when writing down explicitly the
     likelihood function.

#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-07_um_19.32.23.png" class="center">
#+end_export

     As we can see, the likelihood function in this example is more
     complex than the one in the previous example. In particular,
     there is no easy way of decoupling the likelihood of \theta from
     the likelihood of \phi{O_X|x^1} and \phi_{O_X|x^0}. This makes
     sense, since different values of these parameters imply different
     possible values of X when we see a missing value and so affect
     our estimate of \theta. Note therefore that here you cannot
     perform the optimization of the likelihood for the functional
     terms separately by checking the likelihood for the observation
     X and Y respectively. Here you have to optimize the two
     concurrently because changing the *parameter governing the data
     generating process* for the flip realization also affects the
     observation outcome.

     Note that modeling the observation variables observed and writing
     a likelihood for these down, i.e. in other words of the process
     generating the missing values, might quickly lead to a complex
     likelihood as in the case above. Another possibility is to focus
     just on the data generating process for the variables of
     interest - say $X$ here. In some cases this might yield a problem
     that is more tractable. The question now is on how to specify
     this generating process for the variables of interest *decoupling
     it from the observation mechanism*. This will be the topic of the
     next session.

     *Decoupling the Observation Mechanism - Get easier Likelihood*

     So the question of interest is on /when it is safe to ignore the
     observation variables all together/?

     We saw in the previous discussion that it was safe to ignore the
     unobserved variables when the *observation mechanism* is
     completely *independent of the domain variables* of interest.

     This is a result that generally holds and it makes sense
     therefore to formalize such a case:

#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-07_um_21.24.15.png" class="center">
#+end_export

     Note that in the case of data /missing completely at random/ the
     likelihood of X and O_X *decomposes as a product* as previously
     seen. As mentioned as well this has the nice property that we can
     then maximize the terms independently. Note moreover that in such
     a case you cannot just only maximize and get your parameters of
     interest independently. You can in fact *ignore the parameters
     governing the observation mechanism all together*.

     The idea of why this in fact holds true is the following:

     #+begin_quote
     The implications of the decoupling is that we can maximize the
     likelihood of the parameters of the distribution of X without
     considering the values of the parameters governing the
     distribution of O_X. Since we are usually only interested in the
     former parameters, we can simply ignore the later parameters.

     #+end_quote

     Note that the assumption that the data governing mechanism is
     such that data are /missing completely at random/ is a quite
     strong one. Luckily /missing completely at random/ data is a
     *sufficient but not necessary condition* for the decomposition of
     the likelihood function.

     I.e. it is possible to prove a more general condition where,
     rather than assuming marginal independence between O_X and the
     values of X, we assume only that the observation mechanism is
     *conditionally independent* of the underlying variables given other
     observations.

     One example of /conditional independence/ is the following:

     Consider flipping two coins. You observe the result of the first
     coin flip X_1. Based on this you decide whether you will hide or
     not the second coin toss. It follows that conditioning on
     observing X_1, X_2 and O_{X_2} are independent.

     Expressing the above situation in terms of its likelihood gives
     the following with \theta_{X_1} and \theta_{X_2} representing the
     parameters for the likelihood of the flip realizations of heads
     and \phi{O_{X_2} | x_1^1}, \phi{O_{X_2} | x_1^0}:

     - there are six possible cases:

       + 4 cases where we observe both of the coins realization -> all
         possible combinations.

       + 2 cases where we just observe the first coin.

     For the first class - where both are observed the probability is
     straightforward. Both values of X_1 and X_2 play a role such that
     both \theta_{X_1} and \theta_{X_2} enter the relation.

     Note that in contrast to that for the second class this is not
     the case. Consider for instance $P(X_1 = x_1^1, O_{X_1} = o^1,
     O_{X_2} = o^0)$ then the value of X_2 does not play a role at
     all. I.e. the probability is expressed as $ \theta{X_1} * (1 -
     \phi{O_{X_2} | x_1^1})$. This because of the *conditional
     independence* described above. I.e. given the first realization
     and conditioning on the information of this whether we observe
     the second value or not does not depend on the realization
     itself.

     Writing down all of the 6 possibilities and rearranging terms
     yields:
     
#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-08_um_10.03.31.png" class="center">
#+end_export

     Hence we achieved the /local factorization property/ and we
     achieved our goal.

     A formalization of the above to the general case follows.
     
#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-08_um_10.16.20.png" class="center">
#+end_export

     In other words we have /missing at random (MAR)/ data when we
     have independence among o_X and x^y_{hidden} given
     x^y_{observed}.

     This essentially means as expressed above that the /observation
     pattern gives us no additional information about the hidden variables given the observed variables/.

     $$ P_{missing}(x^y_{hidden} | x^y_{obs}, o_X) = P_{missing}(x^y_{hidden} | x^y_{obs}) $$
     
     This essentially means the following important property

#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-08_um_10.25.51.png" class="center">
#+end_export

     Note that in the expression above the first term only involves
     the parameters \phi and the second only the parameters \theta. So
     that if we are interested in our model on the parameters for the
     flip coin realizations and not on the one of the observation
     mechanism we can focus on the latter and decompose the problem.

     I.e. recall *bottom line*:

#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-08_um_10.32.43.png" class="center">
#+end_export

     We will continue the chapter with the assumption that the /MAR/
     condition holds so that we can just focus on the /likelihood/ of
     the realizations that were actually observed without having to
     consider the cases where the hiding mechanism kicked in. 

**** On the Likelihood Function - given the MAR assumption

     We will check here the likelihood  function and we will see that
     both the properties of /global decomposability/ as well as /local
     decomposability/ get lost in the case of partially observed data
     with important consequences for the computational complexity in
     the evaluation of the likelihood function.

     Assume MAR and consider the following structure: we have a
     network $\mathscr{G}$ over a set of variables X. In general, each
     instance has a different set of observed variables. We will
     denote by O[m] and o[m] the observed variables and their values
     in the m’th instance, and by H[m] the missing (or hidden)
     variables in the m’th instance. We use L(\theta : D) to denote
     the probability of the observed variables in the data.

     Recall the previous derivation, i.e.

#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-08_um_10.25.51.png" class="center">
#+end_export

     It follows now that given the above for the likelihood it holds
     marginalizing out the hidden variables as above, and ignoring the
     observability model - the first term - for the reasons discussed
     above (i.e. not the object of interest and theorem 19.1), that:

      $$ L(\theta : D) = \prod_{m=1}^M P(o[m] | \theta)$$

      Note that the above notation might be a bit confusing. I keep it
      for consistency with the reference book. The =o= above in the
      latter equation does not refer at all to the observability
      mechanism. It simply represents the realization of the x_{obs}.

       It might appear that the problem of learning with missing data
       does not differ substantially from the problem of learning with
       complete data. We simply use the likelihood function in exactly
       the same way. Although this intuition is true to some extent,
       the computational issues associated with the likelihood
       function are substantially more complex.

       To understand why it becomes *nasty* understand the following
       issue:

#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-08_um_11.39.46.png" class="center">
#+end_export

       Now consider the case with partially observed data. Recall that
       in such a case the likelihood function is the *sum of all of
       the possible complete (observed) likelihood functions*. This is
       in fact what marginalizing over the hidden variables means.

       In the specific for the above example this means (*important
       example*):
       
#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-08_um_11.47.43.png" class="center">
#+end_export

       Note that we can think of the situation using a geometric
       intuition. /Each one of the complete data likelihood defines a
       unimodal function/. Their sum, however, can be multimodal. In
       the worst case, the likelihood of each of the possible
       assignments to the missing values contributes to a different
       peak in the likelihood function. The total likelihood function
       can therefore be quite complex.

#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-08_um_11.55.42.png" class="center">
#+end_export

       Moreover, on the top of this we /loose the property of
       parameter independence/ and hence of *local
       decomposability*. This can be quickly seen by the coupling of
       the parameters \theta_{x^1} *and* \theta{y^1 | x^1} and
       \theta{y^1 | x^0} as in the above example.

       This means that you cannot look at the local case x^0 or x^1
       and derive the likelihood from there such that the likelihood
       is the product of terms involving local observations. There is
       in fact an interaction among *local-observations* when computing
       the likelihood.


       You can also see that the *local decomposability* property gets
       lost by looking at the meta-network:
       
#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-08_um_11.59.48.png" class="center" style = "width: 40% !important;">
#+end_export


       It is obvious that when we observe X[m] the trail between
       \theta_X and \theta{Y|X} is not active. The observation of
       X[m] makes the information on \theta_X superfluous. When these
       are not observed however obtaining a sample of Y might be
       informative on the effect of \theat_X due to the indirect
       influence. It means then that \theta_X and \theta_{Y|X} are
       not independent anymore as being both influenced by the same
       observations.

       While it is clear that the local decomposability goes lost in
       the case of partially observed data, the question that arises
       is whether the *global decomposability* goes lost as well.

       I.e. the recall that in the case of complete data we observed
       that we could treat the likelihood between /different CPDs/
       independently. I.e. we could maximize the likelihood of each
       separately and the maximization of each of these CPDs combined
       would yield the overall maximum for the combined likelihood.

       /Local vs Global Decomposability/: So to put it in one sentence
       as the line was not drawn in a super clear way in these notes
       so far, for the local decomposability you are interested in a
       /single CPD/ say for instance P(Y|X) as in the case just
       discussed above. The question if you can decompose the
       likelihood for the different local realizations, say x^0 or
       x^1. In contrast, for the case of /global decomposability/ you
       are interested in whether you can decompose the overall
       likelihood for different CPDs. I.e. for instance in a model of
       the following form with H being a /hidden variable/

   #+begin_src plantuml :file ~/Desktop/Blog/images/bayesNet5.png :exports none
   @startuml
   circle H
   circle X
   circle Y

   H -right-> Y
   H -right-> X
   @enduml
   #+end_src

   #+RESULTS:
   [[file:~/Desktop/Blog/images/bayesNet5.png]]

#+begin_export html
 <img src="../../images/bayesNet5.png"  style = "width: 30% !important;">
#+end_export

       the question is whether we can treat each likelihood
       separately - that is just depending on the individual CPDs of H
       on X, and H on Y.

       Note that in the network above the likelihood can be
       represented as:
       
#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-08_um_14.49.02.png" class="center">
#+end_export

       When we had complete data, we rewrote the likelihood function
       as a product of local likelihood functions, one for each
       CPD. This decomposition was crucial for estimating each CPD
       independently of the others. In this example, we see that the
       likelihood is a product of sum of products of terms involving
       different CPDs. *The interleaving of products and sums means
       that we cannot write the likelihood as a product of local likelihood functions.*

       Again the /global decomposability/ is lost; this because we do
       not observe the variable H. Hence we cannot decouple the
       estimation of P(X | H) from that of P(Y | H). I.e. both depend
       on how we reconstruct H and observations of both will alter the
       way we construct and parameterize H via its likelihood.

       Hence for the general case it holds that:
       
#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-08_um_14.58.06.png" class="center">
#+end_export

       *Important - important - important:* This has /very serious
       consequences/. It implies that to compute the likelihood
       function we need to *perform inference for each
       instance*. I.e. we need to compute the probability of each of
       the observations. Note that this problem might be intractable
       depending on the network structure and the pattern of the
       missing values.

       So overall the bottom result is that

       #+begin_quote
       in the presence of partially observed data, we have lost all of the
       important properties of our likelihood function: its unimodality, its
       closed form representation, and the decomposability as a product of
       likelihoods for the different parameters. Without these properties,
       the learning problem becomes substantially more complex.
       #+end_quote

**** On the identifiability issue

     A further issue when dealing with partially observed data is the
     one of the identifiability of the model. The idea is that it
     might not be possible to uniquely identify the model from the
     data.

     The issue is well explained in the following two examples in the
     book

#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-08_um_15.16.18.png" class="center">
#+end_export

#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-08_um_15.16.48.png" class="center">
#+end_export

     So  the identifiability difficulties are clear. It is as well
     clear that collecting more data will not be of any help.

     Recall in this sense that a model is identifiable if each choice
     of parameters implies a different distribution over the observed
     variables. Nonidentifiability implies that there are parameter
     settings that are indistinguishable given the data.

     There is then a brief discussion on the non-identifiability in
     the case of hidden data and the definition of *locally
     identifiable*. This is important in that it states that you do not
     have to guarantee identifiability over the entire parameter
     space, but in neighboring regions. This means that you can have
     two different parameterization that lead to the same likelihood
     but they should be far apart so that you can use locally some
     techniques to reach the maximum.

     Finally, note that a nonidentifiable model /does not/ mean that we
     should not attempt to learn models from data. But it does mean
     that we should be careful not to read into the learned model more
     than what can be distinguished given the available data.

     
**** On Parameter Learning

     So far we discussed and clarified the issues that arise when
     working with partially observed data. This especially in relation
     to the difficulty of specifying the likelihood function and the
     fact that this does not decompose in such a neat way as it was
     the case under complete data.

     The question that we will tackle in this section is on how to
     deal with the optimization of the likelihood function given its
     multimodal shape and in general how to deal with the
     computational difficulties we discussed in the previous section.

     We will see in this sense that there are essentially two
     approaches for dealing with the above:

     - the gradient ascent 

     - the expectation-maximization algorithm

***** TODO The (stochastic) gradient ascent

      Skipped as not being the focus of the thesis. Go back to it when
      you have time. You have the session with Radu soon. No time to
      deal with this session in the detail now.

      It is the very well known algorithm though. Very much used in
      optimization even with the /stochastic/ component variant.

***** The expectation-maximization algorithm

      This is also fairly standard. There is a ton of literature on
      this and it is used in many ML problems. Note that in contrast
      to the gradient ascent it is not a general purpose optimization
      algorithm for non-linear functions. It is rather much more
      tailored to the application to the likelihood
      functions. Moreover, it is especially well-suited for dealing
      with exponential families as there solutions are often
      analytically computable.

      You can check the following two videos. They give you an
      intuition on why the EM work and recap quickly why it is used in
      such cases:

   #+begin_export html
   <div class="container"> 
     <iframe class="responsive-iframe" src="https://www.youtube.com/embed/AnbiNaVp3eQ" frameborder="0" allowfullscreen;> </iframe>
   </div>

   <div class="container"> 
     <iframe class="responsive-iframe" src="https://www.youtube.com/embed/X9yF2djExhY" frameborder="0" allowfullscreen;> </iframe>
   </div>
   #+end_export

      Let's turn to the book notes after having refreshed the
      algorithm.

      There the argument proposed is the following.
      #+begin_quote
      When learning with missing data, we are actually trying to solve
      two problems at once: learning the parameters, and hypothesizing
      values for the unobserved variables in each of the data
      cases. Each of these tasks is fairly easy when we have the
      solution to the other. Given complete data, we have the
      statistics, and we can estimate parameters using the MLE
      formulas we discussed previously.

      Conversely, given a choice of parameters, we can use
      probabilistic inference to hypothesize the likely values (or the
      distribution over possible values) for unobserved
      variables. Unfortunately, because we have neither, the problem
      is difficult.
      #+end_quote

      Then the EM proceed iteratively. I.e. you would start with an
      arbitrarily - or maybe a heuristic based - parameter selection
      and you would solve this chicken-egg problem of above
      iteratively. I.e. you would impute the data according to some
      inference step and then based on this you would compute a new
      paramterization as seen in the previously discussed MLE derivation -
      i.e. the one with the sum term.

      As we will show then such iterative method improves the
      parameters, i.e.

      Each of these operations can be thought of as taking an “uphill”
      step in our search space. More precisely, we will show (under
      very benign assumptions) that: each iteration is guaranteed to
      improve the log-likelihood function; that this process is
      guaranteed to converge; and that the convergence point is a
      fixed point of the likelihood function, which is essentially
      always a *local* maximum.

      Concretely in one example the above would look as follows.

      /Example:/

      Consider the following simple meta-network for the example:

#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-12_um_09.43.51.png" class="center" style = "width: 40% !important;">
#+end_export

      As we discussed previously in the case the likelihood in the
      case of fully observable data for the MLE for the global shared
      parameters would look as:

#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-12_um_09.47.23.png" class="center" style = "width: 40% !important;">
#+end_export

      It is therefore clear that in the case of complete data we can
      easily compute the MLE parameters.

      Consider now the case of missing data. For instance $o = [a^1,
      ?, ?, d^0]$.

      Then there are four combinations for the realization of the
      missing variables.

      The idea is that given an initial parameterization you can
      compute the probability of each of this possible realization
      using inference.[fn:3] 

      What you can then actually do is to treat each of the possible
      combination as an actual observation, i.e. an instantiation and
      weight each of this according to the probability with which it
      occurs. You would then have a new /weighted/ complete dataset on
      which you can do standard MLE estimation as discussed above.

      In general the following holds - where instead of sufficient
      statistics we would in fact work with *expected sufficient
      statistics*:

#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-12_um_10.13.31.png" class="center">
#+end_export

      You can then see in the example that when you compute your MLE
      estimates according to the formula above they are quite
      different from the initial parameters. So you took the uphill
      movement discussed, with an =expectation step= the one where you
      weight each possible occurrence - and a =maximization step=,
      where you compute the new parameters. Both are written in one
      shot in the above.

      This intuition seems nice. However, it may require an
      unreasonable amount of computation. To compute the expected
      sufficient statistics, we must sum over all the completed data
      cases. The number of these completed data cases is much larger
      than the original data set. For each o[m], the number of
      completions is exponential in the number of missing values.

      Fortunately, it turns out that there is a better approach to
      computing the expected sufficient statistic than simply summing
      over all possible completions.

      To understand this consider the following expected sufficient
      statistics:

#+Name: expected_suff_stat      
#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-12_um_10.49.13.png" class="center">
#+end_export

      Therefore it is clear from the above that the new formula is
      identical, except that we have substituted the indicator
      variable with a probability that is somewhere between 0 and 1.

      If in a certain data case we get to observe C, the indicator
      variable and the probability are the same. Thus, we can view the
      expected sufficient statistics as filling in soft estimates for
      hard data when the hard data are not available.

      So notice that you use a sort of posterior probability in the
      inference step when computing expected sufficient statistics.

      So in general for the case of =Table CPDs= you can work as
      follows:

      - choose an initial parameterization \theta^0, then execute the
        following phases for t = 0, 1, ...:

#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-12_um_11.07.13.png" class="center">
#+end_export
     
#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-12_um_11.07.37.png" class="center">
#+end_export

      Note that the maximization step is straightforward given the
      expected sufficient statistics.

      The question is rather on how to compute the conditional
      probabilities necessary for computing the expectation term.  For
      this you must resort to one inference technique over the network
      ${\mathscr{G}, \theta^t}$ such as the /clique tree/ or /cluster
      graph/ algorithm. These are not described in these notes. You
      can read about them in the book. Important however is to
      understand that the properties of such algorithms allows us to
      *do all of the required inference for each data case using one
      run of message-passing calibration*.

      Note that the discussion of above does not just hold for the
      case of table-CPDs but it can rather be quickly generalized to
      arbitrary cases, where you have a sufficient statistics that
      characterizes your probability function. Think for instance to
      the case of an exponential family PDF, then it holds:
      
#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-12_um_11.38.44.png" class="center">
#+end_export

      So it is now clear the algorithmic scope of the
      EM-algorithm. The question is on why the convergence property
      applies. We saw in the video above a very rough
      motivation. I.e. it is possible to interpret it as an iterative
      way of finding the necessary condition for an optimum - i.e. the
      fact that the derivative is 0.

***** Mathematics of EM

      We will now proceed and do a more robust analysis of that to
      understand the mathematical motivation of the algorithm.

      The basic idea is the following: at each iteration can be viewed
      as maximizing an /auxiliary function/, rather than the actual
      likelihood function. The crucial part of the analysis is to show
      how the auxiliary function relates to the likelihood function we
      are trying to maximize.

      As we will show, the relation is such that we can show that the
      parameters that maximize the auxiliary function in an iteration
      also have /better likelihood/ than the parameters with which we
      started the iteration.

      It holds therefore:

#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-12_um_12.31.20.png" class="center" style = "width: 30% !important;">
#+end_export

      To understand why this holds and the fact with the /auxiliary
      variable/ consider the following - i.e. start with the following
      setting:
      
#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-12_um_16.08.27.png" class="center">
#+end_export

      Note that in the case of fully observed data, the *score*
      expressing how well a set of parameters is, was the
      log-likelihood. In the case of partially observed data we work
      with the /expected log-likelihood/ as mentioned above, i.e. as
      discussed the *score* would be given by:

#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-12_um_16.12.30.png" class="center">
#+end_export

      From this it follows immediately inserting the log-likelihood
      for the case of table CPDs that was shown before and using the
      linearity of the expectation that:

#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-12_um_16.20.56.png" class="center">
#+end_export

      This expression has precisely the same form as the
      log-likelihood function in the complete data case, but using the
      expected counts rather than the exact full-data counts. The
      implication is that instead of summing over all possible
      completions of the data, we can evaluate the expected
      log-likelihood based on the expected counts.

      The crucial point here is that the log-likelihood function of
      complete data is *linear in the counts*. This allows us to use
      linearity of expectations to write the expected likelihood as a
      function of the expected counts.

      An analogous result arise in the case of exponential
      family. There it is also possible to arrive to a log-likelihood
      function that is linear in the sufficient statistics.

      The idea is now that the two steps are:

      - you compute the expected sufficient statistics under the =Q=
        measure.

      - then maximizing the expected likelihood is the same task as
        the maximization in the case of /complete data/. the only
        difference lies in the fact that you work with the expected
        number counts instead of the actual number of counts.

      You see therefore the classical two steps of the EM algorithm.

      Recall now that in the above the expected sufficient statistics
      is computed as:
      
#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-12_um_16.46.21.png" class="center" style = "width: 60% !important;">
#+end_export

      And that $Q(\mathscr{H})$ is the distribution:

#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-12_um_16.49.16.png" class="center">
#+end_export

      Note that each iteration uses a different $\mathscr{Q}$
      distribution, i.e. a *different parameterization*, and thus we
      cannot relate the optimization taken in one iteration to the
      ones made in the subsequent one.

      However, note that the choice of $\mathscr{Q}(\mathscr{H}) =
      P(\mathscr{H} | \mathscr{D}, \theta^t)$ allows us to prove that
      at each EM iteration we improve the likelihood.

      In order to understand this we create the new /auxiliary
      function/ discussed above. We will let such function be
      dependent both on \theta and on $\mathscr{Q}$ at each step and we
      will optimize this.

      In order to understand the /auxiliary function/ and its relation
      to the log-likelihood, start by considering the *energy
      functional* expressing the total energy in the system. This can
      be expressed as follows:


      $$ F[P, Q] = E_Q [log(\tilde{P})] + H_Q(\mathscr{X}) $$

      where, $P = \frac{\tilde{P}}{Z}$ such that $\tilde{P}$ is the
      unnormalized state probability and $H_Q$ is the negative
      entropy of the observed particles.

      It is then immediate to see that given such energy functional
      function you can re-express the logarithm of the normalizing
      constant Z as follows:

      $$ log(Z) = F[P, Q] + D(Q || P) $$

      where $D(Q || P)$ is the relative entropy or the KL-divergence
      criteria.

      To see this is immediate as:

      $$ F[P, Q] + D(Q || P) = E_Q [log(\tilde{P})] +
      H_Q(\mathscr{X}) + E_Q [log(Q(\mathscr{X}))] -
      E_Q[log(P(\mathscr{X}))]$$

      The result follows by inserting $P = \frac{\tilde{P}}{Z}$ and
      noting that the negative entropy term cancels in the above.

      Inserting now our probability measures for the $P =
      \frac{\tilde{P}}{Z}$ expression, and noting the consequences of
      this we can come to important appreciations that will lead us to
      understand the key results of this section, i.e. the /relation
      between the auxiliary function and the log-likelihood/:
      
#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-13_um_10.23.37.png" class="center">
#+end_export

      Note that the above is central piece of information.

      From the last corollary, i.e. the last two equalities we can
      derive important understandings.

      /Key Insights from the second equality:/

      I.e. by the fact that both the entropy H_Q is non-negative (log
      of 0-1) as well as the relative entropy is non-negative by the
      same argument (ratio of two positive numbers) we know that the
      *log-likelihood is a lower bound on the expected log-likelihood
      relative to Q for any choice of Q*.

      Moreover note the important fact that selecting $Q(\mathscr{H})
      = P(\mathscr{H} | \mathscr{D}, \theta)$ the relative entropy
      term in the second equality becomes 0. Then you are just left
      with the entropy term that in such case is the overall measure
      on the difference between the expected log-likelihood and the
      real log-likelihood.

      /Key Insights from the first equality:/

      Here it is possible to see that the energy functional is a lower
      bound for the log-likelihood. This for the same argument for the
      relative entropy as above.

      Moreover note, that when choosing - as in our case - $Q(\mathscr{H})
      = P(\mathscr{H} | \mathscr{D}, \theta)$ then the *energy
      functional and the log-likelihood are equal*. This practically
      means that when optimizing the energy functional you are
      maximizing the log-likelihood as well.

      The question is then on how to optimize the energy functional.

      We will next see that with the EM algorithm you are actually
      optimizing the energy functional. We will see next the way
      through which such an optimization is performed.

      We will in fact show that the EM algorithm optimizes the energy
      functional F through a /coordinate ascent/ optimization (same as
      Gibbs sampling - optimize the function in one axis at the
      time). We start with some choice θ of parameters. We then search
      for Q that maximizes $F[\theta, Q]$ while keeping \theta
      fixed. Next, we fix Q and search for parameters that maximize
      $F[\theta, Q]$. We continue in this manner until convergence.

      In order to do that consider the following:

      *Step 1 - Optimization of Q:*
      
#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-13_um_12.02.34.png" class="center">
#+end_export

      Note that the mentioned corollary above is the one at the end of
      the previous pic. The intuition is the one expressed in the
      reasonings above for the first equation. You have in general a
      upper bound on F given by log-likelihood. If you now choose the
      distribution Q in the way described above you know that you have
      reached the upper bound and that such upper bound is
      tight. I.e. it is straightforward to see that your are at the
      maximum for a *given \theta*.

      Note that when computing $P(\mathscr{H} | \mathscr{D},
      \theta^t)$ by which you are going to weight your observations
      you are in your E-step, i.e. in your expectation step. Such that
      it is possible to conclude by saying that :

      #+begin_quote
      we can view the E-step as implicitly optimizing Q by using
      $P(\mathscr{H} | \mathscr{D}, \theta^t)$ in computing the
      expected sufficient statistics.
      #+end_quote

      *Step 2 - Optimization of \theta:*

#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-13_um_12.12.08.png" class="center">
#+end_export

      I.e. this second step is exactly the maximization step of the EM
      algorithm.

      *So it is generally clear that the EM steps corresponds to two
      coordinate ascent optimization procedures.*

      And generally

      #+begin_quote
      We conclude that EM performs a variant of hill climbing, in the
      sense that it improves the log-likelihood at each
      step. Moreover, the M-step can be understood as maximizing a
      lower-bound on the improvement in the likelihood. Thus, in a
      sense we can view the algorithm as searching for the largest
      possible improvement, when using the expected log-likelihood as
      a proxy for the actual log-likelihood.
      #+end_quote

      *Beautiful! You now have an understanding of why the EM algo
      works!*

      
***** Hard-assignment EM

      The hard-assignment EM, also iterates over two steps: one in
      which it completes the data given the current parameters
      \theta^t , and the other in which it uses the completion to
      estimate new parameters \theta^t+1.

      However, rather than using a soft completion of the data, as in
      standard EM, it selects for each data instance o[m] the *single
      assignment* h[m] that maximizes $P(h | o[m], \theta^t)$.

      Although hard-assignment EM is similar in outline to EM, there
      are important differences. In fact, hard-assignment EM can be
      described as optimizing a different objective function, one that
      involves /both/ the learned parameters /and the learned assignment/
      to the hidden variables.

      You can refer to the book for more. However, note the following
      important property, for instance in the case of using EM
      algorithm for performing bayesian clustering:

      #+begin_quote
      Although hard-assignment EM is similar in outline to EM, there
      are important differences. In fact, hard-assignment EM can be
      described as optimizing a different objective function, one that
      involves both the learned parameters and the learned assignment
      to the hidden variables.
      #+end_quote

**** Uncertain Evidence

     This is the topic of the thesis. Essentially the idea is that now
     you are not dealing with the case of totally unobserved or
     missing data. You rather have an uncertain evidence. This means
     an evidence that comes with a probability assigned to it.

     The algorithm is quite intuitive and we will deal with it in this
     section. It is a natural extension of the material covered that
     far.

     The idea is the one of using [Pearl 1998][fn:4] to extend the
     bayesian network with new nodes with CPDs representing the
     likelihood evidence gained from uncertain data.

     I.e. this is best explained on an example - consider the ASIA
     network as in the paper of reference.
     
#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-14_um_09.39.27.png" class="center">
#+end_export

     Then consider that you have doctor handwritings stating whether a
     person had =Dyspoea= or not. Such writings are analyzed via NLP
     such that whether a patient had Dyspoea or not is just associated
     with a *likelihood*. This is in fact the case of /uncertainty/
     evidence we are interested in. I.e. you do not have a missing
     observation but also do not know its value precisely.

     The idea is then that you can convert such structure into a new
     Bayesian Network where you either *observe* or have *missing*
     evidence. However, you do not have uncertain evidence. In such a
     way you can then effectively treat the network in the usual way
     as it would ultimately fall in the classical structure.

     In order to reach such a structure what you do as in Pearl 1998
     is to extend the network by adding a node
     =Variable_Observed=. You would then assign to this variable the
     realization =True= i.e. observed, and that would kick in with the
     given likelihood given the parent (i.e. kick in with the
     likelihood arising from the NLP model). This is quite
     straightforward to understand and you can quickly understand then
     that like this the situation is well mapped.

     To make this 100% clear for when you are coming back in a couple
     of time. Consider the ASIA network above. Consider you have NLP
     that reads the doctor handwritings for the =Dyspoea= case. With
     0.7% the NLP returns =Dyspoea = True= when Dyspoea occurs.

     Then you can you construct your network as follows.

#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-14_um_10.18.40.png" class="center">
#+end_export

     With this modification you can therefore treat any uncertain
     evidence and convert it to standard Bayesian Networks with just
     observed or missing evidence.

     We will now show that it is possible to apply the EM algorithm to
     such augmented network.

     Consider the EM algorithm in the case of a Bayesian Network
     without uncertain evidence. Then the situation would look as
     follows:
     
#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-14_um_10.33.42.png" class="center">
#+end_export

     So you see that it basically consists in the expectation step
     where you compute the expected sufficient statistics as
     previously described in the [[expected_suff_stat][expected_suff_stat]] image above. And
     the maximization step where you essentially take the MLE for your
     distribution. So basically nothing new compared to what was
     discussed above. We simply put this in an algorithm frame.

     For the case of *uncertain evidence* this looks very similar, you
     just have to augment the algorithm with a step where you would
     actually extend and reframe the Bayesian Network to account for
     uncertain evidence as previously done.

     In algorithmic terms we would do the following:
     
#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-14_um_10.57.39.png" class="center">
#+end_export

     So basically it is 1:1 the same after augmenting the
     network. 

     The only thing that is left over is to show that the properties
     of the EM algorithm are not altered by the Network expansion,
     i.e. we have to guarantee the same properties for the EM such
     that such coordinate ascent argumentation holds and we will
     /converge to a global maximum/.

     This is possible to see by noting that again you can easily show
     that the *log-likelihood* is *linear in the sufficient
     statistics*. Given this condition as discussed it is then
     possible to insert the expected sufficient statistics over the
     measure $\mathscr{Q}(\mathscr{H}) = P(\mathscr{H} | \mathscr{D},
     \theta^t)$ and we will have all of the desired properties.

     For the MLE you would then insert the MLE solution for the
     parameters. This amounts in the case of *table CPD* to:
     
#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-03-14_um_12.42.15.png" class="center" style = "width: 30% !important;">
#+end_export

     
     
     

** MLE for Exponential Family as M-Projection

   Note that in the book of Koller the notation is different and I
   like it less. It is for the discrete case and not for the
   continuous.

   Have to double check at the notation in there, but that is
   essentially a 1:1 with your notes of mathematical statistics.

   Note that there is the important property of non-redundancy of the
   parametric exponential family. This goes hand in hand with the
   invertibility of the function $t(\theta)$. This is in your math
   statistics notes the $c(\theta)$.
   
#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-05-07_um_14.02.20.png" class="center">
#+end_export

   Exponential Family in canonical form is termed in /natural form/ in
   the book.

   This gives moreover rise to the idea of /natural parameter
   space/. This and the correspondingly defined /linear exponential
   family/ are defined as follows:

#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-05-07_um_14.27.09.png" class="center">
#+end_export

   Note, that sometimes the exponential family distribution cannot be
   written in the linear form. In such cases additional machinery is
   required, and we leave this case as untreated.

   Note that due to the mathematical form of the exponential such that
   when multiplying two exponent you add the terms in a single
   exponent the multiplication of multiple exponential family CPDs
   give rise to an exponential family distribution.

   This in more rigorous words and notation:
   
#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-05-07_um_14.58.36.png" class="center">
#+end_export

   Then you just have to be careful when chaining such results in such
   a way. Note that if you do not chain normalized CPDs (with the
   partition function Z(\theta) [in mathstat notes d (\theta)]), then
   you have to find a partition function to normalize the unnormalized
   CPD multiplication. Otherwise work with locally normalized CPDs.

   As it is not possible to find a global partition function in a
   general form that normalizes under all of the realizations, it is
   suggested that you work by locally normalizing distribution
   functions.
   
#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-05-07_um_15.30.00.png" class="center">
#+end_export

   This is the usual $-d(\theta)$ in your mathstat notes. So again
   nothing new under the sun.

   Note however that albeit the simple strategy above defines an
   exponential family distribution for the overall network, this does
   not have to be in general a linear exponential distribution. So
   watch out in generally for that and double check you case and make
   explicit the case you are treating.

   Note that once you have a properly defined exponential family
   distribution you can calculate its in entropy in a very neat and
   quick way, as follows:

   - First, recall the formula for the entropy function:

#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-05-07_um_15.59.23.png" class="center" style = "width: 30% !important;">
#+end_export

  Then given this definition it follows immediately that:
   
#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-05-07_um_16.03.20.png" class="center">
#+end_export

   Note that the above defines entropy for the general distribution
   over the entire network. This requires the partition function over
   the entire network and as we mentioned before this might not be
   easy to compute.

   In this sense, as before it makes sense to compute the entropy via
   the local CPD entropies. This can also easily done in Bayesian
   Networks as the following holds:
   
#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-05-07_um_16.32.24.png" class="center">
#+end_export

   So the entropy of the network decomposes as the sum of conditional
   entropies.

   Note however that you cannot do the entropies calculation locally,
   as there information *about the entire network is encoded*.

   This can be seen from the following image. You need inference over
   the network to get the probability of the realization of the
   parents values. For this you have to make your calculations over
   the entire network.
   
#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-05-07_um_16.38.43.png" class="center">
#+end_export

   /Relative Entropy:/

   It follows now that for an arbitrary distribution $Q$ and an
   exponential distribution P, that

#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-05-07_um_17.23.32.png" class="center" style = "width: 60% !important;">
#+end_export

   To see this think of the prove of proposition 16.1 above.

   It follows immediately by combining the above results that:
   
#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-05-08_um_09.55.26.png" class="center">
#+end_export

   Given all of that reasoning and machinery, we turn to the topic of
   distribution projections.

*** Distribution Projections

    Recall that the relative entropy is a measure for the distance
    among two distributions.

    We can therefore use this measure as a tool for finding the
    distribution, within a given exponential family, that is closest
    to a given distribution. This will be our /projection/ task.

    This leads us to the following /definitions/.
    
#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-05-08_um_10.03.35.png" class="center">
#+end_export

    Note the difference between the =M= and =I= projection.

    The =M= projection is defined as:

#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-05-08_um_10.28.29.png" class="center">
#+end_export

    Such that you can see that the only term depending on the
    distribution of interest =Q= is the second one.

    Such that, given the properties of the natural logarithm function,
    we see that here we aim to have high density Q, so that =-ln(Q)=,
    is as small as possible - close to 0 - in regions with high
    probability P. And at the same time we have to watch out not to
    give low probability $Q(X)$, in regions with non-negligible
    probability P, as there you potentially add up infinity.

    Note that the situation for =I-projections= are different:
    
#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-05-08_um_10.33.15.png" class="center">
#+end_export

    Overall, you will have the following pattern: The *M-projection*
    attempts to give all assignments reasonably high probability,
    whereas the *I-projection* attempts to focus on high-probability
    assignments in P while maintaining a reasonable entropy.

    Note that for the projections above some general characteristics
    hold. This is important for characterizing your data:
    
#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-05-09_um_08.46.48.png" class="center">
#+end_export

    This basically says that in order to find the M-projection to an
    exponential family distribution, you can work directly with the
    sufficient statistics, and more specifically with the /expected
    sufficient statistics/.

    Given this the problem of finding the M-projection is just as
    difficult as the *exact inference problem* and the computation of
    the expected sufficient statistics which you would have to invert
    then.

    That in a bit of text.
    
#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-05-09_um_08.54.44.png" class="center">
#+end_export

    By comparing the expected sufficient statistics of $P$ to these of
    distributions in $Q$, we can find the M-projection.

    Then the schema is to find the $ess(\theta)$ map and the
    corresponding $E_{Q_{\theta}}[\tau(X)]$. Then check whether
    $E_{P}[\tau(X)]$ is an image of the map above. You would then
    invert the map to find the parameters of interest.

    
*** MLE as M-projection

    Given the discussion above, it is interesting to note that you can
    interpret the MLE as an M-projection, and find the MLE
    parameterization by doing an M-projection exercise and inverting
    the expected sufficient statistics as discussed above.

    In order to see that the MLE is nothing else than a special case
    of an M-projection look at the following:
    
#+begin_export html
 <img src="../../images/Bildschirmfoto_2021-05-09_um_09.48.13.png" class="center">
#+end_export

    It follows immediately that if $\hat{P}_{D}$ is parameterized
    according to an exponential family, and if the mapping of ess from
    parameters to sufficient statistics is invertible, we can simply
    take the sufficient statistic vector from $\hat{P}_{D}$, and
    invert this mapping to produce the MLE.

    
      
* Footnotes

[fn:4] Pearl, J. 1988. Probabilistic Reasoning in Intelligent Systems:
Networks of Plausible Inference. San Francisco, CA, USA: Morgan
Kaufmann Publishers Inc. ISBN 1558604790.

[fn:3] See the textbook for the exact computation of this. It is
quite straightforward - just an application of the bayes formula.

[fn:2]Note this again similarly to the MLE case. 

[fn:1]Note that With this you would then have in these notes both a
      way to write down the likelihood in compact form via sufficient
      statistics in the case of multinomial parameterization arising
      from the MLE. As well as a way to write in compact form the
      posterior distribution in the case of working with bayesian
      parameter learning. Note that both assume you are ready to
      accept the described prior/parameter space settings.
