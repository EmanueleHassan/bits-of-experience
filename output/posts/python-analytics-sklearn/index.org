#+BEGIN_COMMENT
.. title: Python Analytics - Sklearn
.. slug: python-analytics-sklearn
.. date: 2023-06-06 16:38:45 UTC+02:00
.. tags: Python, Analytics
.. category: 
.. link: 
.. description: 
.. type: text

#+END_COMMENT


This is a repository where I hold some notes about the major sklearn
analytics algorithm in order to perform your data science. 

It is essentially an exercise into the [[https://scikit-learn.org/stable/modules/classes.html][sklearn API]]. 

If you have it well in mind than you have well in mind the major ML
algorithms, the way to preprocess the data and the way create robust
pipelines.


{{{TEASER_END}}}


* General Pattern
   :properties:
   :header-args:ein-python: :session http://127.0.0.1:8888/sklearn.ipynb  :results output
   :end:


  The standard way of working with your data is first performing the
  relevant exploratory data.

  For it you can refer to the relevant separate post.

  There you would inspect the data both analytically as well as
  visually.
  
  The goal is to understand what is available in the data.

  The distribution of the data etc. 

  In such a way it would be possible for you to create the relevant
  features from the data. 

  When creating the features it will be important to *transform* the
  data such that they take a meaningful shape. 

  You might need to one-hot-encode, standardize, impute etc.  

  Note that you might fit data as well in order to get to the
  transform the data accordingly. In this sense, you will find a
  =fit_transform= API in sklearn.

  After the data have been properly transformed it is time to fit the
  actual analytical model. Here you would take one of the relevant
  *supervised* / *unsupervised* models and you would ultimately fit
  them to the main chunck of the data.

  Finally, you would estimate / predict a new existing chunck of the
  data given your fitted model and its parameters.


* The preprocessor object

  In order to pre-process the data sklearn offers the preprocessor
  object class.

  There you have many different APIs that are useful in order to
  process the data.

  We will explore them next.

** Dataset

   In this section I will work with the datasets shipped with the
   sklearn package.

   In order to use the data use the following:

   #+NAME: 87f87de0-fcdc-483a-a400-2b6fa82af4fa
   #+begin_src ein-python
   from sklearn.datasets import fetch_california_housing as fdh
   import pandas as pd
   import numpy as np
   #+end_src

   #+RESULTS: 87f87de0-fcdc-483a-a400-2b6fa82af4fa

   #+NAME: a31fbf10-1899-4744-aeb3-989fd423ac6f
   #+begin_src ein-python
   df_housing = fdh (as_frame=1) ['data']

   df_housingValue = fdh (as_frame=1)['target']

   df_housing.head (5)
   #+end_src

   #+RESULTS: a31fbf10-1899-4744-aeb3-989fd423ac6f
   #+begin_example
      MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \
   0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   
   1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   
   2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   
   3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   
   4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   

      Longitude  
   0    -122.23  
   1    -122.22  
   2    -122.24  
   3    -122.25  
   4    -122.25  
   #+end_example


    
** Binarizer

   This converts data into binary given a certain threshold. 

   You could for instance create a future in the above for checking
   whether the data are in the northern hemisphere or in the
   southern one in the following way:

   #+NAME: 1ce8fb40-36fb-497d-8775-beb6ca97c69d
   #+begin_src ein-python
   from sklearn.preprocessing import Binarizer
   #+end_src

   #+RESULTS: 1ce8fb40-36fb-497d-8775-beb6ca97c69d

   #+NAME: d1830c22-8602-4450-9478-3d4cac05d8e9
   #+begin_src ein-python
   binary = Binarizer () ## note that it holds a parameter threshold. The
			 ## default is 0 and is good for the exercise of
			 ## Northern and Southern Hemisphere.

   binary.fit (np.array (df_housing ["Latitude"]).reshape (-1,1)) ## Note
								  ## that
								  ## the
								  ## Binarizer
								  ## takes
								  ## a 2D
								  ## array
								  ## as
								  ## input. You
								  ## therefore
								  ## need
								  ## to
								  ## reshape
								  ## your
								  ## data
								  ## and
								  ## wrap
								  ## your
								  ## array
								  ## into
								  ## a
								  ## list.
   #+end_src

   #+RESULTS: d1830c22-8602-4450-9478-3d4cac05d8e9
   : Binarizer()


   #+NAME: a9acb8b3-2502-455f-97ac-6a1274fa99cf
   #+begin_src ein-python
   df_housing ["Northern Hemisphere"] = 
      binary.transform (np.array
			(df_housing
			 ["Latitude"]).reshape
			(-1,1)).
      flatten() ## Flatten out back to a 1D array
   #+end_src

   #+RESULTS: a9acb8b3-2502-455f-97ac-6a1274fa99cf


   #+NAME: e48ed517-58b9-401d-9635-81e5c61c87e6
   #+begin_src ein-python
   df_housing.head (5)
   #+end_src

   #+RESULTS: e48ed517-58b9-401d-9635-81e5c61c87e6
   #+begin_example
      MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \
   0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   
   1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   
   2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   
   3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   
   4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   

      Longitude  Northern Hemisphere  
   0    -122.23                  1.0  
   1    -122.22                  1.0  
   2    -122.24                  1.0  
   3    -122.25                  1.0  
   4    -122.25                  1.0  
   #+end_example



   Note that you can specify the threshold yourself. Think about the
   case of Housing age. You want to create a binary if older than
   the age or not.

   You can then use the following:

   #+NAME: d8c07ee6-c88e-461b-8233-6147d0639d3d
   #+begin_src ein-python
   df_housing ["OldHouse"] = Binarizer (threshold= df_housing.HouseAge.mean ()) \
                             .fit_transform (np.array (df_housing ["HouseAge"]).reshape (-1,1)).flatten ()
   #+end_src

   #+RESULTS: d8c07ee6-c88e-461b-8233-6147d0639d3d


   #+NAME: 0a64fcf6-c533-4afc-bd11-737f2c0d6052
   #+begin_src ein-python
   df_housing.head (10)
   #+end_src

   #+RESULTS: 0a64fcf6-c533-4afc-bd11-737f2c0d6052
   #+begin_example
      MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \
   0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   
   1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   
   2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   
   3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   
   4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   
   5  4.0368      52.0  4.761658   1.103627       413.0  2.139896     37.85   
   6  3.6591      52.0  4.931907   0.951362      1094.0  2.128405     37.84   
   7  3.1200      52.0  4.797527   1.061824      1157.0  1.788253     37.84   
   8  2.0804      42.0  4.294118   1.117647      1206.0  2.026891     37.84   
   9  3.6912      52.0  4.970588   0.990196      1551.0  2.172269     37.84   

      Longitude  Northern Hemisphere  OldHouse  
   0    -122.23                  1.0       1.0  
   1    -122.22                  1.0       0.0  
   2    -122.24                  1.0       1.0  
   3    -122.25                  1.0       1.0  
   4    -122.25                  1.0       1.0  
   5    -122.25                  1.0       1.0  
   6    -122.25                  1.0       1.0  
   7    -122.25                  1.0       1.0  
   8    -122.26                  1.0       1.0  
   9    -122.25                  1.0       1.0  
   #+end_example


** QuantileTransformer

   Note that this estimates the quantiles according to a parameteric
   distribution that you impose on the data. 

   It does not compute the quantiles based on the empirical
   distribution of the data. Check in this sense the =qcut= method.

   Note that this preprocessor function allows to estimate and fit
   the quantiles of a uniform and normal distribution.

   #+NAME: 15414542-d55a-4bb3-b0f5-9b69acdd78ce
   #+begin_src ein-python
   from sklearn.preprocessing import QuantileTransformer
   #+end_src

   #+RESULTS: 15414542-d55a-4bb3-b0f5-9b69acdd78ce

   So note that the exercise is different in comparison to the
   quantile cut.

   The idea is to compute =n_quantiles= quantiles from the actual
   data. And then interpolate across them according to a
   distribution of choice - be uniform or normal - in order to
   compute the rest of the quantiles.


*** Plot histogram of the data

    In order to properly understand the operation we first plot the
    histogram.

    From it you can immediately infer that the data is log-normally
    distributed.

    #+NAME: eef5c3a1-28ca-4c8a-8dfc-d15c09f076c1
    #+begin_src ein-python
    import matplotlib.pyplot as plt
    %matplotlib inline 
    %config InlineBackend.figure_format = 'png'
    #+end_src

    #+RESULTS: eef5c3a1-28ca-4c8a-8dfc-d15c09f076c1

    #+begin_src ein-python
    plt.hist(df_housingValue, bins = 20)
    #+end_src

    #+begin_export html
     <img src="../../images/HistogramHousing.png" class="center">
    #+end_export

    We check next to the quantiles and the QuantileTransformer in
    order to properly understand what operations the two perform on
    the data.

*** Quantile Transformation
     
    Note that weird distribution with heavy outliers or skewed
    distribution might be sub-optimal in order to fit statistical
    models to your data.

    This because they the parameter estimation might be biased by
    such properties of the data.

    In this sense it makes often sense to inspect the data and
    create new features that are less likely to bias your results.

**** Plot the relevant transformer


***** TODO go over again in a better fashion over the data.

      It imposes a distribution on the data. It estimates the
      parameters of the distribution, given the empirical
      distribution.

      It then computes the quantiles for the data values given that
      assumed distribution. 

      If you plug then the values in the quantile transforming
      function you would get back normalized values.

      It transforms the data x in quantiles such that they fit the
      relevant distribution.

      Note that even such transformation has limits. See the
      histogram plot that you created.
       

      It is a transformation that tries to achieve a uniform
      distribution of the assigned quantiles.
       
      #+NAME: bad26289-edea-4cdf-8eba-21f037c92b6c
      #+begin_src ein-python
      plt.hist (QuantileTransformer(n_quantiles=5,
				    random_state=0).
		fit_transform (np.array (df_housingValue).
			       reshape(-1,1)).
		flatten (), 20) 
      #+end_src

      #+RESULTS: bad26289-edea-4cdf-8eba-21f037c92b6c
      : array([0.94939597, 0.84893734, 0.84210487, ..., 0.18568402, 0.16742813,
      :        0.17871796])


** Standard Scaler



** MaxAbsScalerp



** LabelEncoder



** OneHotEncoder



** KernelCenterer



** FunctionTransformer



** FunctionTransformer


   

* Linear Regression
   :properties:
   :header-args:ein-python: :session http://127.0.0.1:8888/visualization.ipynb  :results output
   :end:

** Data

   #+NAME: 17ebf434-7d41-4b45-9dd1-e6fb5ef59209
   #+begin_src ein-python :results output
   import pandas as pd
   import numpy as np
   from matplotlib import pyplot as plt

   # Generate 'random' data
   np.random.seed(0)
   X = 2.5 * np.random.randn(100) + 1.5   # Array of 100 values with mean = 1.5, stddev = 2.5
   res = 0.5 * np.random.randn(100)       # Generate 100 residual terms
   y = 2 + 0.3 * X + res                  # Actual values of Y

   # Create pandas dataframe to store our X and y values
   df = pd.DataFrame(
       {'X': X,
	'y': y}
   )

   # Show the first five rows of our dataframe
   df.head()
   #+end_src

   #+RESULTS: 17ebf434-7d41-4b45-9dd1-e6fb5ef59209
   :           X         y
   : 0  5.910131  4.714615
   : 1  2.500393  2.076238
   : 2  3.946845  2.548811
   : 3  7.102233  4.615368
   : 4  6.168895  3.264107


** Getting the Regression Coefficients Manually via Analytical Solution

   Get the relevant coefficients of the regression:

   #+NAME: 3537126f-b6ea-41a6-b110-2b56ed1617fb
   #+begin_src ein-python
   # Calculate the mean of X and y
   xmean = np.mean(X)
   ymean = np.mean(y)

   # Calculate the terms needed for the numator and denominator of beta
   df['xycov'] = (df['X'] - xmean) * (df['y'] - ymean)
   df['xvar'] = (df['X'] - xmean)**2

   # Calculate beta and alpha
   beta = df['xycov'].sum() / df['xvar'].sum()
   alpha = ymean - (beta * xmean)
   print(f'alpha = {alpha}')
   print(f'beta = {beta}')
   #+end_src

   #+RESULTS: 3537126f-b6ea-41a6-b110-2b56ed1617fb
   : alpha = 2.0031670124623426
   : beta = 0.3229396867092763


   #+NAME: f9157557-cbbe-4693-bd50-a8f632a6aa4a
   #+begin_src ein-python
   ypred = alpha + beta * X
   #+end_src

   #+RESULTS: f9157557-cbbe-4693-bd50-a8f632a6aa4a

*** Matplotlib of regression

    Plot the relevant results:

    #+NAME: e3d98987-6807-44e7-9b95-e01cf937c429
    #+begin_src ein-python :results output
    import matplotlib.pyplot as plt
    %matplotlib inline 
    %config InlineBackend.figure_format = 'png'
    #+end_src

    #+RESULTS: e3d98987-6807-44e7-9b95-e01cf937c429


    #+NAME: 455a46f9-0d68-4848-824a-84c00dd9b5e0
    #+begin_src ein-python
    # Plot regression against actual data
    plt.figure(figsize=(12, 6))
    plt.plot(X, ypred)     # regression line
    plt.plot(X, y, 'ro')   # scatter plot showing actual data
    plt.title('Actual vs Predicted')
    plt.xlabel('X')
    plt.ylabel('y')

    plt.show()
    #+end_src

    #+RESULTS: 455a46f9-0d68-4848-824a-84c00dd9b5e0

    #+begin_export html
     <img src="../../images/Regression.png" class="center">
    #+end_export


** Model Fit and Prediction

*** The standard way to fit and predict in skit-learn

    #+NAME: 98d57a2b-5960-451c-bc88-3db23cd5a934
    #+begin_src ein-python
    from sklearn.linear_model import LinearRegression

    # Initialise and fit model
    lm = LinearRegression()
    model = lm.fit(X.reshape(-1, 1), y)
    #+end_src

    #+RESULTS: 98d57a2b-5960-451c-bc88-3db23cd5a934


    #+NAME: a5acf1cd-907a-4baf-b8c8-908569aafdfd
    #+begin_src ein-python
    print(f'alpha = {model.intercept_}')
    print(f'betas = {model.coef_}')
    #+end_src

    #+RESULTS: a5acf1cd-907a-4baf-b8c8-908569aafdfd
    : alpha = 2.003167012462343
    : betas = [0.32293969]


    #+NAME: e41c951f-cc09-422f-a886-e0191537c10b
    #+begin_src ein-python
    model.predict(X.reshape(-1, 1))
    #+end_src

    #+RESULTS: e41c951f-cc09-422f-a886-e0191537c10b
    #+begin_example
    array([3.91178282, 2.81064315, 3.27775989, 4.29675991, 3.99534802,
	   1.69857201, 3.25462968, 2.36537842, 2.40424288, 2.81907292,
	   2.60387001, 3.66168312, 3.10199975, 2.58581077, 2.84592918,
	   2.75696825, 3.69382011, 2.32194218, 2.74033151, 1.79802302,
	   0.42642221, 3.015275  , 3.18547843, 1.88839019, 4.32006116,
	   1.31339555, 2.52451965, 2.33645381, 3.72506464, 3.67386219,
	   2.61267323, 2.79288576, 1.77082341, 0.88838207, 2.20668994,
	   2.61380476, 3.48085076, 3.45831697, 2.17486854, 2.24351265,
	   1.64102813, 1.34112617, 1.11002064, 4.06253353, 2.07610925,
	   2.1338976 , 1.47613319, 3.11528277, 1.18459738, 2.31582084,
	   1.76462232, 2.79994197, 2.07517841, 1.53439407, 2.46482364,
	   2.83338994, 2.54127917, 2.73177699, 1.9754571 , 2.19471775,
	   1.94466613, 2.19729158, 1.83108353, 1.09386364, 2.6308214 ,
	   2.16319902, 1.17143718, 2.86120343, 1.75506992, 2.52951462,
	   3.07620724, 2.59171079, 3.40747079, 1.49064088, 2.81240675,
	   1.93469565, 1.78453915, 2.02024272, 2.23604485, 2.53292159,
	   1.54689373, 3.2148581 , 2.86352875, 1.24729141, 3.68911579,
	   4.01822118, 3.43926331, 2.34231437, 1.62310525, 3.33888732,
	   2.16207195, 3.47451661, 2.65572718, 3.2760653 , 2.77528867,
	   3.05802784, 2.49605373, 3.92939769, 2.59003892, 2.81212234])
    #+end_example





