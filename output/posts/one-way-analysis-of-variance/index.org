#+BEGIN_COMMENT
.. title: One-way Analysis of Variance
.. slug: one-way-analysis-of-variance
.. date: 2019-09-22 15:22:52 UTC+02:00
.. tags: ANOVA
.. category: 
.. link: 
.. description: 
.. type: text
.. has_math: yes
.. status: 
#+end_COMMENT

#+LATEX_HEADER: \usepackage{math}

#+BEGIN_HTML
<br>
<br>
#+END_HTML

This post discusses the comparison of different effects among
different groups. It will start with a refresh of the /t-test/ 
used for /pair-wise/ comparison among different groups and it extends
the concept for situations with $groups \geq 2$ in order to check
whether results are statistically significant among them.

*** T-test

As a brief reminder it is possible to compare the means of two
independent groups using a /two sample t-test/. 

The basic idea of the test is that assuming i.i.d. observation from
two different groups $X_1$ and $X_2$ it is possible to compute summary
statistics for the two.

It is then possible to prove, that the computed mean from the
i.i.d. observation converges in probability to the normal distribution
due to the /Law of the Large Numbers/ and the /Central Limit
Theorem/. 

#+BEGIN_src latex :results drawer :exports results
 \[ \bar{X} \overset{p}{\to} N (\mu, \sigma^2) \]
#+END_src

#+RESULTS:
:RESULTS:
\[ \bar{X} \overset{p}{\to} N (\mu, \sigma^2) \]
:END:

Given the further fact that the linear combination of normal
distributed variables is normal distributed it follows that the
difference among the mean value of the groups of choice $X_1$ and
$X_2$ is as well normal distributed with

#+BEGIN_src latex :results drawer :exports results
 \[ \bar{X}_1 - \bar{X}_2 \overset{p}{\to} N (\mu_1 - \mu_2, \sigma_1^2 + \sigma_2^2) \]
#+END_src

#+RESULTS:
:RESULTS:
\[ \bar{X}_1 - \bar{X}_2 \overset{p}{\to} N (\mu_1 - \mu_2, \sigma_1^2 + \sigma_2^2) \]
:END:

where the variance does not display the covariance between the two
group observation as these were assumed to be independent.

This is essentially the basic theory on which the t-test relies. The
difference in the mean of two groups of choice is then simply
counterpoised to the Null $H_0$ and the normal distribution is
replaced by the t-distribution due to the final sample of the
observations.

#+BEGIN_HTML
<br>
#+END_HTML


*** The $n \geq 2$ case

In case of existing groups $n \geq 2$ it is not possible to directly
apply the t-test outlined above. It is therefore necessary to expand
the theory developed so far.

Consider therefore /m/ different groups containing each /n/
observations. Assuming moreover that for each of the /m/ different
groups we observed a $N(\mu_i, \sigma)$ response variable it is clear
that we observe /m + 1/ parameters, because of the /m/ different means
and the common variance shared among the groups.

Notice now that albeit the different groups will display different
means it is possible to rewrite their response variable disentangling
the different effects affecting the response variables; namely in:

- a common component \mu shared among all the response variables in
  the groups different groups

- a group specific \alpha_i component adjusting the effect of the
  common component \mu to reflect the information of the group
  specific mean \mu_i

- a stochastic component \epsilon_{ij} ~ N (0, \sigma^2). 

It follows that 

#+BEGIN_src latex :results drawer :exports results
 \[ Y_{ij} = \mu + \alpha_i + \eplison_{ij} \].
#+END_src

Notice however how an additional parameter was introduced. /m + 2/
parameters result from the above equation. /We silently introduced an
additional parameter/ with the result that the problem is not
/identifiable/ anymore given that we currently have /m+1/ parameters
to mode /m/ means. 

In order to address the issue it is therefore necessary to add a side
constant in order to add a further equality that will restore the
/well specification/ of the problem. 

It is common to apply one of three major /side-constrains/:

(notice that the table below is not well rendered in nikola. read the
[[http://eyesfreelinux.ninja/posts/nikola-plugins.html][following post]] or consider using latex tables.)


| Name                 | Side-Constant                   | Interpretation of \mu              | ~R-code~         |
|----------------------+---------------------------------+------------------------------------+------------------|
| sum-to-zero          | \sum \alpha_i^{m}_{i=1}  = 0    | \mu = $frac{1}{m}$ \sum \mu_i      |                  |
|                      |                                 |                                    |                  |
| reference group      | \alpha_1 = 0                    | \mu = \mu_1                        | ~contr.sum~      |
|                      |                                 |                                    |                  |
| weigthed sum-to-zero | \sum^{m}_{i=1} n_i \alpha_i = 0 | \mu = $\frac{1}{N}$ \sum n_i \mu_i | ~contr.tratment~ | 

Notice therefore that after adding the side-constant only /g-1/
elements of the groups are allowed to vary freely and therefore we
have /g-1/ degrees of freedom.

#+BEGIN_HTML
<br>
#+END_HTML

*** Variance Decomposition

Given the decomposition above it is possible to set up the general
framework for comparing the different effect of different treatments
groups.

First of all we notice that we estimate the /g-1/ parameters of
interest by the *least square method*.

It follows therefore 

#+BEGIN_src latex :results drawer :exports results
\[ \hat{\mu} \textasciicircum{\alpha_i} = argmin_{\mu, \alpha_i} \sum^{m}_{i=1} \sum^{n}_{j=1} (y_{ij} - \mu - \alpha_i)^2   \]
#+END_src

#+RESULTS:
:RESULTS:
\[ \hat{\mu} \textasciicircum{\alpha_i} = argmin_{\mu, \alpha_i} \sum^{m}_{i=1} \sum^{n}_{j=1} (y_{ij} - \mu - \alpha_i)^2   \]
:END:



 
#+BEGIN_HTML
<br>
#+END_HTML

*** Literature

[[https://stat.ethz.ch/lectures/as19/anova.php#course_materials][Applied Anaysis of Variance - ETH Lecture Notes - Autumn 2019]]


