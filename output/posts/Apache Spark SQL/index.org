#+BEGIN_COMMENT
.. title: Apache Spark SQL
.. slug: Apache Spark SQL
.. date: 2019-09-06 18:21:43 UTC+02:00
.. tags: Big Data, Spark
.. category: 
.. link: 
.. description: 
.. type: text

#+END_COMMENT

#+BEGIN_HTML
<br>
<br>
#+END_HTML

This posts makes the point for Apache Spark SQL. 

Using RDDs API might be quite annoying, especially if you are used to
the industry standard of RDMS and their SQL sytax.

Here ApacheSparkSQL kicks in, providing a SQL interface to your data.

{{{TEASER_END}}}

The RDD is still a first class citizen, so what ApacheSparkSQL
basically does is wrapping the RDD with a DataFrame object actually
abstracting the API to a relational one. So basically you can access
your data but on top of the RDD you have a relational API. Internally
ApacheSpark will execute exacly th same code using the RDD API.

Independently, if you are using SQL or the DataFrame API, or both at
the same time, internally Apache Spark creates a so-called abstract
syntax tree out of this. This tree can be rearranged and transformed
to generate an execution plan which performs best. This is where a
catalyst optimizer kicks in. Having a schema allows the so called,
catalyst optimizer to create a logical query execution plan from your
SQL abstract syntax tree. The LEP is then compiled into many physical
execution plans, and based on cost based statistics on the size of
different data sets involved, and optimal PEP is chosen.

Notice finally that a second layer project Tungsten offers
additionally computational benefits when using ApacheSparkSQL. This
together with the catalyst function suggest the usage of
ApacheSparkSQL as a valid alternative to directly operating via RDDs
APIs.
