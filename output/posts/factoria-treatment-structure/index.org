#+BEGIN_COMMENT
.. title: Factorial Treatment Structure
.. slug: factoria-treatment-structure
.. date: 2019-10-19 15:00:53 UTC+02:00
.. tags: ANOVA
.. category: 
.. link: 
.. description: 
.. type: text
.. has_math: yes

#+END_COMMENT


#+BEGIN_EXPORT html
<br>
<br>
#+END_EXPORT


So far in the ANOVA series no special structure was assumed for the
treatments but rather that each treatment corresponded to a single
factor. This post starts to expand and relax the above assumption
looking at the relation when a treatment is the result of various
combination of individual factors. Think for instance of a plant being
influenced both by /light exposure/ and /fertilizer/.

In case of the above structure we talk about a /factorial treatment
structure/ and this one will be explored next.

{{{TEASER_END}}}

*** Example and Terminology
#+begin_comment
 :Properties:
 :header-args:R: :session anova :results output table :exports both
 :end:
#+end_comment

If we see all possible combinations of the levels of two (or more)
factors, we call them /crossed/. 

#+begin_src R :exports code
## create dataset
acids <- c(1.697, 1.601, 1.830,
           2.032, 2.017, 2.409,
           2.211, 1.673, 1.973, 
           2.091, 2.255, 2.987)
R50 <- rep(c("no", "yes", "no", "yes"), each = 3)
R21 <- rep(c("no", "no", "yes", "yes"), each = 3)
cheddar <- data.frame(R50, R21, acids)
#+end_src

#+RESULTS:

#+begin_src R
## In order to explore the strucutre and the combination of factors for each observation 
## you can leverage the xtabs () function in R

xtabs (~ R50 + R21, data = cheddar)
#+end_src 

#+RESULTS:
: 
:      R21
: R50   no yes
:   no   3   3
:   yes  3   3

Given the factorial structure questions arise of the individual effect
of the single factor variables as well as the influence of the
interplay.

An exploratory solution in lower dimension to answer the above
questions might be to look a the interaction plot. This gives a visual
idea of the interplay and effect of single variables.

#+begin_src R :exports none
# Open a svg file
svg("../images/interactionCheddar.svg") 
# 2. Create a plot
#+end_src

#+RESULTS:

#+begin_src R :cache yes :exports code
  with(cheddar, interaction.plot(x.factor = R50, trace.factor = R21, response = acids))
  ## standard way: interaction.plot(x.factor = cheddar$R50, trace.factor = cheddar$R21,
  ##                                response = cheddar$acids)

  ## Notice that the x-factor is the one being plotted on the x-axis
  ## while the trace factor acts as a dummy.

#+end_src

#+begin_src R :exports none
# Close the pdf file
dev.off()  
#+end_src

[[img-url:/images/interactionCheddar.svg]] 

#+begin_comment
Insert the image with ~C-u C-x C-f~

Inline the image with ~C-c C-x C-v~


[[file:~/Scrivania/Blogging/bits-of-experience/images/interactionCheddar.svg]]

#+end_comment

As the line are parallel you can infer that the the effect of R50 is
independent of R21.

In order to display the single observations and look at the underlying
noise and not simply drawing conclusion on averages you can leverage
the ggplot2 package.

#+begin_src R
library (gglot2)
#+end_src


#+begin_src R :exports none
# Open a svg file
svg("../images/interactionCheddarSingle.svg") 
# 2. Create a plot
#+end_src

#+RESULTS:

#+begin_src R :cache yes :exports code
ggplot(cheddar, aes(x = R50, y = acids, color = R21)) + geom_point() +
  stat_summary(fun.y = mean, geom = "line", aes(group = R21), size = 1) + theme_bw()
#+end_src

#+RESULTS[d784e9a076ae2075d68316dfb74968915222081c]:
#+BEGIN_SRC R
#+END_SRC

#+begin_src R :exports none
# Close the pdf file
dev.off()  
#+end_src

#+RESULTS:
: 
: X11cairo 
:        2

[[img-url:/images/interactionCheddarSingle.svg]] 


When data are to dense it might make sense to look at an aggregated
picture such as a boxplot for the different underlying groups.

#+begin_src R :exports none
# Open a svg file
svg("../images/interactionCheddarBox.svg") 
# 2. Create a plot
#+end_src

#+RESULTS:

#+begin_src R :cache yes :exports code
ggplot(cheddar, aes(x = R50, y = acids, color = R21)) + geom_boxplot() +
  stat_summary(fun.y = mean, geom = "line", aes(group = R21), size = 1) + theme_bw()
#+end_src

#+RESULTS[b5437d78d6162b70a25f237e935284d75c83f8b7]:
#+BEGIN_SRC R
#+END_SRC

#+begin_src R :exports none
# Close the pdf file
dev.off()  
#+end_src

#+RESULTS:
: 
: X11cairo 
:        2

[[img-url:/images/interactionCheddarBox.svg]] 

*** Two-way ANOVA Model

Consider a set-up with two factors A and B, with respectively /a/ and
/b/ number of levels. Assume a /balanced/ dataset with the same number /n/
of data for every combination $a_i b_j \ \forall i = 1,...., a; j =
1,..., b$. Then you will have overall /N = a * b * n/ of observations.

The two-way anova with interaction is then specified by:

#+BEGIN_src latex :results drawer :exports results
\[ Y_{ijk} = \mu + \alpha_i + \beta_j + \alpha \beta_{ij} + \epsilon_{ijk} \]
#+END_src

where

- \alpha_i = main effect of factor A at level i
- \beta_j = main effect of factor B at level j
- \alpha \beta_{ij} = interaction effect.
- \epsilon_{ijk} = i.i.d. N (0, \sigma^2) errors

Typically for the same arguments outlined in the previous post we have
to choose side-constraints in order to pose restrictions for the added
variables.

A typical choice consists in:

#+LATEX_HEADER: \usepackage{math}
#+LATEX_HEADER: \usepackage{asmath}

#+BEGIN_src latex :results drawer :exports results
  \begin{align*} 
    \sum^a_{i=1} alpha_i &=  0\\

    \sum^b_{j=1} \beta_j &=  0\\

    \sum^a_{i=1} \alpha \beta_{ij} &=  0\\

    \sum^b_{j=1} \alpha \beta_{ij} &=  0\\
  \end{align*}  
#+END_src

#+RESULTS:
:RESULTS:
\begin{align*} 
  \sum^a_{i=1} alpha_i &=  0\\

  \sum^b_{j=1} \beta_j &=  0\\

  \sum^a_{i=1} \alpha \beta_{ij} &=  0\\

  \sum^b_{j=1} \alpha \beta_{ij} &=  0\\
\end{align*}
:END:

It follows, that the main effects have /\alpha -1/, /\beta - 1/ and
(/\alpha -1/) * (/\beta - 1/). Important is especially to understand
the latter term. This is because each constraint of a variable is
associated with the full spectrum of the other factor constraints.

Notice that as always the exact interpretation of the two-way anova
with interaction terms depends on the choice of the applied
constraints. 

Given the above general framework it is possible to compare the
effects of the factors and compare it to the model overall stochastic
error to see if that is significant.

The average expected value from the two-way anova is:

#+BEGIN_src latex :results drawer :exports results
 \[ \bar{y}_{ij} = \hat{\mu} + \hat{\alpha_i} + \hat{\beta_j} + \hat{\alpha\beta_{ij}} \]  
#+END_src

#+RESULTS:
:RESULTS:
\[ \bar{y}_{ij} = \hat{\mu} + \hat{\alpha_i} + \hat{\beta_j} + \hat{\alpha\beta_{ij}} \]
:END:

As in the case of the one-way ANOVA one can decompose the sum of
squared errors as:

#+BEGIN_src latex :results drawer :exports results
\[ SS_T = SS_A + SS_B + SS_AB + SS_E  \]
#+END_src

where,

#+begin_comment
check at the indices. It is complex shit. 
#+end_comment

- $SS_A = \sum^{a}_{i=1} bn (\hat{a_i})^2$ and $\hat{a_i} =
  \frac{1}{a}\sum^{b}_{j=1}\sum^{n}_{k=1} y_{ijk} -
  \frac{1}{nab}\sum^{n}_{k=1}\sum^{a}_{i=1}\sum^{b}_{j=1}y_{ijk}$ 
- $SS_B = \sum^{b}_{j=1} an (\hat{\beta_j})^2$ and $\hat{\beta_i} =
  \frac{1}{b}\sum^{a}_{i=1}\sum^{n}_{k=1} y_{ijk} -
  \frac{1}{nab}\sum^{n}_{k=1}\sum^{a}_{i=1}\sum^{b}_{j=1}y_{ijk}$ 
- $SS_{AB} = \sum^{a}_{i=1}\sum^{b}_{j=1}n\hat{a_{ij}}^2$ and
  $\hat{a_{ij}}^2 = (\frac{1}{ab}\sum^{b}_{j=1}\sum^{a}_{i=1} y_{ijk} + \frac{1}{nab}\sum^{n}_{k=1}\sum^{a}_{i=1}\sum^{b}_{j=1}y_{ijk} - \frac{1}{a}\sum^{b}_{j=1}\sum^{n}_{k=1} y_{ijk} - \frac{1}{b}\sum^{a}_{i=1}\sum^{n}_{k=1} y_{ijk})^2$ 
- $SS_E = (\sum^{n}_{k=1}\sum^{a}_{i=1}\sum^{b}_{j=1}y_{ijk} -
  \frac{1}{ab}\sum^{b}_{j=1}\sum^{a}_{i=1} y_{ijk})^2$ 
- $SS_T = (\sum^{n}_{k=1}\sum^{a}_{i=1}\sum^{b}_{j=1}y_{ijk} -
  \frac{1}{nab}\sum^{n}_{k=1}\sum^{a}_{i=1}\sum^{b}_{j=1}y_{ijk})^2$

#+begin_quote
Notice that the above is the exact reason why the exam might be
annoying. You have to train the decomposition of the variance with all
of the indices and get familiar with a fast exposition of the latter.
#+end_quote

From here on you can create the classical anova table. Being aware of
the degrees of freedom.

Especially it holds:

- df factor A = (a -1)
- df factor B = (b -1)
- df interaction AB = (a -1) (b-1)
- df error = (nab - (a-1) [A factor] - (b-1) [B factor] - (a-1) (b-1) [interaction] - 1 [overall mean]) = ab (n-1)


Given the ANOVA table and the computed mean squared errors based on
the sum of squares and degrees of freedom above it is possible to
compare the single influence of each factor and the interaction term
with the underlying stochastic noise.

The following tests result:

- H_0: $\hat{a\beta_{ij}} = 0 \ \forall i,j$ vs. H_A: $\hat{a_{ij}} \neq
  0$ for at least one /i,j/. 

  Under the Null:

  $$ \frac{MS_{AB}}{MS_E} \mathtt{\sim} F_{(a-1) (b-1), ab (n-1)} $$

- H_0: $\hat{a_{i}} = 0pp \ \forall  i$ vs. H_A: $\hat{a_{i}} \neq
  0$ for at least one /i/. 

  Under the Null:

  $$ \frac{MS_{A}}{MS_E} \mathtt{\sim} F_{(a-1), ab (n-1)} $$

- H_0: $\hat{\beta_{j}} = 0 \ \forall j$ vs. H_A: $\hat{\beta_{j}} \neq
  0$ for at least one /j/. 

  Under the Null:

  $$ \frac{MS_{B}}{MS_E} \mathtt{\sim} F_{(b-1), ab (n-1)} $$

*** Single Replicates

Notice that if you have a /single/ observation for each level
combination then, it is not possible anymore to make statistical
inference with a model including an interaction term. The reason for
that is being unable to disentangle the interaction term component
from the stochastic error component \epsilon_{ijk}.

While, it is theoretically possible to fit a /main effect model/ and
estimate this accordingly, when the underlying data generating
mechanism actually contains an interaction, we are fitting a /wrong/
model. The consequence is that the /corresponding tests will be too conservative as attributing a higher standard error, and therefore p-values will be too large./ 
As the Type I error will therefore be under control you can choose
such an approach at the expense of a low power and an increased Type
II error.

Quite often however, it is possible to apply transformations to the
independent variables. This will transform the multiplicative nature
of the interaction term to an additive scheme on the log-scale, which
can then be accordingly estimated.


*** Unbalanced Data
#+begin_comment
 :Properties:
 :header-args:R: :session anova :results output table :exports both
 :end:
#+end_comment

So far, when looking at the two-way ANOVA model we have made the
restrictive assumption of a /balanced/ data design. Each level
combination displayed the same amount of observation. This led to the
beneficial conclusion that when changing the level of a single factor,
the resulting change in the response variable was affected directly by
the factor level change. In regression terminology this is called an
/orthogonal design/.

Relaxing the assumption of a /balanced/ dataset the analysis of
variance becomes more complex. A shift in factor level does not
influence the response variable just directly but rather also
indirectly due to the inherit dataset imbalances.

Think about the following case for instance

#+begin_src R
running <- read.table("http://stat.ethz.ch/~meier/teaching/data/running.dat", header = TRUE)
str(running)
#+end_src

#+RESULTS:
: 
: 'data.frame':	70 obs. of  3 variables:
:  $ gender: Factor w/ 2 levels "female","male": 2 2 2 2 2 2 2 2 2 2 ...
:  $ drink : Factor w/ 2 levels "A","B": 1 1 1 1 1 1 1 1 1 1 ...
:  $ y     : num  40.6 49.7 42.1 42.2 39 44.2 44.1 43.1 44.7 46.3 ...

#+begin_src R
xtabs (~ gender + drink, data = running)
#+end_src

#+RESULTS:
:         drink
: gender    A  B
:   female 10 40
:   male   10 10


The data reports a sports experiment using a factorial design with
factor gender and factor energy drink (having two levels). Response
was running time in seconds for a specific track. 

As it is possible to see above the data is /unbalanced/ containing 50
female and 20 male participants. 

It follows that when changing the level of the drink factor for
analyzing the effect of the drink component you automatically change
the underlying structure of factor /gender/ due to the unbalanced
dataset.

The result is a intricate relation among the different factors and the
end result that a decomposition of the sum of squared errors in the
single components as in the previous section is not possible.

In order to approach an unbalanced data scheme an alternative model
comparison design is needed. In this sense the concept of /reduction
of residual sum of squares/ is introduced.

The /reduction of residuals sum of squares/ states how much the
introduction of the factor reduces the sum of squares in comparison to
a smaller restricted model.

In order to see that consider again a one-way ANOVA model with

$$ SS_T = SS_Trt + SS_E $$

where 

#+BEGIN_src latex :results drawer :exports results
  \begin{align*} 
    SS_T   &=  \sum^{m}_{i=1} \sum^{n_i}_{j=1} (y_{ij} - \frac{1}{N}\sum^{m}_{i=1} \sum^{n_i}_{j=1} y_{ij})^2 \\

    SS_{Trt} &= \sum^{m}_{i=1} \sum^{n_i}_{j=1} (\frac{1}{n_i} \sum^{n_i}_{j=1} y_{ij} -  \frac{1}{N}\sum^{m}_{i=1} \sum^{n_i}_{j=1} y_{ij} )  \\

    SS_E   &= \sum^{m}_{i=1} \sum^{n_i}_{j=1} (y_{ij} - \frac{1}{n_i} \sum{n_i}_{j=1} y_{ij}) \\
  \end{align*}  
#+END_src

#+RESULTS:
:RESULTS:
\begin{align*} 
  SS_T   &=  \sum^{m}_{i=1} \sum^{n_i}_{j=1} (y_{ij} - \frac{1}{N}\sum^{m}_{i=1} \sum^{n_i}_{j=1} y_{ij})^2 \\

  SS_Trt &= \sum^{m}_{i=1} \sum^{n_i}_{j=1} (\frac{1}{n_i} \sum^{n_i}_{j=1} y_{ij} -  \frac{1}{N}\sum^{m}_{i=1} \sum^{n_i}_{j=1} y_{ij} )  \\

  SS_E   &= \sum^{m}_{i=1} \sum^{n_i}_{j=1} (y_{ij} - \frac{1}{n_i} \sum{n_i}_{j=1} y_{ij}) \\
\end{align*}
:END:

There the SS_{Trt} represented the reduction of the sum of squares
(i.e. the error amount captured by the treatment specific mean
effects) in comparison to the overall mean model. A similar
interpretation is possible for the /reduction of residual sum of
squares/ for the unbalanced case.


It follows then that in an unbalanced test setting with multiple
factors you can answer questions as: "Do we need factor =B= in the
model if we already have factor =B=?"

By sequentially building up a model and looking at the corresponding
/reduction of residuals sum of squares/.

#+BEGIN_EXPORT html
<br>
#+END_EXPORT

#+begin_quote
Notation:

We denote the residual sum of squares for factor B given factor A and
an overall constant \mu as:

~SS (B | 1, A)~. 

This consists therefore in the reduction in the sum of squares between 

~y ~ A + B~ and ~y ~ A~. 

The sequential nature follows automatically from the definition above
as we need the restricted model to compute the reduction in the
squared errors. 
#+end_quote

#+BEGIN_EXPORT html
<br>
#+END_EXPORT

There are now different ways to or /types/ of model comparison
approaches yielding different results. The choice of the model is
highly debated in the academic literature and is still an open issue.

- Model Comparison I: Sequentially building up a model by computing

#+begin_center
  =SS (A|1)= \\
  =SS (B|1, A)= \\
  =SS (AB|1, A, B)=
#+end_center  

- Model Comparison III: Control for /all/ other terms
  
#+begin_center
  =SS (A|1, B, AB)= \\
  =SS (B|1, A, AB)= \\
  =SS (AB|1, A, B)=
#+end_center  

- Model Comparison II: Control the influence of the largest
  /hierarchical/ model *not including the term of interest*. This
  consists essentially in model III dropping the terms that are
  /hierarchically/ lower than the factor for which the reduction in
  residual sum of squares is being computed.
  
#+begin_center
  =SS (A|1, B)= \\
  =SS (B|1, A)= \\
  =SS (AB|1, A, B)=
#+end_center  

/Some technical details are omitted here at the bottom. Very
generically stated both in the script and in the slides and it is not worth the effort of mentioning them here/.  
Have a look at them when preparing for the exam.


*** Literature

[[https://stat.ethz.ch/lectures/as19/anova.php#course_materials][Applied Anaysis of Variance - ETH Lecture Notes - Autumn 2019]]
