#+BEGIN_COMMENT
.. title: Contrasts and Multiple Testing
.. slug: contrasts-and-multiple-testing
.. date: 2019-10-09 18:02:38 UTC+02:00
.. tags: ANOVA
.. category: 
.. link: 
.. description: 
.. type: text
.. status: 
.. has_math: yes
#+END_COMMENT

#+BEGIN_HTML
<br>
<br>
#+END_HTML

#+LATEX_HEADER: \usepackage{math}
#+LATEX_HEADER: \usepackage{asmath}

So far in the /ANOVA/ series we have encountered a simple one-way
analysis of variance test. This was based on the idea of comparing the
standardized sum of squares of the treatment effect and the one of the
stochastic component. We did this with the /omnibus/ F-test testing
the null of no-effect of the treatments.

What happens however when we have a rather specific question, such as
the when we are concerned about the effect of a specific
treatment/factor and we question whether this is significant?

{{{TEASER_END}}}

#+BEGIN_HTML
<br>
#+END_HTML

* Setting up Hypothesis

In order to address our specific question we must set up an
hypothesis. In this sense, we can formulate a vector containing our
hypothesis.

For instance assuming a dataset containing a factor with three levels
such as the one encountered in the previous post. We might be
interested in comparing the mean of two of the levels of the factor.
We might then set up the hypothesis

#+begin_comment
interesting. note from below that the & in align is not required to go
on the equal sign. any place will do. it simply guarantees consistency
among the lines no matter where it is written.
#+end_comment

#+BEGIN_src latex :results drawer :exports results
  \begin{align*}
    \vec{c} = \begin{pmatrix} 1 \\ -1 \\ 0\end{pmatrix} ; & \hspace{10mm} \vec{\alpha} = \begin{pmatrix} \alpha_1 \\ \alpha_2 \\ \alpha_3 \end{pmatrix} \\ 

    \\

    H_0: \vec{c}^\intercal * \vec{\alpha} &= 0  \\

    H_A: \vec{c}^\intercal * \vec{\alpha} &\neq 0 
  \end{align*}
#+END_src

#+RESULTS:
:RESULTS:
\begin{align*}
  \vec{c} = \begin{pmatrix} 1 \\ -1 \\ 0\end{pmatrix} ; & \hspace{10mm} \vec{\alpha} = \begin{pmatrix} \alpha_1 \\ \alpha_2 \\ \alpha_3 \end{pmatrix} \\ 

  \\

  H_0: \vec{c}^\intercal * \vec{\alpha} &= 0  \\

  H_A: \vec{c}^\intercal * \vec{\alpha} &\neq 0 
\end{align*}
:END:

#+BEGIN_HTML
<br>
#+END_HTML

In the above case, we expressed the Null of equal treatment effects
for the two first levels. In an analogous way we can formulate any
other linear hypothesis.

Notice that the above is called a contrast and we usually provide a
side-constraint such that we ensure that the contrast above is testing
the difference among the treatments rather than the overall level of
the response. 

$$\sum^{m}_{i=1} c_i = 0$$

Notice however that if we are interested in the testing the difference
in levels of the treatment groups a Null as:

$$\vec{c} = \begin{pmatrix} 1 \\ 2 \\ 0 \end{pmatrix}$$

might make completely sense.

#+BEGIN_HTML
<br>
#+END_HTML


* Analysis of Variance and F-tests

It is possible to demonstrate that every contrast is associated with
a sum of squares:

#+BEGIN_src latex :results drawer :exports results
\[ SS_c = \frac{(\sum^{m}_{i=1} c_i 1/n_i \sum^{n_i}_{j=1} y_{ij})^2}{\sum^{m}_{i=1} \frac{c_i^2}{n_i}}  \]
#+END_src

#+RESULTS:
:RESULTS:
\[ SS_c = \frac{(\sum^{m}_{i=1} c_i 1/n_i \sum^{n_i}_{j=1} y_{ij})^2}{\sum^{m}_{i=1} \frac{c_i^2}{n_i}}  \]
:END:


having /one degree of freedom/. Notice, that above the nominator
represents the squared treatment group mean, weighted by the contrast
weight assigned to each group. The nominator is then weighted by being
multiplied by the number of observations in each group normalized by
the contrast weight.

It is then possible to see that under the null hypothesis it holds

$$\frac{MS_c}{MS_E} = \frac{SS_c}{MS_E} \mathtt{\sim} F_{1,N-m}$$


* Orthogonal, Non-redundant Contrasts

A particularly interesting family of contrasts are orthogonal
contrasts. These are non redundant contrasts. 

Two contrasts $c$ and $c^{*}$ are orthogonal when it holds:

#+BEGIN_src latex :results drawer :exports results
\[ \sum_{i = 1}^{m} \frac{c_i * c_i^{*}}{n_i} = 0\]
#+END_src

#+RESULTS:
:RESULTS:
\[ \sum_{i = 1}^{m} \frac{c_i * c_i^{*}}{n_i} = 0\]
:END:


Then no information contained in one contrast is present in a second
and contrasts are therefore non-redundant. To see that consider the
toy example:

#+BEGIN_src latex :results drawer :exports results
\[ \psi_1  =   \begin{pmatrix} 1 \\ 1 \\ 0\end{pmatrix}  \hspace{10mm}   \psi_2  = \begin{pmatrix} 0 \\ 0 \\ 1\end{pmatrix} \hspace{10mm}  \psi_3  = \begin{pmatrix} 1 \\ 0 \\ 1\end{pmatrix}  \]
#+END_src

#+RESULTS:
:RESULTS:
\[ \psi_1  =   \begin{pmatrix} 1 \\ 1 \\ 0\end{pmatrix}  \hspace{10mm}   \psi_2  = \begin{pmatrix} 0 \\ 0 \\ 1\end{pmatrix} \hspace{10mm}  \psi_3  = \begin{pmatrix} 1 \\ 0 \\ 1\end{pmatrix}  \]
:END:

Then, \psi_3 is a redundant contrast and having it or not does not add
any info to the experimenter.

What is important now is that given our /m/ treatments, we can always
find /m-1/ orthogonal treatments; this because one dimension is
already occupied by the global mean, where we constrain $c = (1,
...., 1)^\intercal$. Given those, we individual sum of squares for
individual contrasts are independent by definition and the total
/treatment sum of squares/ can be rewritten as the sum of the
individual contrasts.

$$ SS_{c_1} + .... + SS_{c_m} = SS_{Trt} $$ 

*This means that posing the right /m-1/ questions about the treatment
you get all the information that is involved with the it.* 

#+BEGIN_HTML
<br>
#+END_HTML

* Testing Multiple Contrasts

While using contrasts is a neat tool to test a set of linear
hypothesis one should be careful, when repeatedly testing multiple
contrasts hypothesis.

For instance, when testing /j/ independent Null $H_{0,j}$ with
confidence \alpha, the probability of rejecting at least one Null when
/all of the Null are correct/ is

$$ 1 - (1-a)^j$$

It is straightforward to see that such probability tends to one, no
matter how high the confidence as j gets large.

This requires a way to holistically handle all of the different
Nulls. This is the topic of the chapter.

In this sense, it is of use looking at the /family-wise error
rate/. This expresses the /probability of rejecting at least one of
the true H_0 [V] /:  

$$ FWER = P(V \geq 1). $$

Given the above definition we say that *a procedure controls the
family-wise error rate in the strong sense* at level \alpha iff:

$$ FWER = P(V \geq 1) \leq \alpha. $$

#+BEGIN_HTML
<br>
#+END_HTML

** FWER and FDR

An important by product results that naturally emerges is that when
controlling the family-wise error rate in the strong sense, we
automatically bound the /false discovery rate (FDR)/  defined as:

$$ FDR = \frac{number\ of\ rejected\ Null\ where\ the\ Null\ was
correct}{total\ number\ of\ rejected\ Null} = fraction\ of\ wrongly\ rejected\
Null$$

This concretely means that when bounding the FWER to \alpha we
automatically bound the FDR to \alpha.

** FWER and Confidence Intervals

We can extend the concept of FWER to confidence intervals. This should
then express the probability that /all intervals/ cover the
corresponding true parameter value is (1 - \alpha). This means that
when looking at all the confidence intervals simultaneously we should
should get a probability of (1-\alpha) of capturing the true
parameter. The concept is defined as *simultaneous confidence
intervals*. 

Notice, that in the above cases, we typically start with “individual”
p-values that we modify (or adjust) such that the appropriate overall
error rate (like FWER) is being controlled. Interpretation of an
individual p-value is as you learned it in your introductory course
(“the probability to observe an event as extreme as …”). The modified
p-values should be interpreted as the smallest overall error rate such
that we can reject the corresponding null hypothesis. 

** Bonferroni 

This is a rather generic but very conservative approach and it
obviates to compute the exact FWER and Simultaneous Confidence
Interval introduced above.

It consists of using the rather restrictive *individual* significance
level $a^{*} = \frac{a}{m}$. This will control the /family-wise error
rate/ in the strong sense albeit being quite restrictive. 

Moreover, the confidence intervals based on the adjusted significance
level are /simultaneous/.

** Bonferroni-Holm

This is generally /less conservative/ and uniformly more powerful than
Bonferroni. 

It works as follows:

1. Sort p-values from the smallest to the largest: $p_{(1)} \leq
   ... \leq p_{(j)} $

2. for i = 1,2 ....: Reject the Null if $p_{(i)} \leq \frac{a}{m-i+1}$

3. stop when reaching the first non-significant p-value.

Notice therefore that under Bonferroni-Holm the higher p-values are
compared with lower confidence levels. This has therefore a relaxing
effect in comparison to the standard Bonferroni test.

** Scheffé

The Scheffé procedure controls for the search over /any possible
contrast/. This means that we can try out as many contrasts as we like
and still get honest p-values. This comes, however at the expense of
low power. That is of a /low probability of rejecting when the
alternative is right/.  Scheffé contains therefore, the issue of a
Type I error at the expense of a higher probability of committing Type
II errors.

The Scheffé procedure works as follows:

1. Calculate the F-ratio as if ordinary contrast 

2. use the distribution $ (m-1) F_{m-1, N-m}$ instead of $F_{m-1,
   N-m}$ to calculate p-values.


** Tukey Honest Significance Difference

A special case for a multiple testing problem is when comparing
between all possible pairs of treatments. 

Overall, there are $frac{m* (m-1)}{2}$ treatment pairs that we can
compare. When testing all such different pairs we could proceed by
calculating the pairwise F-statistics and applying /Bonferroni-Holm/
to obtain the p-values.

A more /powerful/ approach in such case consists in applying the
/Tukey Honest Significant Difference/. This is custom tailored for
this test design and the details were omit in the class script.


** Multiple Comparison with a Control (MCC)

This is in a similar spirit as the Tukey Honest Significance
difference above. It is a high /power/ test for comparing the all the
treatment groups with an overall control group.

The corresponding test yielding the best p-values in such a case is
called /Dunnett procedure/ and might be further inspected inc ase of
interest.


