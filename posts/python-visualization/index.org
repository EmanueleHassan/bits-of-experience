#+BEGIN_COMMENT
.. title: Python Visualization
.. slug: python-visualization
.. date: 2020-03-19 14:02:40 UTC+02:00
.. tags: 
.. category: 
.. link: 
.. description: 
.. type: text
.. status: 
#+END_COMMENT

#+begin_export html
<style>
img {
display: block;
margin-left: auto;
margin-right: auto;
}
</style>
#+end_export


Here some python visualization dummy scripts are saved

{{{TEASER_END}}}

#+BEGIN_EXPORT html
<br>
<br>
#+END_EXPORT

* Data
   :properties:
   :header-args:ein-python: :session http://127.0.0.1:8888/visualization.ipynb  :results output
   :end:
   

#+NAME: 34EAA941-EAD4-468F-8FED-B9E16FDF0717
#+begin_src ein-python :results output
import matplotlib.pyplot as plt
%matplotlib inline 
%config InlineBackend.figure_format = 'png'
import numpy as np
#+end_src

#+NAME: F7E22CAC-CB72-4675-A4E3-E543629F59D9
#+begin_src ein-python :results output
N = 30
x_1 = np.random.randn(N) * 3
x_2 = np.random.randn(N) * 3 + 2
x_3 = np.random.randn(N) * 3 + -2
#+end_src

* Work with pandas
   :properties:
   :header-args:ein-python: :session http://127.0.0.1:8888/visualization.ipynb  :results output
   :end:

#+NAME: 75F79485-A875-4FE1-82A3-C97987A8104B
#+begin_src ein-python :results output
import pandas as pd
#+end_src

- create data from numpy matrix

#+NAME: B4B1951C-33E1-4583-BC59-8A5A1FD88259
#+begin_src ein-python :results output
mydat = pd.DataFrame (np.matrix ([x_1,x_2,x_3]).transpose ())
#+end_src

- create data from two numpy arrays

here you need to flatten the arrays first

#+begin_src ein-python :results output
X = 2 * np.random.rand(100,1)
y = 4 +3 * X+np.random.randn(100,1)

mydat = pd.DataFrame ({'x':X.flatten(), 'y': y.flatten()})
#+end_src

- describe the data

#+NAME: 62E7CBD5-4D53-4C5E-A1AC-70E42CBA1F0A
#+begin_src ein-python :results output
mydat.describe ()
#+end_src

- sort multiple variables in pandas data frames.

#+begin_src python
df.sort_values(['Year', "Happiness_Score"], ascending=[True, False], inplace=True)
#+end_src

- Set the name of the columns

#+NAME: DDDA7E5F-FC2A-4A7D-BC7B-D7CF473D3C49
#+begin_src ein-python :results output
mydat.columns = ["nomean", "posmean", "negmean"]
#+end_src

#+NAME: A30028B2-C614-406C-AF97-2DF4F99E1648
#+begin_src ein-python :results output
mydat.columns
#+end_src

#+NAME: 665F633A-521D-4384-84FF-677EF7633056
#+begin_src ein-python :results file
plt.plot(mydat.nomean, c = "lightblue", ls = '--', marker = "o",
         ms = 6, label ="nomean")
plt.plot(mydat.posmean, c = "darkorange", ls = '--', marker = "s",
         ms = 6, label ="posmean") # ms = marker size
plt.plot(mydat.negmean, c = "lightgreen", ls = '--', marker = "^",
         ms = 6, label ="negmean")
plt.legend(loc = 'upper right',bbox_to_anchor = (1,1))
plt.xticks(list(range(20)), rotation = 'vertical') # sets the ticks on
                                                   # x-axis and puts
                                                   # them vertically.
#+end_src


- get the entry (x,y) in your relational table

#+NAME: D3530789-6ACE-490F-A62F-1F738ABE00D3
#+begin_src ein-python :results output
mydat.head (5)
#+end_src

#+NAME: E45EF453-DF18-4DA7-8510-1E6B2480F4A7
#+begin_src ein-python :results output
mydat.iloc[2,1]
#+end_src

- slicing the data

#+NAME: 392E582F-AFE3-4B9A-BF7A-18B9D55B15F6
#+begin_src ein-python :results output
mydat[0:4]
#+end_src

- get by column

#+NAME: 72EFF5A5-8589-4813-A918-91547491D259
#+begin_src ein-python :results output
mydat[['nomean', 'posmean']].head (3)
#+end_src

- get by row and column

#+NAME: D0ECAD01-5802-45DE-B009-8C0ED8926068
#+begin_src ein-python :results output
mydat[['nomean', 'posmean']][::2]
#+end_src

#+NAME: 7EE3C177-63AE-46F2-80AE-6FC33912E63E
#+begin_src ein-python :results output
print(mydat[['nomean', 'posmean']].iloc [0,1])
#+end_src

- new column

#+NAME: C7AEAA0C-FE4F-44F1-B4E2-941105BD1F9C
#+begin_src ein-python :results output
mydat['sum'] = mydat['nomean'] + mydat['posmean'] + mydat['negmean']
#+end_src

#+NAME: E676C563-E1E9-43EB-8C63-8EA4C417C995
#+begin_src ein-python :results output
mydat.columns
#+end_src

#+NAME: 74B65603-E9C1-4760-82AA-780263961A4C
#+begin_src ein-python :results output
mydat2 = mydat
mydat.head ()
#+end_src

- drop a column

#+NAME: 4C89C61A-D2C8-4720-9357-021883EC97FE
#+begin_src ein-python :results output
mydat2.drop ('sum', axis = 1).head ()
#+end_src

- filter through booleans

#+NAME: FE3A32A6-A5AD-41B5-A156-A8FF68D0BE85
#+begin_src ein-python :results output
filt = mydat.nomean < 0

print (mydat[['sum', 'negmean']][filt])
#+end_src


- remove duplicates

#+NAME: 66AC1B55-FDF1-496A-9A1C-EC03BDFDC01A
#+begin_src ein-python :results output
print (np.matrix ([[1,2,1],[3,3,3]]).transpose ())

pd.DataFrame (np.matrix ([[1,2,1],[3,3,3]]).transpose ()).iloc[:,1].unique ()
#+end_src

another possibility that addresses data frame wide duplicates and not
simply column duplicates is by leveraging =.duplicated ()= method:

#+begin_src python :results output
import pandas as pd

romeo = pd.DataFrame({'a': [1,1,3], 'b' : [2,2,3]})

print(romeo)
print(romeo.duplicated())

romeo = pd.DataFrame({'a': [1,1,3], 'b' : [2,4,3]})
print(romeo)
print(romeo.duplicated())
#+end_src

- on the difference between =.iloc= and =.loc=

notice by the way that this are outdated. by now you should use =.iat=
and =.at=.

The difference is the following.

=.iat= : uses the strict position in the matrix

=.at= : search by index and column names


#+NAME: 9F6C4632-2087-4EF2-BE4D-6C4E948FA712
#+begin_src ein-python :results output
print (mydat)
mydat.at[0, 'negmean']
#+end_src

#+NAME: A0ABCC18-127A-4686-ADC1-E131136EE271
#+begin_src ein-python :results output
mydat.iat[0,2]
#+end_src

To further understand the thing consider

#+NAME: 67DEF99A-FF5E-4AFC-8F45-C713CDBE0A4F
#+begin_src ein-python :results output
mydat2 = mydat[::2]
mydat2
#+end_src

#+NAME: 50340226-FF1D-4B09-BDE9-7CE3DF62C058
#+begin_src ein-python :results output
print (mydat2.at[6, 'nomean']) ## this by real index independent of the axis
print (mydat2.iat[6, 0]) ## this sixth row in the matrix
#+end_src

- categorical variables

This is important for treating them as dummies in regression as well
for facet_wraps. 

#+NAME: 7BCD560B-E1CB-4B0C-B8A6-92B4D3BFC054
#+begin_src ein-python :results output
mydat.head ()
#+end_src


#+NAME: B5E0FFC5-18B4-421C-AB89-9A83A3FD8FBD
#+begin_src ein-python :results output
mydat["category"] = np.repeat (range (4), 5)
mydat.info ()
#+end_src

To transform this into a category use

#+NAME: 3C08E5B6-01FF-4D91-9F28-7BB440385BB5
#+begin_src ein-python :results output
mydat.category = mydat.category.astype ('category')
mydat.info ()
#+end_src


To view the category types

#+NAME: 6A930BF7-8560-4027-B1A6-D105861E9B24
#+begin_src ein-python :results output
mydat.category.cat.categories
#+end_src

To check for missing values =NaN= 

#+begin_src python :exports both
## check if there are null values for stream_id column
df_streams[['stream_id']].isnull().values.any()

## get the null values
df_streams[np.array(df_streams[['stream_id']].isnull())]
#+end_src

In order to apply a function sequentially to each entry in a column
dataframe you can use the =apply= function on pandas dataframe.

For instance for the following dataframe

#+begin_src python :session sparse :results output
cust_id.head()
#+end_src

Then you could specify a function return the years since the date in
column dob

#+begin_src python
def age(x):
    datetime_object = datetime.strptime(x, '%m/%d/%y')

    datetime_now = datetime.today()

    return datetime_now.year - datetime_object.year
#+end_src

Finally you can pass the =apply= function to each row of your
pandas dataframe column by

#+begin_src python
print(cust_id['dob'].apply (lambda row: age(row)))

cust_id['age'] = cust_id['dob'].apply (lambda row: age(row))

cust_id.head()
#+end_src

#+begin_example
        dob  1            2        3               4  5              6  \
0  07/30/98  1         Todd    Kasen  South Carolina  m  united_states  
1  04/12/89  2        Garza   Ensley            None  f      singapore
2  09/12/97  3        Carey  Lillian         Alabama  f  united_states
3  01/28/99  4  Christensen     Beau        New York  m  united_states
4  03/23/98  5       Gibson  Ernesto            None  m      singapore 

age  
22  
31  
23  
21  
22  
#+end_example

- deleting missing data

In order to delete missing data you can simply rely on the 

#+begin_src python :results output :exports both :session hello
import pandas as pd
from numpy import nan

df = pd.DataFrame({'name':['apple','banana','orange'],
                   'price':[1.95, 3.00, nan], 'inventory':[nan, 12, 23]})


print(df)

print ("\nAfter deleting missing values:")
print(df.dropna())
#+end_src

In the above you eliminate the entire *row* where the missing value
occurred.

The above is particular important when missing observations occurs
randomly such that you might safely ignore individual observations
without increasing the bias of your analysis given your data.

A second possibility, when dealing with missing data consists in
deleting entire features (i.e. columns). You can do that by setting
the argument ='columns'= in your =.dropna ()= method

#+begin_src python :results output :exports both :session hello
print(df)

print ("\nAfter deleting missing values:")

print(df.dropna(axis = 'columns'))
#+end_src

- impute missing data

a different approach on handling missing data is to *impute* missing
data. This means that instead of removing the data observations you
try to replace them with some meaningful information.

This might be useful for instance when understanding that data are not
missing at random and you might use the dependency on other features -
i.e. a predictive model based on that - to impute missing variables. 

Other simpler methods might involve taking simple features means as a
data-filler or some simple analogous measure. An example in this sense
might be 

#+begin_src python :results output :exports both :session hello
from sklearn.impute import SimpleImputer

print (df)

features = ['price', 'inventory']
imp = SimpleImputer()

# Use .values attribute bc sklearn works with arrays rather than DataFrames
imp.fit(df[features].values)

print(imp.transform(df[features].values))
#+end_src

where notice that in the above the simple average was taken to replace
missing values. Notice moreover how the standard sklearn API applies -
i.e. the =.fit ()= and =.transform ()= methods.

** New dataset
   :properties:
   :header-args:python: :session pandas
   :end:

For the next sections I will use the following new dataset

#+begin_src python
import re
import numpy as np
import pandas as pd
#+end_src

#+NAME: D5E71680-E3E5-4FBD-B754-061941C9C71D
#+begin_src ein-python
df = pd.read_csv("~/Desktop/Learning/AI_workflow_Coursera/Visualization/world-happiness.csv",index_col=0)
print("df: {} x {}".format(df.shape[0],df.shape[1]))

## clean up the column names and remove some
df.columns = [re.sub("\s+","_",col) for col in df.columns.tolist()]
df.head(n=4)
#+end_src

#+RESULTS:
:        Country                           Region  Happiness_Rank  ...  Generosity  Dystopia_Residual  Year
: 0  Afghanistan                    Southern Asia           153.0  ...     0.36510            1.95210  2015
: 1      Albania       Central and Eastern Europe            95.0  ...     0.14272            1.89894  2015
: 2      Algeria  Middle East and Northern Africa            68.0  ...     0.07822            2.43209  2015
: 3       Angola               Sub-Saharan Africa           137.0  ...     0.12344            1.94939  2015
: 
: [4 rows x 12 columns]


 - view a review of all of the null values

 #+begin_src python
## missing values summary
print("Missing Value Summary\n{}".format("-"*35))
print(df.isnull().sum(axis = 0))
 #+end_src

 #+begin_example
Missing Value Summary
-----------------------------------
Country                           0
Region                            0
Happiness_Rank                   25
Happiness_Score                  25
Economy_(GDP_per_Capita)         25
Family                           25
Health_(Life_Expectancy)         25
Freedom                          25
Trust_(Government_Corruption)    25
Generosity                       25
Dystopia_Residual                25
Year                              0
dtype: int64
 #+end_example


- pivot data frame and create aggregate measures for them (similar to
  mutate in =dplyr=)

  #+begin_src python
columns_to_show = ["Happiness_Score","Health_(Life_Expectancy)"]
pd.pivot_table(df, index= 'Year',values=columns_to_show,aggfunc='mean').round(3)
  #+end_src


- groupby

analogously to the previous version one can compute aggregated
measures using the groupby command

   #+begin_src python
   df.groupby(['Year'])[columns_to_show].mean().round(3)
   #+end_src


Aggregating by multiple columns is straightforward.

#+begin_src python 
pd.pivot_table(df, index = ['Region', 'Year'], values=columns_to_show).round(3)
## or
# df.groupby(['Region', 'Year'])[columns_to_show].mean().round(3)
#+end_src

Finally you can pass a column entry instead of displaying the results
as above. This is more user friendly in case you might have to merge
the results into other data frames.

#+begin_src python
pd.pivot_table(df,index='Region',columns='Year',values="Happiness_Score")
#+end_src



- create factor variables out of continuous variables through the
  =cut= method.

#+begin_src python 
pd.cut (df['Happiness_Rank'], bins = 4)
#+end_src

- append a column to a dataframe

#+begin_src python 
pd.concat(objs = [df, pd.cut (df['Happiness_Rank'], bins = 4)], 
          axis = 1)
#+end_src

* Sparse Matrices
:properties:
:header-args:python: :session sparse :results output :exports both
:end:

It is essential for data engineers and data scientists to know how to
work with sparse matrices. This are matrices with many =0= entries and
just a few non-zero entries. 

Instead of representing these kind of matrices in their =dense
representation= i.e. with a bunch of =0= entries you might save the
information in a more compressed way. This will allow to save on
memory on your machines and to effectively work with huge matrices.

Sparse matrices are moreover important for the development of ML
jobs. The idea is that once you found a well performing ML model you
might create data pipelines that extract transform and load data to
your ML model in the desired shape.

At the beginning, when you are exploring the data and trying to come
up with a meaningful model it does not make sense to create such data
pipelines as the job of coming up with them might be time-consuming
and you might end up with no business value added from your ML
application. 

It is therefore important to postpone the creation of such time
consuming tasks at the end. when you have a production ready ML
model. 

Before of that it is advisable to leverage dumps of data and to
explore your system through them. In this sense sparse matrices are
especially beneficial when data can be efficiently represented through
them. These allow you to store a huge amount of data due to their
efficient memory management - i.e. by saving just the relevant portion
of your data -.

In python you can work with sparse matrices via:

#+begin_src python 
import numpy as np
from scipy import sparse
#+end_src

Notice that a matrix, is said to be sparse if more than 50% of its
entries are 0.

For instance the following matrix is sparse

#+begin_src python
A = np.random.randint(0,2,100000).reshape(100,1000)
sparcity = 1.0 - (np.count_nonzero(A) / A.size)
print(round(sparcity,4))
#+end_src

There are essentially four types of sparse matrices used for
computation.

CSC (Compressed Sparse Column) and CSR (Compressed Sparse Row) are
more compact and efficient, but difficult to construct "from
scratch". 

Coo (Coordinate) and DOK (Dictionary of Keys) are easier to construct,
and can then be converted to CSC or CSR via matrix.tocsc() or
matrix.tocsr().

CSC is more efficient at accessing *column-vectors or column
operations*, generally, as it is stored as arrays of columns and their
value at each row.

CSR matrices are the opposite; stored as arrays of rows and their
values at each column, and are *more efficient at accessing row-vectors
or row operations*.

[[https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.coo_matrix.html#scipy.sparse.coo_matrix][coo matrix]]

#+begin_src python
A = np.random.poisson(0.3, (10,100))
B = sparse.coo_matrix(A)
C = B.todense()

print("A",type(A),A.shape,"\n"
      "B",type(B),B.shape,"\n"
      "C",type(C),C.shape,"\n")
#+end_src

You see that you can transform back and forth from dense matrices to
sparse matrices.

Coo sparse matrix built from the COOrdinates and values of the
non-zero entries.

#+begin_src python
print(B)
#+end_src


[[https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html#scipy.sparse.csc_matrix][csc_matrix]]

#+begin_src python
B = sparse.csc_matrix(A)

print(B)
#+end_src

See that the above respects the column array storage. I.e. the
non-zero coordinates are returned column by column.

[[https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix][csr_matrix]]

Like the CSC format there is a CSR format to account for data that
repeat along the rows

#+begin_src python
B = sparse.csr_matrix(A)
print(B)
#+end_src

See that the above respects the row array storage. I.e. the
non-zero coordinates are returned row by row.

*Notice:* that many np.<methods> used for performing matrix operations
do not work on sparse matrices. You might refer to [[https://docs.scipy.org/doc/scipy/reference/sparse.html][this link]] for
checking on how to deal with that. A solution proposed is to
transform the sparse matrix in an array.

Finally, it is easy to populate a sparse matrix by

#+begin_src python
rows = [0,1,2,8] ## coodinates
cols = [1,0,4,8]
vals = [1,2,1,4]

A = sparse.coo_matrix((vals, (rows, cols)))
print(A.todense())
print()
print(A.tocsr())
#+end_src

you can finally stack two sparse matrices, be it horizontally or
vertically together.

#+begin_src python
C = sparse.csr_matrix(np.array([0,1,0,0,2,0,0,0,1]).reshape(1,9))
print(A.shape,C.shape)


print("adding horizontally") 
D = sparse.vstack([A,C])
print(D.todense())

print("adding vertically") 
D = sparse.hstack([A,C.reshape(9,1)])
print(D.todense())
#+end_src


* MatplotLib
   :properties:
   :header-args:ein-python: :session http://127.0.0.1:8888/visualization.ipynb  :results output
   :end:


#+NAME: 7B605A9A-38A5-40A9-991B-EE6B0A394437
#+begin_src ein-python :results output
plt.plot(x_1, c = "lightblue", ls = '--', marker = "o",
         ms = 6, label ="nomean")
plt.plot(x_2, c = "darkorange", ls = '--', marker = "s",
         ms = 6, label ="posmean") # ms = marker size
plt.plot(x_3, c = "lightgreen", ls = '--', marker = "^",
         ms = 6, label ="negmean")
plt.legend(loc = 'upper right',bbox_to_anchor = (1,1))
plt.xticks(list(range(20)), rotation = 'vertical') # sets the ticks on
                                                   # x-axis and puts
                                                   # them vertically.
#+end_src

#+BEGIN_EXPORT html
<br>
<br>
#+END_EXPORT

#+begin_export html
 <img width="60%" height="100%" src="../../images/ob-ein-e4d60f65359faf963dc9edc65a3851e0.png" class="center">
#+end_export

#+BEGIN_EXPORT html
<br>
<br>
#+END_EXPORT

- stacked histograms via matplotlib

#+NAME: C5CFE08C-0A8D-4CDE-8B30-6878D058AE31
#+begin_src ein-python :results output
plt.hist([mydat[mydat.bitwise == True].nomean,
          mydat[mydat.bitwise == False].posmean])
#+end_src

#+BEGIN_EXPORT html
<br>
<br>
#+END_EXPORT

#+begin_export html
 <img width="60%" height="100%" src="../../images/ob-ein-4e5223a40dc5aad0bb91fba3fb97f4c6.png" class="center">
#+end_export

#+BEGIN_EXPORT html
<br>
<br>
#+END_EXPORT

#+NAME: DED50857-0468-45BE-8BDC-7EF70F426AFC
#+begin_src ein-python :results output
plt.hist([mydat[mydat.bitwise == True].nomean,
          mydat[mydat.bitwise == False].posmean],
         stacked = True)
#+end_src

#+BEGIN_EXPORT html
<br>
<br>
#+END_EXPORT

#+begin_export html
 <img width="60%" height="100%" src="../../images/ob-ein-a029aa36eeb77d6b33e123c2a7023261.png" class="center">
#+end_export

#+BEGIN_EXPORT html
<br>
<br>
#+END_EXPORT

* Seaborn
   :properties:
   :header-args:ein-python: :session http://127.0.0.1:8888/visualization.ipynb  :results output
   :header-args:python: :session pandas :results output
   :end:

#+NAME: 05C46A89-D1D4-4DFE-9E2A-01CEC3634EAE
#+begin_src ein-python :results output
sns.distplot (mydat['nomean'])
#+end_src

#+BEGIN_EXPORT html
<br>
<br>
#+END_EXPORT

#+begin_export html
 <img width="60%" height="100%" src="../../images/ob-ein-6b9e2671be86b00c00b82558a0c879f3.png" class="center">
#+end_export

#+BEGIN_EXPORT html
<br>
<br>
#+END_EXPORT

#+NAME: CEEFDC10-1359-478E-97FB-A3B03C01C59E
#+begin_src ein-python :results output
mydat.head ()
#+end_src

#+NAME: FBDCB0BF-41E2-4642-BC31-C41CCEDD1F97
#+begin_src ein-python :results output
g = sns.lmplot(x="posmean", y="sum", data = mydat)
#+end_src

#+BEGIN_EXPORT html
<br>
<br>
#+END_EXPORT

#+begin_export html
 <img width="60%" height="100%" src="../../images/ob-ein-fdb6afb1310d32cd9d7ab3e7504bdc14.png" class="center">
#+end_export

#+BEGIN_EXPORT html
<br>
<br>
#+END_EXPORT

#+NAME: 484343C4-2E49-4800-AF8E-EDFC576D1E1E
#+begin_src ein-python :results output
import statsmodels
g = sns.lmplot(x="posmean", y="sum", col="bitwise", hue="bitwise", data=mydat,
               y_jitter=.02,  truncate=False)
#+end_src

#+BEGIN_EXPORT html
<br>
<br>
#+END_EXPORT

#+begin_export html
 <img width="60%" height="100%" src="../../images/ob-ein-bd913d592aa2ad7d32dda045f217c4b5.png" class="center">
#+end_export

#+BEGIN_EXPORT html
<br>
<br>
#+END_EXPORT

To further inspect different visualization techniques check at the
[[https://seaborn.pydata.org/examples/index.html][seaborn page]].


#+NAME: AAC523C6-CCD2-43EE-84AC-10A2227BE562
#+begin_src ein-python :results output
g = sns.lmplot(x="posmean", y="sum", data = mydat, 
               hue = 'bitwise', fit_reg = False)
#+end_src

#+BEGIN_EXPORT html
<br>
<br>
#+END_EXPORT

#+begin_export html
 <img width="60%" height="100%" src="../../images/ob-ein-ffa4f4a083eb7753d04349001d0af209.png" class="center">
#+end_export

#+BEGIN_EXPORT html
<br>
<br>
#+END_EXPORT


- joinplot for inspecting the density of two functions


#+NAME: C2CD0FFE-51BF-44EA-B24E-A7E8C45D37F5
#+begin_src ein-python :results output
g = sns.jointplot (data = mydat, x = 'posmean', y = 'negmean')
#+end_src

#+NAME: DB58737D-A14C-4118-AAD8-C4D6A69287BE
#+begin_src ein-python :results output
g = sns.jointplot (data = mydat, x = 'posmean', y = 'negmean', kind = 'hex')
#+end_src

#+NAME: 53BA694B-7746-427C-8CD7-20505E9B70FA
#+begin_src ein-python :results output
sns.jointplot (data = mydat, x = 'posmean', y = 'negmean'). \
    plot_joint(sns.kdeplot, zorder=3, n_levels=6) ## add isolines
#+end_src

#+BEGIN_EXPORT html
<br>
<br>
#+END_EXPORT

#+begin_export html
 <img width="60%" height="100%" src="../../images/ob-ein-9fcf615aed09e4928c87ae2b4b547b4e.png" class="center">
#+end_export

#+BEGIN_EXPORT html
<br>
<br>
#+END_EXPORT

- kdeplot without data

#+NAME: 454BEFE2-3E0C-4F18-BF9A-582B2BA3534E
#+begin_src ein-python :results output
 sns.kdeplot (mydat.posmean, mydat.nomean)
#+end_src

#+BEGIN_EXPORT html
<br>
<br>
#+END_EXPORT

#+begin_export html
 <img width="60%" height="100%" src="../../images/ob-ein-c31fe9f7c879bc265853bd1dc9039845.png" class="center">
#+end_export

#+BEGIN_EXPORT html
<br>
<br>
#+END_EXPORT

To draw the isolines via colour scale use the shade bool

#+NAME: 7F1664FF-89F4-42EA-9EB5-3FCE7CC06B3E
#+begin_src ein-python :results output
 sns.kdeplot (mydat.posmean, mydat.nomean,
              shade = True)
#+end_src

#+BEGIN_EXPORT html
<br>
<br>
#+END_EXPORT

#+begin_export html
 <img width="60%" height="100%" src="../../images/ob-ein-2b28d7b239bb2b2c97d91ddec01caab4.png" class="center">
#+end_export

#+BEGIN_EXPORT html
<br>
<br>
#+END_EXPORT

To still see the grids

#+NAME: CB2014DE-0554-4DCE-8DD1-6F60498379E3
#+begin_src ein-python :results output
 sns.kdeplot (mydat.posmean, mydat.nomean,
              shade = True, shade_lowest = False)
#+end_src

#+BEGIN_EXPORT html
<br>
<br>
#+END_EXPORT

#+begin_export html
 <img width="60%" height="100%" src="../../images/ob-ein-9a36b78d9df73e73b323e41e10582027.png" class="center">
#+end_export

#+BEGIN_EXPORT html
<br>
<br>
#+END_EXPORT

- change the background style

#+NAME: F8E12BC0-D8F9-4663-AC65-CACACA118F56
#+begin_src ein-python :results output
sns.set(style="darkgrid")
sns.kdeplot (mydat.posmean, mydat.nomean)
#+end_src

#+BEGIN_EXPORT html
<br>
<br>
#+END_EXPORT

#+begin_export html
 <img width="60%" height="100%" src="../../images/ob-ein-7ff3fea35de715f49145204980aeb263.png" class="center">
#+end_export

#+BEGIN_EXPORT html
<br>
<br>
#+END_EXPORT

- create subplots

#+NAME: B93D66F2-8C49-4C99-B9E0-FE71FEAF668E
#+begin_src ein-python :results output
## specify your R par(mfrow=c(x,y))
f, axes = plt.subplots(1,2, figsize = (12, 6))
k1 = sns.kdeplot(mydat.posmean, mydat.nomean, ax = axes[0], cmap = 'Greens')
k2 = sns.kdeplot(mydat.posmean, mydat.negmean, ax = axes[1], cmap = 'Greens')
#+end_src

#+BEGIN_EXPORT html
<br>
<br>
#+END_EXPORT

#+begin_export html
 <img width="60%" height="100%" src="../../images/ob-ein-76f47b1626f35945ac24a2fdb16cd94b.png" class="center">
#+end_export

#+BEGIN_EXPORT html
<br>
<br>
#+END_EXPORT

with more columns the axis index becomes more complex.

#+NAME: 5E915FC1-4336-43F0-AC81-E6D4606DB7DC
#+begin_src ein-python :results output
## specify your R par(mfrow=c(x,y))
f, axes = plt.subplots(2,2, figsize = (12, 6))
k1 = sns.kdeplot(mydat.posmean, mydat.nomean, ax = axes[0,1], cmap = 'Greens')
k2 = sns.kdeplot(mydat.posmean, mydat.negmean, ax = axes[1,0], cmap = 'Greens')
#+end_src

#+begin_export html
 <img width="60%" height="100%" src="../../images/ob-ein-67a82e7e06d0dbde403da30e12915fb0.png" class="center">
#+end_export

#+BEGIN_EXPORT html
<br>
<br>
#+END_EXPORT

- violinplot

same information as boxplot. on top of it you have the width that
tells you how many datapoints falls into each level for each category

#+NAME: 7127AD7E-0D11-4E74-8D13-7E39AD57BE3C
#+begin_src ein-python :results output
k1 = sns.violinplot(data = mydat, x = 'bitwise', y = 'nomean')
#+end_src

#+begin_export html
 <img width="60%" height="100%" src="../../images/ob-ein-76e27ae206053454189960ae1872c755.png" class="center">
#+end_export

#+BEGIN_EXPORT html
<br>
<br>
#+END_EXPORT

- facet grids

this is useful when plotting many variables and inspecting their
properties for different categories.

you might do that as follows

#+NAME: 3C0D07B1-EACF-4670-8F81-A6A4C63368BE
#+begin_src ein-python :results output
## create some more categories
rand = np.random.randn(200)
category = ['2008' if x > 0 else '2009' for x in rand]
mydat['year'] = pd.Series(category).astype('category')
category = ['posExtreme' if x > 2 else 'normal' if x < 2 else 'negExtreme' for x in rand]
mydat['extreme'] = pd.Series(category).astype('category')

mydat.info()
#+end_src


#+NAME: 350A97B1-7BF7-491E-A074-D32855FDFC2B
#+begin_src ein-python :results output
set(['posExtreme' if x > 2 else 'normal' if x < -2 else 'negExtreme' for x in rand])
#+end_src


#+NAME: 9D15BE1A-47B1-4165-A2C5-B82E0738FBB5
#+begin_src ein-python :results output
g =  sns.FacetGrid(data = mydat, row = 'year', col = 'extreme', hue = 'year')
g.map(plt.scatter, x = 'nomean', y = 'posmean')
#+end_src

#+BEGIN_EXPORT html
<br>
<br>
#+END_EXPORT

#+begin_export html
 <img width="60%" height="100%" src="../../images/ob-ein-0b867ca8bdca2f2900ea9cfed8557013.png" class="center">
#+end_export

#+BEGIN_EXPORT html
<br>
<br>
#+END_EXPORT


- pairs plot

#+begin_src ein-python :session http://127.0.0.1:8888/data-visualization.ipynb
sns.set(style="ticks", color_codes=True)

## make a pair plot
columns = ['Happiness_Score','Economy_(GDP_per_Capita)', 'Family', 'Health_(Life_Expectancy)',
           'Freedom', 'Trust_(Government_Corruption)']

axes = sns.pairplot(df,vars=columns,hue="Year",palette="husl")
#+end_src

#+RESULTS: 0C7B2019-492C-4BF5-8008-C98ECC1B2A26

#+BEGIN_EXPORT html
<br>
<br>
#+END_EXPORT

#+begin_export html
 <img width="60%" height="100%" src="../../images/ob-ein-3a2a26107ad61803388d9c611a423e28.png" class="center">
#+end_export

#+BEGIN_EXPORT html
<br>
<br>
#+END_EXPORT

- correlation matrix plot

#+NAME: C42D6014-CB8C-494C-858F-0B9FC5034166
#+begin_src ein-python :results output  :session http://127.0.0.1:8888/data-visualization.ipynb
# Compute the correlation matrix
corr = df.corr()

# Generate a mask for the upper triangle
mask = np.triu(np.ones_like(corr, dtype=np.bool))

# Set up the matplotlib figure
f, ax = plt.subplots(figsize=(11, 9))

# Generate a custom diverging colormap
cmap = sns.diverging_palette(220, 10, as_cmap=True)

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5})
#+end_src

#+RESULTS: C42D6014-CB8C-494C-858F-0B9FC5034166


#+BEGIN_EXPORT html
<br>
<br>
#+END_EXPORT

#+begin_export html
 <img width="60%" height="100%" src="../../images/ob-ein-bd9b1dfc192e85b5422d813cc082a59d.png" class="center">
#+end_export

#+BEGIN_EXPORT html
<br>
<br>
#+END_EXPORT


- On PairGrid

This is a very interesting option to compute different visualization
according to pair plots

#+NAME: CDFB4918-09FE-49EA-AA84-95DA766B060B
#+begin_src ein-python :results output :session http://127.0.0.1:8888/data_visualization_self_done.ipynb
df = pd.read_csv("~/Desktop/Learning/AI_workflow_Coursera/Visualization/Visualization_2.csv")

df.columns

df = df.dropna(axis= 0)

#+end_src

#+RESULTS: CDFB4918-09FE-49EA-AA84-95DA766B060B
: Index(['customer_id', 'country_name', 'age', 'customer_name', 'is_subscriber',
:        'subscriber_type', 'num_streams'],
:       dtype='object')


#+NAME: F86A78CB-D318-4828-88E8-A1F7C73A4B30
#+begin_src ein-python :results output :session http://127.0.0.1:8888/data_visualization_self_done.ipynb
g = sns.PairGrid(df[['age', 'num_streams', 'is_subscriber']], hue = 'is_subscriber')
g = g.map_upper(sns.scatterplot)
g = g.map_lower(sns.kdeplot, colors="C0")
g = g.map_diag(sns.kdeplot, lw=2).add_legend()
#+end_src

#+RESULTS: F86A78CB-D318-4828-88E8-A1F7C73A4B30

#+BEGIN_EXPORT html
<br>
<br>
#+END_EXPORT

#+begin_export html
 <img width="60%" height="100%" src="../../images/ob-ein-6d0a3dbe8709898e545dbecff8afb91b.png" class="center">
#+end_export

#+BEGIN_EXPORT html
<br>
<br>
#+END_EXPORT

You can then specify pretty much everything that you want on the upper
and lower diagonal matrix entries. Check at this link to [[https://seaborn.pydata.org/generated/seaborn.PairGrid.html][get inspiration]].

You might even combine pyplot and seaborn elements

#+NAME: AAF6B0F8-46A5-40ED-8187-969386ED60AF
#+begin_src ein-python :results output :session http://127.0.0.1:8888/data_visualization_self_done.ipynb
g = sns.PairGrid(df[['age', 'num_streams', 'is_subscriber']], hue = 'is_subscriber')
g = g.map_upper(sns.scatterplot, alpha = 0.3)
g = g.map_lower(sns.kdeplot)
g = g.map_diag(plt.hist, lw=2, alpha = 0.2)
#+end_src

#+RESULTS: AAF6B0F8-46A5-40ED-8187-969386ED60AF

#+BEGIN_EXPORT html
<br>
<br>
#+END_EXPORT

#+begin_export html
 <img width="60%" height="100%" src="../../images/ob-ein-6d61b403db2296406af36083af76f80a.png" class="center">
#+end_export

#+BEGIN_EXPORT html
<br>
<br>
#+END_EXPORT

