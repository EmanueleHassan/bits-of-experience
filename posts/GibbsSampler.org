#+BEGIN_COMMENT
.. title: Gibbs Sampler
.. slug: Gibbs-Sampler
.. date: 2019-12-18 15:00:53 UTC+02:00
.. tags: Simulation
.. category: 
.. link: 
.. description: 
.. type: text
.. status: xk
.. has_math: yes

#+END_COMMENT

#+BEGIN_HTML
<br>
<br>
#+END_HTML

This post explores the Gibbs sampler algorithm. 

In Bayesian Statistics sometimes it is not possible to integrate
analytically over the posterior distribution. It is hence not possible
to minimize over the loss function spanned by the posterior
distribution of the parameters of interest in order to take the bayes
decision.

A solution in this case is to leverage simulation to obtain an
i.i.d. sample of the posterior distribution of the parameters of
interest. Given an i.i.d. sample of it is then possible to leverage
the properties of the CLT to get an unbiased estimate of the
parameters of interest. This is for instance the backbone of
simulation methods such as Monte Carlo.

A further evolution consists in generating a dependent sample from the
distribution of interest through a reversible Markov Chain. In such a
case it is possible to prove that the series generated from the Markov
Chain will converge to the stationary equilibrium of the Chain -
i.e. the distribution of interest - such that it is possible at a
later stage to apply the CLT to the converged series to obtain
estimates and CI for the parameters of interest.

In this sense the Gibbs Sampler is an algorithm generating explicitly
a reversible Markov Chain that gives the opportunity to obtain
estimates for the parameter of interest from a nasty, analytically not
integrable posterior distribution.

To illustrate the algorithm the post looks now at the simple case of
sampling from a bivariate distribution.

{{{TEASER_END}}}

*** Generate from a Bivariate leveraging Cholesky
    :properties:
    :header-args:ipython: :session hello :async t :exports both
    :end:

 Import the 

 #+begin_src ipython :exports code
 import numpy as np
 import seaborn as sns ## for plotting joint

 import matplotlib.pyplot as plt
 %matplotlib inline 
 %config InlineBackend.figure_format = 'png'
 #+end_src

 #+RESULTS:
 : # Out[391]:

 #+begin_src ipython :exports code
 rho = 0.6

 corr = np.array([[1, 0.8],[0.8, 1]])
 #+end_src

 #+RESULTS:
 : # Out[143]:

 Generate a sample from a bivariate distribution with the above
 mentioned correlation structure.

 #+begin_src ipython :results output
 N = 1000
 np.random.seed(1) ## for reproduceability. 

 ## generate standard normal sample
 sn=np.column_stack([np.random.normal(0,1,N), np.random.normal(0,1,N)])

 ## convert it to generate a bivariate sample with the given
 ## correlation structure.

 means = [0, 0]         # Mean values of each column
 sds = [1, 1]           # SD of each column
 sd = np.diag(sds)         # SD in a diagonal matrix for later operations

 cov_matrix = np.dot(sd, np.dot(corr, sd))   # The covariance matrix

 Chol = np.linalg.cholesky(cov_matrix)             # Cholesky decomposition

 sam_eq_mean = Chol .dot(sn.T)             # Generating random MVN (0, cov_matrix)

 s = sam_eq_mean.transpose() + means               # Adding the means column wise
 samples = s.transpose()                           # Transposing back

 print(np.corrcoef(samples))                       # Checking correlation consistency.

 #+end_src

 #+RESULTS:
 : [[1.         0.79080652]
 :  [0.79080652 1.        ]]

 #+begin_src ipython :results file
 sns.jointplot(x = samples[0, :], y = samples[1, :]) \
    .plot_joint(sns.kdeplot, zorder=3, n_levels=6)
 #+end_src

 #+RESULTS:
 [[img-url:/images/obipy-resources/eTz0eo.png]]

*** Sample from a Bivariate Using Gibbs Sampler
    :properties:
    :header-args:ipython: :session hello :async t :exports both
    :end:

 Using Cholesky decomposition it is easy to derive the conditional 
 distribution of a bivariate distribution, as shown in [[https://www2.stat.duke.edu/courses/Spring12/sta104.1/Lectures/Lec22.pdf][this lecture]].

 Given such conditional distribution the Gibbs Sampler generates then a
 reversible Markov Chain by iterating over the following two steps:

 - Generates $X^{t}_{I_t} ~ \phi_{I_t}{X^{t}_{I_t} | X{t-1}_{-I_t}}$,
   following a deterministic or random process I_t = {1,2,...,k} where
   k is large enough to sample from all the possible space of the
   conditional distribution \phi and t=1,2... represent the epoch in ML
   terms.

 - Update X^{t-1}_{I_t-1} to X^{t}_{I_t} and continue to iterate over
   the two steps according to the visiting schedule conditioning on the
   updated vector containing the newer sampled variables.

 Algorithmically in python this involves:

 #+begin_src ipython :exports code
 ## specify the two conditional distributions

 def p_x_given_y(y, mean, covar):
     mu = mean[0] + covar[1, 0] / covar[0, 0] * (y - mean[1])
     sigma = covar[0, 0] - covar[1, 0] / covar[1, 1] * covar[1, 0]
     return np.random.normal(mu, sigma)


 def p_y_given_x(x, mean, covar):
     mu = mean[1] + covar[0, 1] / covar[1, 1] * (x - mean[0])
     sigma = covar[1, 1] - covar[0, 1] / covar[0, 0] * covar[0, 1]
     return np.random.normal(mu, sigma)

 ## sample according to the defined vistiing schedule (here
 ## deterministic ordered {1,2,...,k}
 def gibbs_sampling(init_Value, means, cov_matrix, iter=10000):
     samples = np.zeros((iter, 2)) ## generate an empty vector.
     y = init_Value ## set the initial value

     for i in range(iter):
         x = p_x_given_y(y, means, cov_matrix) ## notice how y gets
                                               ## automatically updated
                                               ## each at each
                                               ## iteration with the
                                               ## value below
         y = p_y_given_x(x, means, cov_matrix)
         samples[i, :] = [x, y]

     return samples
 #+end_src

 #+RESULTS:
 : # Out[448]:

 #+begin_src ipython :exports code
 gibbsSamples = gibbs_sampling(5, means, cov_matrix, iter =1000)
 #+end_src

 #+RESULTS:
 : # Out[541]:

 #+begin_src ipython :results file
 sns.jointplot(x = gibbsSamples.T[0, :], y = gibbsSamples.T[1, :]) \
    .plot_joint(sns.kdeplot, zorder=3, n_levels=6)
 #+end_src

 #+RESULTS:
 [[img-url:/images/obipy-resources/t4dF4t.png]]


 The above looks graph does not display a bivariate distribution with
 the moments of the underlying distribution. This is because the first
 samples where the Markov Chain did not converge to the underlying
 distribution are included in the plotted samples with the result that
 the latter is in fact bias.

 To obviate such a problem it is possible to inspect the times series
 plot for the two series and just include the observations for which
 the chain converged to the desired distribution and is hence mean
 stationary.

 #+begin_src ipython :exports code
 from statsmodels.graphics.tsaplots import plot_acf
 #+end_src

 #+RESULTS:
 : # Out[543]:

 #+begin_src ipython :results file
 plot_acf(gibbsSamples[:,0])
 plt.show()
 #+end_src

 #+RESULTS:
 [[file:# Out[572]:
 [[img-url:/images/obipy-resources/3vlKhV.png]]]]


 As expected the samples are autocorrelated due to the very nature of
 Markov Chains.

 #+begin_src ipython :results file
 plt.plot(gibbsSamples[:,1])
 plt.show()
 #+end_src

 #+RESULTS:
 [[file:# Out[578]:
 [[img-url:/images/obipy-resources/ijg19g.png]]]]

 #+begin_src ipython :results file
 sns.jointplot(x = gibbsSamples.T[0, 10:], y = gibbsSamples.T[1, 10:]) \
    .plot_joint(sns.kdeplot, zorder=3, n_levels=6)
 #+end_src

 #+RESULTS:
 [[file:# Out[580]:
 : <seaborn.axisgrid.JointGrid at 0x13fb99c50>
 [[img-url:/images/obipy-resources/pU3V8t.png]]]]


 Confront the two empirical distributions w.r.t. the theoretical
 quantiles of a normal.

 #+begin_src ipython :exports code
 import scipy.stats as stats
 #+end_src


 #+RESULTS:
 : # Out[656]:

 #+begin_src ipython :results file

 fig, ax = plt.subplots(2, 2, sharex='col', sharey='row')

 stats.probplot(gibbsSamples.T[0, :], dist="norm", plot = plt.subplot(2, 2, 1))
 stats.probplot(gibbsSamples.T[1, :], dist="norm", plot = plt.subplot(2, 2, 2))

 stats.probplot(gibbsSamples.T[0, 10:], dist="norm", plot = plt.subplot(2, 2, 3))
 stats.probplot(gibbsSamples.T[1, 10:], dist="norm", plot = plt.subplot(2, 2, 4))

 plt.subplot(2,2,3).set_title("W/o Burn in")
 plt.subplot(2,2,4).set_title("W/o Burn in")

 plt.show()

 #+end_src

 #+RESULTS:
 [[file:# Out[667]:
 [[img-url:/images/obipy-resources/Du6NLL.png]]]]


