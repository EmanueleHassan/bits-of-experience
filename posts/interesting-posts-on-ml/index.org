#+BEGIN_COMMENT
.. title: A list of interesting Posts On ML techniques
.. slug: interesting-posts-on-ml
.. date: 2020-07-21 21:40:24 UTC+02:00
.. tags: Machine Learning
.. category: 
.. link: 
.. description: 
.. type: text

#+END_COMMENT


This is a list of good material I found on various ML techniques. Some
you might want to explore them deeper in later posts. Some will just
stay as they are here.

{{{TEASER_END}}}

#+BEGIN_EXPORT html
<br>
<br>
#+END_EXPORT

* Neural Networks Times Series

Best explaination in the world wide web for [[https://colah.github.io/posts/2015-08-Understanding-LSTMs/][LSTM models architecture]].  

A good [[https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks][cheat sheet on RNN.]] On the website you can find other similar
scripts for other Neural Networks Architectures.


* Regression Methods

** Support Vector Regression.

 This is an interesting technique. [[https://www.mathworks.com/help/stats/understanding-support-vector-machine-regression.html][It draws on the idea of SVM]].


* Outlier Detection


** Isolation Forest
:properties:
:header-args:ein-python: :session http://127.0.0.1:8888/EDA.ipynb :tangle ~/Desktop/test.py
:header-args:python: :session hello :file-name ~/Desktop/test.py :tangle ~/Desktop/test.py 
:end: 


#+NAME: 4817F5D4-D98F-40EB-890F-B392AE192CB3
#+begin_src ein-python :results output 
#!START_LIB
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import IsolationForest
#!END_LIB
#+end_src

#+RESULTS: 4817F5D4-D98F-40EB-890F-B392AE192CB3


#+NAME: CE68EB4B-E683-4C32-AEED-A071BEB01670
#+begin_src ein-python :results output
rng = np.random.RandomState(42)

X = 0.3 * rng.randn(100, 2)
X_train = np.r_[X + 2, X - 2]
#+end_src

#+NAME: 35669D45-CEB5-4B88-9474-759962100F16
#+begin_src ein-python :results output 
# Generate some regular novel observations
X = 0.3 * rng.randn(20, 2)
X_test = np.r_[X + 2, X - 2]
# Generate some abnormal novel observations
X_outliers = rng.uniform(low=-4, high=4, size=(20, 2))
#+end_src

Obviously from the generated process you would have quite values >= 3,
which knowing the basics of the normal should be quite rare
observations in our train sample.

#+NAME: 340BFE35-6149-4011-9AFC-7C00DCC25BE5
#+begin_src ein-python :results output
# fit the model
clf = IsolationForest(max_samples=100, random_state=rng)
clf.fit(X_train)
y_pred_train = clf.predict(X_train)
y_pred_test = clf.predict(X_test)
y_pred_outliers = clf.predict(X_outliers)
#+end_src


#+NAME: 9CE8DE60-C5CB-49DA-AEDB-106425C022EE
#+begin_src python :results output
# plot the line, the samples, and the nearest vectors to the plane
xx, yy = np.meshgrid(np.linspace(-5, 5, 50), np.linspace(-5, 5, 50))
Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
#+end_src


#+begin_src python :results output
# fit the model
clf = IsolationForest(max_samples=100, random_state=rng)
clf.fit(X_train)
y_pred_train = clf.predict(X_train)
y_pred_test = clf.predict(X_test)
y_pred_outliers = clf.predict(X_outliers)

# plot the line, the samples, and the nearest vectors to the plane
xx, yy = np.meshgrid(np.linspace(-5, 5, 50), np.linspace(-5, 5, 50))
Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

plt.title("IsolationForest")
plt.contourf(xx, yy, Z, cmap=plt.cm.Blues_r)

b1 = plt.scatter(X_train[:, 0], X_train[:, 1], c='white',
                 s=20, edgecolor='k')
b2 = plt.scatter(X_test[:, 0], X_test[:, 1], c='green',
                 s=20, edgecolor='k')
c = plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c='red',
                s=20, edgecolor='k')
plt.axis('tight')
plt.xlim((-5, 5))
plt.ylim((-5, 5))
plt.legend([b1, b2, c],
           ["training observations",
            "new regular observations", "new abnormal observations"],
           loc="upper left")
plt.show()
#+end_src



