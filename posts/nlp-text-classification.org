#+BEGIN_COMMENT
.. title: NLP text classification
.. slug: nlp-text-classification
.. date: 2020-05-20 22:22:37 UTC+02:00
.. tags: 
.. category: 
.. link: 
.. description: 
.. type: text

#+END_COMMENT

#+BEGIN_EXPORT html
<br>
<br>
#+END_EXPORT

 This post briefly introduce two key concepts for representing NLP
 text data. The first is simply based on some frequency metric given a
 text corpus, the second =word2vec= tries a more elegant and elaborate
 approach by trying to extract the semantical representation of words
 in a low dimensional space.

 Both will be briefly explained. 

 {{{TEASER_END}}}

#+BEGIN_EXPORT html
<br>
#+END_EXPORT

*** Approach 1: On a Frequency Representation 

 The first approach consists in directly leveraging the vocabulary
 associated with the category that you aim to categorize.

 This is then essentially the idea of the algorithm that you have
 created manually for the SRZ project. The idea is simply to use a
 more standard ML model for classification such as logistic
 regression, SVM or Naive Bayes.

 The concept is however always the same. Get all the words related to
 one category and assign the new text to a category based on a metric
 using such bag of words. If a new corpus of words arrives that has no
 relation to the one observed in the training dataset, then
 well... your algorithm will completely fail the classification task.

 More explicitly, The bag of words associated with a specific category
 gives rise to the features matrix. This is a vector representation of
 the words. A straight representation could be for instance a
 one-hot-encoding of the words available for each class. But more
 advanced vector representations involving the term frequency might be
 used.

 Some quick introduction to the idea behind some standard
 representations belonging to this category might be found under the
 [[https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/][following link]].

 You can then apply standard classification algorithms to such vector
 representation of your text corpus.


#+BEGIN_EXPORT html
<br>
#+END_EXPORT


*** Approach 2: On a Semantic Representation

 A second technique that is more involving but conceptually cleaner is
 the one of using Word2Vec embeddings. This consists in extracting the
 semantic meaning of a word by looking in which context it
 appears. I.e. it inspects the "friends" of the word to understand the
 word itself. This is a nice [[https://en.wikipedia.org/wiki/Anthropomorphism#In_film,_television,_and_video_games][anthropomorphic]] way to think about the
 approach I recently read on a blog, which I particularly enjoyed.

 You can get the gist of the idea behind word2vec [[https://towardsdatascience.com/word-to-vectors-natural-language-processing-b253dd0b0817][here]].

 The mathematical idea is essentially to extract a vector
 representation for each word based on a 1-layered neural network
 trained on your corpus of contex-related data.

 More concretely you can think about it in the following simplified
 way: given a N-bag of N-related words around a center word *V_C*
 represented by a vector, think of a hidden layer of N-nodes (neurons)
 assigning weights to each of the input N-words such that the
 probability of observing the N-bag of words given the central word is
 maximized. Such weights would represent then the vector
 representation of each word.

#+BEGIN_EXPORT html
<br>
#+END_EXPORT

#+begin_export html
 <img width="100%" height="100%" src="../../images/Bildschirmfoto_2020-05-20_um_21.24.25.png" class="center" align = "right">
#+end_export


 The probability is then represented in the most simple case by the
 softmax funtion of the doct-products of the N-bag of words vectors
 and the centroid vector.

#+BEGIN_EXPORT html
<br>
#+END_EXPORT

#+begin_export html
 <img width="100%" height="100%" src="../../images/Bildschirmfoto_2020-05-20_um_20.11.40.png" class="center" align = "right">
#+end_export


#+BEGIN_EXPORT html
<br>
#+END_EXPORT

 The weights are obtained by standard backpropagation with a loss
 function representing the cross entropy between the truth vector and
 the softmax "probability" vector. 

 The final architecture is well summarized on the below picture.

#+BEGIN_EXPORT html
<br>
#+END_EXPORT

#+begin_export html
 <img width="100%" height="100%" src="../../images/Bildschirmfoto_2020-05-20_um_22.13.03png" class="center">
#+end_export

#+BEGIN_EXPORT html
<br>
#+END_EXPORT



 For a more in depth analysis of the key mathematical ideas of
 word2vec the following video is advised:

 #+begin_export html
 <iframe width="800" height="400"
 src="https://www.youtube.com/embed/ERibwqs9p38">
 </iframe>
 #+end_export

#+BEGIN_EXPORT html
<br>
#+END_EXPORT

 Interesting is then the choice on how you define the bag of N-words
 used by the algorithm. A quick introduction to the topic might be
 found [[https://kavita-ganesan.com/comparison-between-cbow-skipgram-subword/][here]].

 Notice, that the approach not only has the benefit of being
 theoretically more sound but it also gives the advantage that if the
 semantic embeddings of your text corpus have been properly inferred
 via ML algorithms and can be represented in low dimensional space you
 might start classifying your text corpus based on the geometrical
 representation of your classes and the text you aim to classify. The
 idea is that if the text and the class share the same semantical
 meaning they should lie in some close geometrical subspace. Using
 some unsupervised algorithm to define such geometrical subspace (or
 even using some simple metrics) you can then properly classify.
