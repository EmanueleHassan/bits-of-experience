<!DOCTYPE html>
<html prefix="        og: http://ogp.me/ns# article: http://ogp.me/ns/article#     " vocab="http://ogp.me/ns" lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<title>Gibbs Sampler | Bits of Experience</title>
<link href="https://fonts.googleapis.com/css?family=Bitter:400,400i,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.13/css/all.css" integrity="sha384-DNOHZ68U8hZfKXOrtjWvjxusGo9WQnrNx2sqG0tfsghAvtVlRW3tvkXWZh58N9jp" crossorigin="anonymous">
<link href="../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" hreflang="en" href="../../rss.xml">
<link rel="canonical" href="https://marcohassan.github.io/bits-of-experience/posts/Gibbs-Sampler/">
<!--[if lt IE 9]><script src="../../assets/js/html5shiv-printshiv.min.js"></script><![endif]--><meta name="author" content="Marco Hassan">
<link rel="prev" href="../factoria-treatment-structure/" title="Factorial Treatment Structure" type="text/html">
<link rel="next" href="../On%20the%20Monty%20Hall%20Problem%20and%20the%20importance%20of%20understanding%20the%20underlying%20uncertainty%20source/" title="On the Monty Hall Problem and the importance of understanding the underlying uncertainty source" type="text/html">
<meta property="og:site_name" content="Bits of Experience">
<meta property="og:title" content="Gibbs Sampler">
<meta property="og:url" content="https://marcohassan.github.io/bits-of-experience/posts/Gibbs-Sampler/">
<meta property="og:description" content="This post explores the Gibbs sampler algorithm. 



In Bayesian Statistics sometimes it is not possible to integrate
analytically over the posterior distribution. It is hence not possible
to minimize ">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2019-12-18T15:00:53+02:00">
<meta property="article:tag" content="Simulation">
</head>
<body>
    <a href="#page-content" class="sr-only sr-only-focusable">Skip to main content</a>
    
    <section class="social"><ul>
<li><a href="../../pages/aboutme/" title="About me"><i class="fa fa-user-circle"></i></a></li>
            <li><a href="../../pages/bits-of-experience-a-readable-view-on-my-study-adventures/" title="Home"><i class="fa fa-home"></i></a></li>
            <li><a href="../../index.html" title="Blog"><i class="fa fa-edit"></i></a></li>
            <li><a href="../../pages/emacs/" title="A life Configuring Emacs"><i class="fa fa-code"></i></a></li>
            <li><a href="../../pages/papers/" title="Term and Research Papers"><i class="fa fa-university"></i></a></li>
            <li><a href="../../pages/foto-blog/" title="Foto Blog"><i class="fa fa-camera-retro"></i></a></li>
            <li><a href="https://github.com/MarcoHassan" title="My Github"><i class="fab fa-github"></i></a></li>
            <li><a href="https://stackoverflow.com/users/9731177/mhass" title="My Stack"><i class="fab fa-stack-overflow"></i></a></li>
            <li><a href="../../rss.xml" title="RSS"><i class="fa fa-rss"></i></a></li>
            <li><a href="../../categories/index.html" title="Tags"><i class="fa fa-tags"></i></a></li>
    
    

        </ul></section><section class="page-content"><div class="content" rel="main">
            
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">Gibbs Sampler</a></h1>

        <div class="metadata">
            <p class="dateline"><a href="." rel="bookmark"><i class="fa fa-clock"></i> <time class="published dt-published" datetime="2019-12-18T15:00:53+02:00" itemprop="datePublished" title="2019-12-18 15:00">2019-12-18 15:00</time></a></p>
            <p class="byline author vcard"> <i class="fa fa-user"></i> <span class="byline-name fn" itemprop="author">
                    Marco Hassan
            </span></p>
            
        <p class="sourceline"><a href="index.org" class="sourcelink"><i class="fa fa-file-code"></i> Source</a></p>

            

            
    <div class="tags">
<h3 class="metadata-title">
<i class="fa fa-tags"></i> Tags:</h3>
        <ul itemprop="keywords" class="tags-ul">
<li><a class="tag p-category" href="../../categories/simulation/" rel="tag">Simulation</a></li>
        </ul>
</div>

        </div>
    </header><div class="e-content entry-content" itemprop="articleBody text">
    <br><br><p>
This post explores the Gibbs sampler algorithm. 
</p>

<p>
In Bayesian Statistics sometimes it is not possible to integrate
analytically over the posterior distribution. It is hence not possible
to minimize over the loss function spanned by the posterior
distribution of the parameters of interest in order to take the bayes
decision.
</p>

<p>
A solution in this case is to leverage simulation to obtain an
i.i.d. sample of the posterior distribution of the parameters of
interest. Given an i.i.d. sample of it is then possible to leverage
the properties of the CLT to get an unbiased estimate of the
parameters of interest. This is for instance the backbone of
simulation methods such as Monte Carlo.
</p>

<p>
A further evolution consists in generating a dependent sample from the
distribution of interest through a reversible Markov Chain. In such a
case it is possible to prove that the series generated from the Markov
Chain will converge to the stationary equilibrium of the Chain -
i.e. the distribution of interest - such that it is possible at a
later stage to apply the CLT to the converged series to obtain
estimates and CI for the parameters of interest.
</p>

<p>
In this sense the Gibbs Sampler is an algorithm generating explicitly
a reversible Markov Chain that gives the opportunity to obtain
estimates for the parameter of interest from a nasty, analytically not
integrable posterior distribution.
</p>

<p>
To illustrate the algorithm the post looks now at the simple case of
sampling from a bivariate distribution.
</p>

<!-- TEASER_END -->

<div id="outline-container-org08ea89a" class="outline-2">
<h2 id="org08ea89a">Generate from a Bivariate leveraging Cholesky</h2>
<div class="outline-text-2" id="text-org08ea89a">
<p>
Import the 
</p>

<div class="highlight"><pre><span></span> import numpy as np
 import seaborn as sns ## for plotting joint

 import matplotlib.pyplot as plt
 %matplotlib inline 
 %config InlineBackend.figure_format = 'png'
</pre></div>

<div class="highlight"><pre><span></span> rho = 0.6

 corr = np.array([[1, 0.8],[0.8, 1]])
</pre></div>

<p>
Generate a sample from a bivariate distribution with the above
mentioned correlation structure.
</p>

<div class="highlight"><pre><span></span> N = 1000
 np.random.seed(1) ## for reproduceability. 

 ## generate standard normal sample
 sn=np.column_stack([np.random.normal(0,1,N), np.random.normal(0,1,N)])

 ## convert it to generate a bivariate sample with the given
 ## correlation structure.

 means = [0, 0]         # Mean values of each column
 sds = [1, 1]           # SD of each column
 sd = np.diag(sds)         # SD in a diagonal matrix for later operations

 cov_matrix = np.dot(sd, np.dot(corr, sd))   # The covariance matrix

 Chol = np.linalg.cholesky(cov_matrix)             # Cholesky decomposition

 sam_eq_mean = Chol .dot(sn.T)             # Generating random MVN (0, cov_matrix)

 s = sam_eq_mean.transpose() + means               # Adding the means column wise
 samples = s.transpose()                           # Transposing back

 print(np.corrcoef(samples))                       # Checking correlation consistency.
</pre></div>


<div class="highlight"><pre><span></span> sns.jointplot(x = samples[0, :], y = samples[1, :]) \
    .plot_joint(sns.kdeplot, zorder=3, n_levels=6)
</pre></div>

<p>
<img src="../../images/obipy-resources/eTz0eo.png" alt="nil"></p>
</div>
</div>

<div id="outline-container-org9286a09" class="outline-2">
<h2 id="org9286a09">Sample from a Bivariate Using Gibbs Sampler</h2>
<div class="outline-text-2" id="text-org9286a09">
<p>
Using Cholesky decomposition it is easy to derive the conditional 
distribution of a bivariate distribution, as shown in <a href="https://www2.stat.duke.edu/courses/Spring12/sta104.1/Lectures/Lec22.pdf">this lecture</a>.
</p>

<p>
Given such conditional distribution the Gibbs Sampler generates then a
reversible Markov Chain by iterating over the following two steps:
</p>

<ul class="org-ul">
<li>Generates \(X^{t}_{I_t} ~ \phi_{I_t}{X^{t}_{I_t} | X{t-1}_{-I_t}}\),
following a deterministic or random process I<sub>t</sub> = {1,2,…,k} where
k is large enough to sample from all the possible space of the
conditional distribution φ and t=1,2… represent the epoch in ML
terms.</li>

<li>Update X<sup>t-1</sup><sub>I<sub>t</sub>-1</sub> to X<sup>t</sup><sub>I<sub>t</sub></sub> and continue to iterate over
the two steps according to the visiting schedule conditioning on the
updated vector containing the newer sampled variables.</li>
</ul>
<p>
Algorithmically in python this involves:
</p>

<div class="highlight"><pre><span></span> ## specify the two conditional distributions

 def p_x_given_y(y, mean, covar):
     mu = mean[0] + covar[1, 0] / covar[0, 0] * (y - mean[1])
     sigma = covar[0, 0] - covar[1, 0] / covar[1, 1] * covar[1, 0]
     return np.random.normal(mu, sigma)


 def p_y_given_x(x, mean, covar):
     mu = mean[1] + covar[0, 1] / covar[1, 1] * (x - mean[0])
     sigma = covar[1, 1] - covar[0, 1] / covar[0, 0] * covar[0, 1]
     return np.random.normal(mu, sigma)

 ## sample according to the defined vistiing schedule (here
 ## deterministic ordered {1,2,...,k}
 def gibbs_sampling(init_Value, means, cov_matrix, iter=10000):
     samples = np.zeros((iter, 2)) ## generate an empty vector.
     y = init_Value ## set the initial value

     for i in range(iter):
	 x = p_x_given_y(y, means, cov_matrix) ## notice how y gets
					       ## automatically updated
					       ## each at each
					       ## iteration with the
					       ## value below
	 y = p_y_given_x(x, means, cov_matrix)
	 samples[i, :] = [x, y]

     return samples
</pre></div>

<div class="highlight"><pre><span></span> gibbsSamples = gibbs_sampling(5, means, cov_matrix, iter =1000)
</pre></div>

<div class="highlight"><pre><span></span> sns.jointplot(x = gibbsSamples.T[0, :], y = gibbsSamples.T[1, :]) \
    .plot_joint(sns.kdeplot, zorder=3, n_levels=6)
</pre></div>

<p>
<img src="../../images/obipy-resources/t4dF4t.png" alt="nil"></p>


<p>
The above looks graph does not display a bivariate distribution with
the moments of the underlying distribution. This is because the first
samples where the Markov Chain did not converge to the underlying
distribution are included in the plotted samples with the result that
the latter is in fact bias.
</p>

<p>
To obviate such a problem it is possible to inspect the times series
plot for the two series and just include the observations for which
the chain converged to the desired distribution and is hence mean
stationary.
</p>

<div class="highlight"><pre><span></span> from statsmodels.graphics.tsaplots import plot_acf
</pre></div>

<div class="highlight"><pre><span></span> plot_acf(gibbsSamples[:,0])
 plt.show()
</pre></div>

<p>
<img src="../../images/obipy-resources/3vlKhV.png" alt="nil"></p>


<p>
As expected the samples are autocorrelated due to the very nature of
Markov Chains.
</p>

<div class="highlight"><pre><span></span> plt.plot(gibbsSamples[:,1])
 plt.show()
</pre></div>

<p>
<img src="../../images/obipy-resources/ijg19g.png" alt="nil"></p>

<div class="highlight"><pre><span></span> sns.jointplot(x = gibbsSamples.T[0, 10:], y = gibbsSamples.T[1, 10:]) \
    .plot_joint(sns.kdeplot, zorder=3, n_levels=6)
</pre></div>

<p>
<img src="../../images/obipy-resources/pU3V8t.png" alt="nil"></p>


<p>
Confront the two empirical distributions w.r.t. the theoretical
quantiles of a normal.
</p>

<div class="highlight"><pre><span></span> import scipy.stats as stats
</pre></div>


<div class="highlight"><pre><span></span> fig, ax = plt.subplots(2, 2, sharex='col', sharey='row')

 stats.probplot(gibbsSamples.T[0, :], dist="norm", plot = plt.subplot(2, 2, 1))
 stats.probplot(gibbsSamples.T[1, :], dist="norm", plot = plt.subplot(2, 2, 2))

 stats.probplot(gibbsSamples.T[0, 10:], dist="norm", plot = plt.subplot(2, 2, 3))
 stats.probplot(gibbsSamples.T[1, 10:], dist="norm", plot = plt.subplot(2, 2, 4))

 plt.subplot(2,2,3).set_title("W/o Burn in")
 plt.subplot(2,2,4).set_title("W/o Burn in")

 plt.show()
</pre></div>

<p>
<img src="../../images/obipy-resources/Du6NLL.png" alt="nil"></p>
</div>
</div>
    </div>
    <aside class="postpromonav"><nav><ul class="pager hidden-print">
<li class="previous">
                <a href="../factoria-treatment-structure/" rel="prev" title="Factorial Treatment Structure">Previous post</a>
            </li>
            <li class="next">
                <a href="../On%20the%20Monty%20Hall%20Problem%20and%20the%20importance%20of%20understanding%20the%20underlying%20uncertainty%20source/" rel="next" title="On the Monty Hall Problem and the importance of understanding the underlying uncertainty source">Next post</a>
            </li>
        </ul></nav></aside><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha384-3lJUsx1TJHt7BA4udB5KPnDrlkO8T6J6v/op7ui0BbCjvZ9WqV4Xm6DTP6kQ/iBH" crossorigin="anonymous"></script><script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    displayAlign: 'center', // Change this to 'left' if you want left-aligned equations.
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": 0}}
    }
});
</script></article><footer id="footer"><p>Contents © 2022         <a href="mailto:marco.hassan30@gmail.com">Marco Hassan</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         </p>
            
        </footer>
</div>
    </section><script src="../../assets/js/all-nocdn.js"></script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(2, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script>
</body>
</html>
