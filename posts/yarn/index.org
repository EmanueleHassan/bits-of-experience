#+BEGIN_COMMENT
.. title: YARN
.. slug: yarn
.. date: 2020-05-23 14:21:55 UTC+02:00
.. tags: Big Data
.. category: 
.. link: 
.. description: 
.. type: text

#+END_COMMENT

#+BEGIN_EXPORT html
<br>
<br>
#+END_EXPORT

This post briefly introduces YARN - *Yet Another Resource Negotiator*.
It was introduced to overcome the limits of MapReduce v 1.0,
especially the idle resources component, [[https://marcohassan.github.io/bits-of-experience/posts/mapreduce/][discussed in the previous post]].

{{{TEASER_END}}}

#+BEGIN_EXPORT html
<br>
#+END_EXPORT

* Issues with MapReduce v 1.0


    In MapReduce 1, you had a single Master - the JobTracker - being
    responsible for too many tasks. This created a bottleneck. The
    responsibilities of the JobTracker involved among the rest: 

          - the resource management scheduling; i.e. assigning
            different jobs to different *slots*/*containers*
            (virtualized hardware) among the machines.

	  - the monitor; i.e. checking the smooth operations of the
            jobs and that there was no failure. 

	  - job lifecycle; i.e. upon failure reschedule the jobs.


    Finally, the possible greatest shortcoming of MapReduce was the
    static allocation of slots. These are assigned in Mapreduce1 at
    the beginning -i.e. in the *configuration* time, saving X Cores
    and Y GB of memory on different machines for Reducer and Mappers
    operations. It is then clear that being MapReduce a sequential
    operation, the resources of the Reducers would sit idle, while the
    Mappers are performing.

#+BEGIN_EXPORT html
<br>
#+END_EXPORT

* The YARN Solution

    The idea of YARN was then to reduce the amount of operations the
    JobTracker was responsible for. The key idea was the one of
    separating the monitoring and scheduling tasks therefore
    increasing the overall performance.

    The idea was therefore to elect two different Masters types. 

    One - the ResourceManager - acting as the Master for the entire
    cluster of machines, being responsible for connecting with the
    client and scheduling the jobs. 

    One - the Application Master - working on at the Node/slave level
    being responsible for monitoring the jobs, negotiating the
    resources needed for them with the ResourceManager.

    Important, is that, in contrast to the old version, the
    Application Master would now negotiate the slots for the resources
    needed with the ResourceManager in a separate way for the Mapper
    and Reducer operation. That is it will first ask resources for the
    Mappers without reserving any slots for the Reducers. Just when
    the Mappers are done it will free up the slots assigned for such
    operations and ask new resources for performing the Reducer
    operations. You can understand that with this new architectural
    design no resources would sit idle waiting for being assigned.

* Literature

[[https://www.systems.ethz.ch/courses/spring2020/bigdataforeng/material][Big Data for Engineers - ETH course]]


