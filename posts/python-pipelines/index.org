#+BEGIN_COMMENT
.. title: Python Pipelines
.. slug: python-pipelines
.. date: 2020-07-02 12:44:32 UTC+02:00
.. tags: 
.. category: 
.. link: 
.. description: 
.. type: text

#+END_COMMENT

Pipelines are important when working on ML projects in python. They
first appeared through the implementation of ML tasks sklearn. There
the design setting of the maintainer of the library was to design a
set of APIs through which to govern the entire ML process. The three
major class of APIs are =tranformers=, i.e. transformations to the
feature and dependent variables, =estimators=, i.e. the ML model that
you fit on your transformed features matrix, and finally =predictors=,
i.e. the API method used for predicting your desired variable based on
your fitted estimator.

The above logic became so widespread that further important
libraries - such as the spark ML modules for using ML models
leveraging the spark engine for distributed computation - decided to
keep the same design setting and keep the above API structure.

This posts aims at keeping an overview of some important transformers,
estimators and predictors as well as to set up the general framework
to combine the three together in easy to use Pipelines. 

{{{TEASER_END}}}

#+BEGIN_EXPORT html
<br>
<br>
#+END_EXPORT

* The general Case 
:properties:
:header-args:python: :session pipeline :results output :exports both
:end:

In fact, all of the ML models in the sklearn and Spark-ML libraries -
for instance RandomForest, SVM, LogisticRegression etc. - are built
according to the three general =APIs= mentioned above. It is therefore
clear that given this design setting you can formulate your model as
following:

- choose your transformers

- fit your tranformers

- transform the data

- choose your ML model

- fit your ML Model to the transformed data

- use predictors to achieve your goals.

Your goal will then be to apply all of the above steps given your
application and to combine them into Pipelines that are easy to work
with in production.

#+BEGIN_EXPORT html
<br>
#+END_EXPORT


* An Example with Pipeline Understanding
:properties:
:header-args:python: :session pipeline :results output :exports both
:end:

A simple basic example that aims to provide your with the general
workflow is the following:

#+begin_src python 
import numpy as np

from sklearn.model_selection import train_test_split

from sklearn.ensemble import RandomForestRegressor

from sklearn.pipeline import Pipeline

from sklearn.feature_selection import SelectKBest

from sklearn.metrics import median_absolute_error, r2_score

from sklearn.preprocessing import StandardScaler

from sklearn.datasets import load_boston

## notice the elegance of the sklearn library. you have different
## modules - model_selection, ensemble, pipeline, feature_selection
## etc.- that exposes the user directly to the logic of each method.

## load the boston dataset
boston = load_boston()
X, y = boston['data'], boston['target']
features = boston['feature_names']

# split in training and testing dataset - i.e. look at generalization
# of your model out of sample
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

# create your pipeline that you can apply directly to your data. you
# can then fit the pipeline and predict using it.
pipe = Pipeline([("scaler", StandardScaler()), ## standardize your

                                               ## data according to
                                               ## the normal
                                               ## standardization. 
                 ("featsel", SelectKBest(k=10)), ## notice that this
                                                 ## selectes the 10
                                                 ## feutures that
                                                 ## explain the most
                                                 ## of the variance in
                                                 ## the predictor
                                                 ## variable. it is
                                                 ## based on a
                                                 ## multiple ANOVA
                                                 ## given the
                                                 ## different
                                                 ## features.
                 ("rf",RandomForestRegressor(n_estimators=20))]) ## specify
                                                                 ## a
                                                                 ## fit
                                                                 ## via
                                                                 ## a
                                                                 ## RandomForest
                                                                 ## with
                                                                 ## 20
                                                                 ## estimated
                                                                 ## bootstrapped
                                                                 ## trees

## train the data
pipe.fit(X_train,y_train)

## evaluate the model
y_pred = pipe.predict(X_test)
print(r'R^2=%.2f, MAE=%.2f'%(r2_score(y_test, y_pred), median_absolute_error(y_test, y_pred)))
#+end_src

#+RESULTS:
: R^2=0.75, MAE=1.51

Notice that you could have performed the above also without pipelining
all of the different methods

#+begin_src python :results output

## Scale the data
scaler = StandardScaler()

# print(X_train.shape)
# print(y_train.shape)

## merge the data and apply the scaler on the entire matrix in one shot
train = np.hstack((X_train, y_train.reshape(379,1)))

# print (train.shape)

scaled_fit = scaler.fit (train)

scaled = scaled_fit.transform (train)

featsel_fit = SelectKBest(k=10).fit (scaled[:, :13], scaled[:, 13])

featsel = featsel_fit.transform (scaled[:, :13])

# the extracted 10 features
# print (featsel.shape)

## Fit the data

rf = RandomForestRegressor(n_estimators=20).fit (featsel, scaled[:, 13])


# get corrected testing data

test = np.hstack((X_test, y_test.reshape(len (y_test),1)))

ttt = scaled_fit.transform (test)

X = featsel_fit.transform (ttt[:,:13])

# print (X.shape)

y_pred = rf.predict (X)

# print (y_pred.shape)

# print (ttt[:, 13].shape)

ttt1 = scaled_fit.inverse_transform(ttt)

ttt2 = scaled_fit.inverse_transform(np.hstack((ttt[:, :13], y_pred.reshape(len (y_pred),1))))


## Get your metrics for the fit

print(r'R^2=%.2f, MAE=%.2f'%(r2_score(ttt1[:,13], ttt2[:,13]), 
                             median_absolute_error(ttt1[:,13], ttt2[:,13])))

# print (scaled_X.shape) ## we have 13 features
# print (scaled_X)

#+end_src

#+RESULTS:


Notice that the results for the above are comparable to the one
obtained above. So you get the idea of what is behind a pipeline.

You fit all of the transformers and your predictor given your training
data. You then apply such fit to the training data. You scale back
your data and you compute the metrics on the original scaled data.



* Why you should always work through Pipelines
:properties:
:header-args:python: :session pipeline :results output :exports both
:end:

It is recommended that you always work with pipelines for your
workflow. 

This will help you to clearly define your workflow. On top of it you
might access and perform the usual transform and fit operations on
specific transformers /  models.

In order to see that consider the following example:

#+begin_src python :results output
## load the boston dataset
boston = load_boston()
X, y = boston['data'], boston['target']
features = boston['feature_names']

# split in training and testing dataset - i.e. look at generalization
# of your model out of sample
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

# create your pipeline that you can apply directly to your data. you
# can then fit the pipeline and predict using it.
pipe = Pipeline([("scaler", StandardScaler()), ## standardize your

                                               ## data according to
                                               ## the normal
                                               ## standardization. 
                 ("featsel", SelectKBest(k=10)), ## notice that this
                                                 ## selectes the 10
                                                 ## feutures that
                                                 ## explain the most
                                                 ## of the variance in
                                                 ## the predictor
                                                 ## variable. it is
                                                 ## based on a
                                                 ## multiple ANOVA
                                                 ## given the
                                                 ## different
                                                 ## features.
                 ("rf",RandomForestRegressor(n_estimators=20))]) ## specify
                                                                 ## a
                                                                 ## fit
                                                                 ## via
                                                                 ## a
                                                                 ## RandomForest
                                                                 ## with
                                                                 ## 20
                                                                 ## estimated
                                                                 ## bootstrapped
                                                                 ## trees


scaled_fit = pipe['scaler'].fit (np.hstack((X_train, y_train.reshape(379,1))))

scaled = scaled_fit.transform (np.hstack((X_train, y_train.reshape(379,1))))

print (scaled)

#+end_src

#+RESULTS:
#+begin_example
[[-0.3906002   0.42637011 -0.74491444 ...  0.34049624  0.82212111
  -0.44484602]
 [-0.40127639  0.5525335  -0.84901832 ...  0.42774893 -0.46241699
  -0.32575001]
 [-0.40110543  1.18335044 -0.66648002 ...  0.34184377 -0.90310809
   1.14670973]
 ...
 [-0.3954927  -0.49882807 -0.15309105 ...  0.40091059 -0.31227617
  -0.36905765]
 [-0.38599992 -0.49882807 -0.59517599 ...  0.38103449  0.86938766
  -0.6505573 ]
 [-0.39692832 -0.49882807 -1.003035   ...  0.42774893  0.29801844
   0.05319184]]
#+end_example

