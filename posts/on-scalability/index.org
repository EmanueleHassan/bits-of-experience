#+BEGIN_COMMENT
.. title: On Scalability
.. slug: on-scalability
.. date: 2020-07-17 08:43:49 UTC+02:00
.. tags: IT Architecture
.. category: 
.. link: 
.. description: 
.. type: text
.. has_math: yes
#+END_COMMENT


This post quickly examines two different laws of scalability: Amdahl's
law and Gustafson's law.

Both laws addresses the extent to which a problem can be speed up
through scaling out. 

The premise of setting under which the laws operate is
different. Amdahl's law assumes a *fix problem size*, while
Gustafson's law assumes an *an expanding problem size in the number of
machines scaling out*.

{{{TEASER_END}}}

This leads tot the following formula:

$$Amdahl's Law Speedup = \frac{1}{1-p+p/s}$$

$$Gustafson's Law Speedup = 1-p+ps$$

/where in the above: p = parallelizable component 
                     s = achievable speedup through scale up/

It is therefore clear from the above that for a fixed problem size
continuously increasing the number of machines in order to scale out
will not be that beneficial and you might end up with a very poor
utilization of those. You have as bottleneck the non parallelizable
component and scaling out will not help.

To help visualizing the concept, please see

 #+begin_export html
<style>
.container {
  position: relative;
  left: 15%;
  width: 70%;
  margin-top: 60px;
  margin-bottom: 60px;
  overflow: hidden;
  padding-top: 56.25%; /* 16:9 Aspect Ratio */
  display:block;
  overflow-y: hidden;
}

.responsive-iframe {
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  right: 0;
  width: 100%;
  height: 100%;
  border: none;
  display:block;
  overflow-y: hidden;
}
</style>

<div class="container"> 
  <iframe class="responsive-iframe" src="https://www.youtube.com/embed/oMyqwXjbnVM" frameborder="0" allowfullscreen;> </iframe>
</div>
 #+end_export


