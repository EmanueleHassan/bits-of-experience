#+BEGIN_COMMENT
.. title: On Multithreading
.. slug: on-multithreading
.. date: 2022-04-13 16:47:16 UTC+02:00
.. tags: threading, Python, java, software-engineering
.. category: 
.. link: 
.. description: 
.. type: text

#+END_COMMENT


So apparently this is a thing that I will have to master sooner or
later.

I am not a fun of it as I know it gets tricky to write solid programs
with mulit-threading when complexity increases.

For a solution that I am trying to construct I will need that bit.

This for two reasons:

1. the existing solution uses it; meaning that it is beneficial for
   you to read and understand it in order for understand the current
   design and borrow from it.

2. your new solution would either have flavours of it or use queues. I
   am rather inclined for the second but I promised to provide a
   solution for both and this is what I am currently working for.

   /Update:/ after one day I could set up a working solution with
   multithreading. Was not too difficult. But it is just at conceptual
   level. The gist of it is more less along [[https://alyssaq.github.io/2014/how-do-I-return-a-http-response-to-caller-and-continue-processing/][these]] lines.

I am not an extremely low level programmer due to my background. At
least not so far. So this is the reason I do not conceptually like the
thing and would prefer to go with a much more simple queueing
solution.

I learned in life that you should not stop in front of your conceptual
barriers. You should be aware of your gaps and taking extra care when
making a step in that direction so that you do not hurt yourself, but
by baby steps everything is possible. This is how we learn since
inception.

{{{TEASER_END}}}

** TODO Java

   Will come at a later stage. Good is that the theoretical
   fundamental is the same. 
   
** Python

   So as you know underneath Python there is a lot of C and the well
   known CPyhton which actually manages the implementation of Python
   and the C stuff.

   Yuo can understand this by the following wiki entry:

   #+begin_quote
CPython can be defined as both an interpreter and a compiler as it
compiles Python code into bytecode before interpreting it. It has a
foreign function interface with several languages, including C.
   #+end_quote

   Because of the way CPython implementation of Python works,
   threading may not speed up all tasks. This is due to interactions
   with the GIL that essentially limit one Python thread to run at a
   time.

   In more technical terms:

   #+begin_quote
The Python Global Interpreter Lock or GIL, in simple words, is a mutex
(or a lock) that allows only one thread to hold the control of the
Python interpreter.

Since the GIL allows only one thread to execute at a time even in a
multi-threaded architecture with more than one CPU core, the GIL has
gained a reputation as an “infamous” feature of Python.
   #+end_quote

   So basically this is why people often say that you cannot really
   multithread in Python. You can read more about [[https://realpython.com/python-gil/][it here]] - this is a
   very nice article.

   So now understand the following two general sources of bottlenecks
   as mentioned in the article above.

   #+begin_quote
=CPU-bound= programs are those that are pushing the CPU to its
limit. This includes programs that do mathematical computations like
matrix multiplications, searching, image processing, etc.

=I/O-bound= programs are the ones that spend time waiting for
Input/Output which can come from a user, file, database, network,
etc. I/O-bound programs sometimes have to wait for a significant
amount of time till they get what they need from the source due to the
fact that the source may need to do its own processing before the
input/output is ready, for example, a user thinking about what to
enter into an input prompt or a database query running in its own
process.
   #+end_quote

   =I/O-bound= tasks that spend much of their time waiting for
   external events are generally good candidates for threading in
   python. Cause you understand that in such a way you might progress
   with your application through threads as you are breaking that
   bound.

   When you have mulitple threads performing multiple jobs you have
   then the chance to switch between the threads such that you can
   release the I/O bottleneck and potentially continue with the work.

   You can run the following snippet to check what is going on:

   #+begin_src python
# multi_threaded.py
import time
from threading import Thread

COUNT = 50000000

def countdown(n, thread):
    while n>0:
        n -= 1

        if n % 24000000 == 0:
            print("processing thread {}".format(thread))

t1 = Thread(target=countdown, args=(COUNT//2, 1,))
t2 = Thread(target=countdown, args=(COUNT//2, 2,))

start = time.time()
t1.start()
t2.start()
t1.join()
t2.join()
end = time.time()

print('Time taken in seconds -', end - start)
   #+end_src

   Problems that require heavy CPU computation and spend little time
   waiting for external events have in theory no advantage at all.

*** Concepts of Python MultiThreading

    I remember that at some point I read an entire book about
    concurrent programming in C++. Many notions are already sitting
    somewhere in my mind.

    It is ok, not even that difficult as a concept and can well do
    it.

    Just understand the following in Python:

    - Threads can run in =deamon mode= or not.

      #+begin_src python
x = threading.Thread(target=thread_function, args=(1,), daemon=True)

# vs.

x = threading.Thread(target=thread_function, args=(1,))
      #+end_src

      If a thread is run in deamon mode it runs in the background,
      meaning that your main program does not carry too much about
      it. Once the main thread is over, the program will end and the
      deamon threads will be killed no matter if they finished their
      computations or not.

      This is likely not the way you want to work.

      So one way of fixing this is to explicitely use the
      =<thread>.join ()= method that will require the main thread to
      wait for the =<thread>= to finish its operations before going
      on.

      The other one is the idea of starting a thread without hte
      deamon mode. There before finishing the main thread does
      actually call =<thread>.join= on all of the existing
      =<thread>=. 

    - You can start multiple threads as follows:

      #+begin_src python
    with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
        executor.map(thread_function, range(3))
      #+end_src

      #+begin_quote
The end of the with block causes the ThreadPoolExecutor to do a
*.join() on each of the threads in the pool*. It is strongly recommended
that you use ThreadPoolExecutor as a context manager when you can so
that you never forget to .join() the threads.
      #+end_quote

    - Locking

      So this is to avoid race conditions. You can read about the
      example [[https://realpython.com/intro-to-python-threading/#producer-consumer-threading][here]]. I mean you have a good understanding of it from
      the time you were reading that book.

      The idea is then that you can process the thing if you have
      acquired the lock - think again of the seashell of the /[[https://en.wikipedia.org/wiki/Lord_of_the_Flies][Lord of
      the Flies]]/.

      Recall as well the potential danger of running into deadlocks
      when you start working in such a way.

      The typical example, I have a lock on an object and you other
      thread on another. We both need to acquire a lock on the thing
      you have to finish our operations. We both will be stuck
      forever.

*** TODO Cap your thread amount

*** On small experiments for using threading in rest-calls

    So basically I did a lot of small experiments in order to properly
    understand the solution along the lines mentioned at the
    beginning.

    That is a very good solution to solve the timeouts in http
    communication.
    
**** On the number of threads

     In order to understand this component I did a little bit of
     detective work along these lines.

     I used the =threading.enumerate()= method to get an idea about
     the different threads running. I inspected the names and which
     where open at what time.

     I basically came to the following conclusion - note that I did
     this job with the Werkzeug server used in development mode - do
     not know how the thing works with WSGI and would need to
     investigate but I guess along the same lines.

     Basically you have always two threads working:

     - Mainthread
     - Thread-1

     Then for each new request if you work along the lines above you
     would get two new threads.

     One of the two threads would be very short-lived. You would close
     it immediately after answering the incoming request.

     The other one performing the slow job will exist until the
     called function through which you opened the thread is
     completed.

     So basically that is good and the design is effective. 
     
**** On the type of job

     The thing you should note than is that you cannot use this
     multi-threading solution for intensive CPU bounded jobs.

     The solution is then generally okey if you have I/O issues. You
     have lots of threads orchestrating these I/O bounded jobs.

     It is not that ideal in the case of CPU bounded jobs. 

     In order to see this you can check what happens if you trigger
     the following job from an endpoint.

     Note that one version is CPU bounded and the other mimicks an I/O
     bounded case

     #+BEGIN_SRC python
def process_data(message, Id = 0):
    """This is the function to be called by the multiple threads.

    As discussed it will be either this one or the Queue Solution.

    """

    print("starting to sleep for job: {}".format(Id))

    # I/O bound case #
    # time.sleep(30) 

    # CPU bound case #
    start = time.time()
    countdown(50000000)
    end = time.time()

    ti = end - start

    with total_time.get_lock():
        total_time.value += ti
        totT = total_time.value        

    print("End time job {} : {}".format(Id, totT))
     #+END_SRC

     Note that the total_time is a ~multiprocessing.Value()~
     object. This basically has the locking mechanism in order to
     avoid the race conditions and set your total time properly. 
     

*** TODO Multi-processing

    This starts an entire new process with a new interpreter.

    You do not have the GIL problem in this case, and if your infra
    architecture allows it you can possibly address CPU-bound issues
    with it.

    ------------

    You might want to check at this option in case you would have CPU
    bounded long-running jobs that you would need to trigger from your
    endpoints. 
