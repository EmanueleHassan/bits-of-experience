#+BEGIN_COMMENT
.. title: Python Testing
.. slug: python-testing
.. date: 2022-05-12 13:35:32 UTC+02:00
.. tags: testing, software-engineering, Python
.. category: 
.. link: 
.. description: 
.. type: text

#+END_COMMENT


#+begin_export html
<style>
img {
display: block;
margin-top: 60px;
margin-bottom: 60px;
margin-left: auto;
margin-right: auto;
width: 70%;
height: 100%;
class: center;
}

.container {
  position: relative;
  left: 15%;
  margin-top: 60px;
  margin-bottom: 60px;
  width: 70%;
  overflow: hidden;
  padding-top: 56.25%; /* 16:9 Aspect Ratio */
  display:block;
  overflow-y: hidden;
}

.responsive-iframe {
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  right: 0;
  width: 100%;
  height: 100%;
  border: none;
  display:block;
  overflow-y: hidden;
}
</style>
#+end_export

So you already worked with python testing frameworks.

The set up was already in place back then.

Now it is more than a year that I do not touch the thing and
everything is solidly gone.

Luckily I have a repo on my Github with some structure of it.

Will leverage that now, plus I will start to write some notes, such
that people that will have to collaborate with me can reference to
it.

On the top of it, it will be useful for the future as well.

Plus finally it is always nice to see the things in multiple
languages. It is always the same story. By adding pieces to the puzzle
the picture becomes systematically clearer not the opposite. 

{{{TEASER_END}}}

* Pytest vs. Unittests

  So basically these are the two big testing frameworks in the Python
  space.

  I had worked with both in the past. 

  I will go with the first. This decision is not 100% thought through
  decision but when you work you understand that sometimes you have to
  cut corners and based on what I could quickly read online it is
  sensible.

  There is even the possibility of running =unittests= tests through
  pytest so it makes sense to leverage on this. 


* Writing tests

  So a good start is the good-practices section of the [[https://docs.pytest.org/en/6.2.x/goodpractices.html][official
  documentation]].

  Read as well the following documentation:

  - [[https://docs.pytest.org/en/6.2.x/reference.html][Reference API]]

  - [[https://realpython.com/pytest-python-testing/][Python testing]] 
  
** Conventions for Python test discovery

   So basically there are two options:

   - Embedding the tests within a package - Tests inside of the
     application code

   - Tests outside of the application code

   /Update:/ Go with the second version. It is the best. 

*** AVOID - Tests outside of the application

    Note that basically you have a set of disadavantages if you work
    in such a way.

    Among the others:
       
    - there is no 1:1 mapping among python testing and packages
      modules.

    If you want to work this way you have to find a solution for it.
    You can probably go into the direction with markers but I do
    not know how much helpful this will be.

    So basically the idea is to follow these conventions such that you
    can properly map the modules to the tests.

    That said, should you go this way you should work in the following
    way, well noting the following points.

    Note that in constrast to Java here the situation is much more
    messy because of the python module imports and how everything
    enters the =sys.path=. You can read more about it [[https://docs.pytest.org/en/6.2.x/pythonpath.html#import-modes][here]].

    You can refer to that page in general, but use the following set up
    in general as your north star:

    #+begin_src 
setup.py
src/
    mypkg/
        __init__.py
        app.py
        view.py
tests/
    __init__.py
    foo/
        __init__.py
        test_view.py
    bar/
        __init__.py
        test_view.py
    #+end_src

    Note that you need to create testing packages as in the above
    (=foo=, =bar=) if you have testing modules called in the same way.

*** USE THIS - Tests inside of the application code

    This is a cleaner solution in the sense that it is clear which
    tests belong to which modules.

    #+begin_src 
setup.py
mypkg/
    __init__.py
    app.py
    view.py
    test/
        test_app.py
        test_view.py
    #+end_src

    You can then quickly tests them by running:

    #+begin_src sh
    pytest --pyargs mypkg
    #+end_src


** Personal Conventions
   
*** Always use classes

    Always run your tests within classes. This will be of paramount
    importance.

    I will usually run tests in the following way:

    #+BEGIN_SRC python
# content of test_moduleX.py
class Test_ModuleX:
    def test_one(self):
        x = "this"
        assert "h" in x

    def test_two(self):
        x = "hello"
        assert hasattr(x, "check")
    #+END_SRC

    then you can run the tests for the single module as follows:

    #+begin_src sh
   $ pytest -q test_moduleX.py
    #+end_src

    On the top of it you can as well create complex logic by
    inheritance etc.

    Finally it is important to realize that despite the way you set up
    your tests, the following benefits are there:

    #+begin_quote
    - Test organization

    - Sharing fixtures for tests only in that particular class

    - Applying marks at the class level and having them implicitly
      apply to all tests
    #+end_quote

    You can as well use the flag =-k= to run tests by regular
    expression.


** Some Useful commands

   Note that you have a couple of plug-ins in pytest, depending on
   what you aim to do.

   You can check them as follows:

   #+begin_src sh
   $ pip install pytest-html
   #+end_src

   Then you’re able to run tests with:

   #+begin_src sh
   $ pytest –-html=report.html
   #+end_src


** Interpret Results
   
   You have to understand the following options:

   #+begin_quote
- f - failed

- E - error

- s - skipped

- x - xfailed

- X - xpassed

- p - passed

- P - passed with output
   #+end_quote

*** Logging

    It is possible to set up logging options for the tests in order to
    redirect standard error and standard output to given sources as
    well all of the INFO, WARNING etc. messages you will need in your
    application.

    You can read how to do that for the pytest module [[https://docs.pytest.org/en/6.2.x/logging.html][here]].

    In order to keep consistency I think I will rather go with a
    general Logger object that I will use in my project and reference
    this.

    Will see in time if this strategy will be solid or not. 

   
* Test Coverage

  In order to generate your coverage reports you can use the
  =coverage= library.

  This is important as you can properly sleep at night, once your code
  is properly covered. Meaning you have a good overview of the quality
  of your code etc.

  Note that this are the python native tools for making such quality
  checks. SonarQube is something similar that goes into the
  direction. Probably more sophisticated. Would have to check into
  it. In any case it is the program that Sergio mentioned to you in
  order to inspect your Java code. So might be interesting as well to
  explore such that you can have a unique consistent way for
  inspecting all of your code irrepsective of the language. Might be
  useful as well in this sense as it will reduce the burden of
  managing the entire thing. 

  This should also go into your [[https://www.scrum.org/resources/blog/done-understanding-definition-done?gclid=Cj0KCQjw4PKTBhD8ARIsAHChzRIlof8n0e1pwXTQ-QBKWB56BuHONmIqLLL99zyOrJBa57FCg3heLUUaAgkIEALw_wcB][definition of =DONE=]], meaning you can
  consider a project done just if you reach a given threshold of test
  coverage. 

  You will release your projects just when you will have covered a
  decent coverage level.

  Now you can explore the following modules in order to properly work
  in here:

  - [[https://pytest-cov.readthedocs.io/en/latest/][pytest-cov]]

  - [[https://coverage.readthedocs.io/en/6.3.2/][coverage]]

  So basically from my current understanding the second depends on the
  first, i.e. it leverages on it but has some additional
  functionalities such that it makes sense to go for this one. 

** Coverage

   This is one of the most popular code coverage tools for Python.

   I will not go through it; as mentioned I rather go with the next
   library, which builds on the top of it. 
   
** Pytest-Cov

   Pytest-cov is a Python plugin to generate coverage reports. So as
   the above.

   In addition to functionalities supported by coverage command, it
   also supports centralized and distributed testing. It also supports
   coverage of subprocesses.

*** Usage

    After you installed the [[https://pypi.org/project/pytest-cov/][relevant library]], you can install
    everything in the following way:

    #+BEGIN_SRC python
    pytest --cov=<myproject> --cov-config=.coveragerc
    #+END_SRC

    this, if you want to test a project with a structure as in [[*USE THIS - Tests inside of the application code][here]].

    Note that =.coveragerc= is a configuration file where you can
    specify the options for writing your tests. You might start by
    reading the [[https://pytest-cov.readthedocs.io/en/latest/config.html][official documentation]] for it. Or [[https://breadcrumbscollector.tech/how-to-use-code-coverage-in-python-with-pytest/][this]].

    Recall that it is good practice to eliminate your tests files from
    the coverage reports. This way you are not interested in them,
    they are actually used for covering the standard source code not
    the opposite.

    You can do it as follows:

    #+begin_src
[run]
omit = **/test/*
    #+end_src

    You can specify similar options over there.

    Another important flag for working with the coverage is the
    following: =--cov-fail-under=MIN=

    The idea is that your tests will fail if the coverage is less than
    MIN. This goes a bit into [[*Set up your CI pipeline incorporating this bit][this]] direction.

    All good in such a way you can create solid CI pipelines and
    create a good definition of DONE.

*** Practical Example for the above:

    You can see a practical example for the above in the following:

    #+begin_src sh

    cd c:/Dev/pythonWorkspace/relationaloperations/

    # recall to activate the virtualenv with the relavant packages for the
    # below.

    pytest --pyargs src # normal pytest for the src project

    pytest --cov=src --cov-config=.coveragerc # pytest-cov 

    #+end_src

    There you see for instance the test_Testing.py module and
    similar.

    I will introduce here some toy examples of different ways of
    testing cases.

    This is a bit different in comparison to Java. There you have more
    methods that already cover most of the assertion cases you might
    be interested in. Say for instance assertThrows etc.

    In =pytest= you simply work with ~assert~ statements and you twist
    everything according to different flavours. Check in this sense
    [[https://mvolkmann.github.io/blog/python/python-testing/#skipping-tests][this source]].
    

**** How to test that a method returns an Error

     In order to check at this you can use the following schema:

     #+BEGIN_SRC python
     import pytest

     def test_Raise_Error(self):

        with pytest.raises(SyntaxError):
            connectionString('DRIVER={ODBC Driver 17 for SQL Server};',
                             'hello;', True)


     # The above is the general Syntax. So you see you make sure that the
     # value of the context is an error of the type you are expecting.

     #+END_SRC
     
     You can see then how to test that a method returns a user-defined
     error message as in the URL above.

     
**** Skip and skipif

     These are two very useful methods in order to skip tests or skip
     them by condition. This is quite handy. Think for instance of
     skipping tests if they are below a certain version etc.

     
**** Xfail

     This is also practical. You mark such tests in such a way when
     you know that they are supposed to fail.

     Think for instance of having bugs in your code such that you
     expect them to fail.

     Then if such tests pass you will have them marked in your
     reports. In such a way you know that something went wrong and you
     did not obtain the expected behaviour. 
     

**** Setup and Teardown

     Similar to java you can as well have modules that contain source
     code to be run before or after the tests.

     You can use the following in this sense:

     - setup_module - run once before all the test functions
     - setup_function - run once before each test function
     - teardown_function - run once after each test function
     - teardown_module - run once after all the test functions


**** Fixtures

     Basically in such a way you can pass data to your tests. So I
     think it goes into the direction of Mocks in order to write
     tests.

     You can find the way of writing such tests [[https://docs.pytest.org/en/latest/how-to/fixtures.html][here]].


*** TODO distributed tests

    this will become important when you will have a lot of tests
    covering your code.
    
    Not so important to this stage.
   

** On the coverage Metrics

   Note that the above two basically report by the default the
   coverage of the [[https://en.wikipedia.org/wiki/Statement_(computer_science)][statements]] in your program.

   While this is the standard it will require you to write tons and
   tons of tests. So either you will go with TDD or you will likely
   get poor coverage ratios such that it will be difficult to assess
   from there if you tested at least the basics of your programs.

   Another option is to use different metrics such as functions
   coverage. The idea here is simple. Instead of checking how many
   statements you did cover you can check at how many functions,
   methods etc. your code did cover.

   In order to go down that different road you could start to explore
   other plugins as [[https://pypi.org/project/pytest-func-cov/][this]]. This is a simlar plug in to the pytest
   package as the coverage one.


** TODO Set up your CI pipeline incorporating this bit

   What is a good coverage ratio?

   Basically the idea is [[https://www.atlassian.com/continuous-delivery/software-testing/code-coverage#:~:text=With%20that%20being%20said%20it,fairly%20low%20percentage%20of%20coverage.][the following]].

   Given this you see that there are rough indications with respect to
   the coverage your applications should have.

   The idea is then to embedd this into the CI pipeline such that if
   the coverage is not reached the CI will fail.

   This will be hard to achieve for existing projects. It makes
   probably sense to aim for it for newly created projects.   
      

** TODO CI pipeline

   This does not quite much has to do with python testing etc. Will
   hold it here for now as it is in the CI space.

   I would eventually have to reorganize things a bit.

   In any case this is necessary as it gives you visibility. 

   -------

   The idea is of a remainder to put badges on your projects,
   i.e. with the coverage, if the thing is passing or not etc.

   This would help you to gain the necessary visibility etc.

   We are using bitbucket to this stage for version control.
   You can check into [[https://shay-palachy.medium.com/bitbucket-repositoty-status-badges-2271637635aa][this]] in this sense.
  

* Read into Tox

  So basically this is your gradle/maven correspondent for Python.

  So this is a nice automation tool. Should properly check into it and
  make it run.

  Note that basically the way you do it currently is manually. There
  is no big difference. But might make sense to start using this in
  order to work more in a streamlined way. 
  
  I.e. you can set up virtual enviornments, package your code, run
  your tests etc.

  In such a way you can work in a structured way in this dimension as
  well following the best practices that these tools are imposing on
  you.

  Note that in general tox is mostly used for organizing your
  tests. You can then publish via other tools such as =twine=, which
  ensure encryption at upload time. 

** Introduction

   The general idea of tox is to specify all of the relevant
   operations in order to standardize your workflow in a =tox.ini=
   file.

   Based on this, then the following workflow usually follows:
   
   #+begin_export html
    <img src="../../images/Screenshot 2022-05-30 105049.png" class="center">
   #+end_export

   
** General Key Strengths of tox

   With it you can test your code against multiple versions of
   python...

   So that is quite handy. You see nonetheless that you do not come
   out with a clean docker image you can run and properly
   orchestrate.

   
** General Issue to keep in mind

   This is quite an annoying one. You should not underestimate it.

   #+begin_quote
There is a danger of using a stale tox venv. One of tox’s weaknesses
is its inability to track changes in the dependencies in the setup.py
and/or requirements.txt files. This is something to keep in mind.

When you have made such a change, always be sure to pass the tox -r
(recreate) flag so that the environments are…you guessed it,
recreated.
   #+end_quote


** General Utilization

   The thing is that you know about docker and the way of creating
   docker images.

   Well now the point is that you can package your applications as
   docker images and this solves quite much of the packaging issues
   you were leveraging tox for.

   You can as well integrate it apparently. Not that clear what can
   happen.

   The question is then about how to run all of the necessary tests
   before packaging your images.

   For this you usually go with a solid CI pipeline. Or as a second
   possibility you can investigate into [[https://blog.chmouel.com/2014/09/08/dox-a-tool-that-run-python-or-others-tests-in-a-docker-container/][these topics]]. 

   So use it now as your team is not on the docker road. But make sure
   that if you end up in that space you adapt.

   Meaning it is a good substitute in the meanwhile to move in that
   space. I found this [[https://christophergs.com/python/2020/04/12/python-tox-why-use-it-and-tutorial/][good series]] explaining tox in the
   meantime. Here the [[https://github.com/ChristopherGS/tox_examples][101 repo]] for it.
   
   With it you should good to go to start leveraging the tool.

   Especially important are the following things that you should well
   understand:

   #+begin_quote
As per the tox docs: “At the moment tox supports three configuration
locations prioritized in the following order:

- pyproject.toml

- tox.ini

- setup.cfg
   #+end_quote

   I am going for the =tox.ini= one.

   And the second important thing you should note is that:

   #+begin_quote
skipsdist which we need to set when we are not testing a Python
package (e.g. for a service or simple set of scripts).

Anytime tox doesn’t find a setup.py file this flag will need to be
set.
   #+end_quote


** Packaging

   This is nice. You used to create your artifacts via docker. Such
   artifacts did not just have the code but rather the entire snapshot
   of the OS etc. They are more thorough in this sense.

   In this new team you simply want to package the code you write in a
   consumable way. So you use other packaging tools rather than
   docker. 

   You can start by checking at the 101 example in the repo mentioned
   above.

   In general you first build with:

   #+begin_src 
python -m build --wheel .
   #+end_src

   Then you upload with:

   #+begin_src 
twine upload -r <Feed Specified in $HOME/pypirc> dist/*
   #+end_src
   
   This will be important as in such a way you can create that full
   pipeline necessary to have your stack well set up.

   Everything is properly set up with versions etc.

   Note that you cannot upload two times the same version.

   So this is ok but it means that you have just to trigger the twine
   upload from the main branch. From all of the other branches you
   would simply have to test and deploy. 

   Have still to figure out the correct set up.
