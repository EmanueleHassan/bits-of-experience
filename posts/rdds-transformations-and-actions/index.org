#+BEGIN_COMMENT
.. title: RDDs Transformations and Actions
.. slug: rdds-transformations-and-actions
.. date: 2020-05-03 15:51:24 UTC+02:00
.. tags: BigData, Spark
.. category: 
.. link: 
.. description: 
.. type: text
#+END_COMMENT

#+BEGIN_EXPORT html
<br>
<br>
#+END_EXPORT


This post continues the discussion started a few times ago on [[https://marcohassan.github.io/bits-of-experience/posts/spark-session-initalization/][RDD and
Spark]].

I will try to go here in the lifecycle of an RDD and will present the
major Transformation and Actions functions. I will moreover touch on
the physical implementation of Spark as this will give you the mental
tools to properly understand how to properly structure your Spark
workflow in order to optimize the performance.   

{{{TEASER_END}}}

#+BEGIN_EXPORT html
<br>
#+END_EXPORT

** On RDDs

A brief overview on RDDs was given in the previous post and you are
referred to it for a brief introduction.

RDDs are lazy. This, means that only if the data is needed for a
certain computation the data is read from the underlying storage
system.

An RDD in Spark is simply an immutable distributed collection of
objects. Each RDD can be split into multiple partitions, which may be
computed on different nodes of the cluster.

The typical RDD lifecycle is as follows:

- An RDDs is first created from stable storage or by some Python objects.

RDDs offer then two types of operations: *transformations* and *actions*.

- *Transformations* create a new RDD from an existing one.
  Transformations are lazy, meaning that no transformation is executed
  until you execute an action.

- *Actions* compute a result based on an RDD, and either return it to
  the driver program or save it to an external storage system (e.g.,
  HDFS). This is the end of the lifecycle.

Transformations and actions are different because of the way Spark
computes RDDs. Although you can define new RDDs any time, Spark
computes them only in a *lazy* fashion, that is, the first time they
are used in an *action*.

For the creation of RDDs and the partitions of them please refer to
the previous post I will now briefly introduce the physical execution
of spark before illustrating some of the most the key transformations
and actions.

#+BEGIN_EXPORT html
<br>
#+END_EXPORT


** TODO On the physical Execution

#+BEGIN_EXPORT html
<br>
#+END_EXPORT


** Transformations
:properties:
:header-args:ein-python: :session http://127.0.0.1:8888/Spark.ipynb :results output
:end:

Following are examples of some of the common transformations
available.

For a detailed list, see [[https://spark.apache.org/docs/2.0.0/programming-guide.html#transformations][RDD Transformations]]

Run some transformations below to understand this better.

*Note:* If some of the queries are taking too long to complete, try
restarting the kernel, and rerunning the cell /above/.


#+NAME: 04795EA5-7FB2-4F84-8A23-D25ADDF13D25
#+begin_src ein-python :results output
  from pyspark.sql import SparkSession

  spark = SparkSession \
      .builder \
      .master ("local[8]") \
      .appName("My first Spark Session") \
      .getOrCreate()

  sc = spark.sparkContext
#+end_src

#+RESULTS: 04795EA5-7FB2-4F84-8A23-D25ADDF13D25


#+NAME: CEAE9B99-8441-44CB-99D8-409B6E788758
#+begin_src ein-python :results output
fruits = sc.textFile('wasb:///example/data/fruits.txt')
#+end_src

#+RESULTS: CEAE9B99-8441-44CB-99D8-409B6E788758


#+NAME: 06D28E67-59DA-4A52-977B-775105FC9F67
#+begin_src ein-python :results output
fruits.collect()
#+end_src


#+BEGIN_SRC ein-python 

# map
fruitsReversed = fruits.map(lambda fruit: fruit[::-1]) ## the fruit[::-1] inverts the letters of the word

# Note: the `collect` command is NOT a Transformation, it is an Action
# used here for the purposes of showing the results! Just use it when
# you know that the action will be small enough to be handled by the
# memeory of the machine you are working on. Otherwise, no chance you
# will be able to display your results and you will better have to
# save the results on a HDFS cluster.
fruitsReversed.collect()
#+END_SRC

#+BEGIN_SRC ein-python
# filter
shortFruits = fruits.filter(lambda fruit: len(fruit) <= 5)
shortFruits.collect()
#+END_SRC

#+BEGIN_SRC ein-python
# flatMap
characters = fruits.flatMap(lambda fruit: list(fruit))
characters.collect()
#+END_SRC

#+BEGIN_SRC ein-python
# union
fruitsAndYellowThings = fruits.union(yellowThings)
fruitsAndYellowThings.collect()
#+END_SRC

#+BEGIN_SRC ein-python
# intersection
yellowFruits = fruits.intersection(yellowThings)
yellowFruits.collect()
#+END_SRC

#+BEGIN_SRC ein-python
# distinct
distinctFruitsAndYellowThings = fruitsAndYellowThings.distinct()
distinctFruitsAndYellowThings.collect()
#+END_SRC

#+BEGIN_SRC ein-python
# groupByKey
yellowThingsByFirstLetter = yellowThings.map(lambda thing: (thing[0], thing)).groupByKey()
for letter, lst in yellowThingsByFirstLetter.collect():
        print("For letter", letter)
        for obj in lst:
                print(" > ", obj)
#+END_SRC

#+BEGIN_SRC ein-python
# reduceByKey
numFruitsByLength = fruits.map(lambda fruit: (len(fruit), 1)).reduceByKey(lambda x, y: x + y)
numFruitsByLength.collect()
#+END_SRC

**** Some quick note on reduce and reduceByKey

  The logic of the reduce function is as follows

  [[img-url:/images/Bildschirmfoto_2020-05-04_um_17.54.18.png]]

  #+NAME: A1B8DF15-A6A1-430E-9ED2-871FC9AB0F2B
  #+begin_src ein-python :results output
  input_list = sc.parallelize(range(5))
  sum_of_squares = input_list.map(lambda x: x ** 3).reduce(lambda x, y: x + y)
 
  print(sum_of_squares)
  #+end_src

  #+RESULTS: A1B8DF15-A6A1-430E-9ED2-871FC9AB0F2B
  : 100

  It is now clear from the examples below that the lambda function of
  the reduce by key function below takes as x the value of the key and
  as y the second value of the key. This in analogy to the reduce key
  above. It performs hence essentially the same function as the reduce
  option for each individual key.

  #+NAME: F440D9E1-0F30-48FA-9F91-DA788BEFCCF8
  #+begin_src ein-python :results output
  fruits = sc.parallelize(["apple", "orange", "java", "call++"])

  ## Example 1 ##

  numFruitsByLength = fruits.map(lambda fruit: (len(fruit), 2))
  print(numFruitsByLength.collect())

  numFruitsByLength = numFruitsByLength.reduceByKey(lambda x, y: x + y)
  print(numFruitsByLength.collect())

  ## Example 2 ##

  numFruitsByLength = sc.parallelize([(5, 2), (6, 3), (4, 2), (6, 2)])
  print(numFruitsByLength.collect())

  numFruitsByLength = numFruitsByLength.reduceByKey(lambda x, y: x + y)
  print(numFruitsByLength.collect())
  #+end_src

  #+RESULTS: F440D9E1-0F30-48FA-9F91-DA788BEFCCF8
  : [(5, 2), (6, 2), (4, 2), (6, 2)]
  : [(5, 2), (6, 4), (4, 2)]
  : [(5, 2), (6, 3), (4, 2), (6, 2)]
  : [(5, 2), (6, 5), (4, 2)]



  #+BEGIN_EXPORT html
  <br>
  #+END_EXPORT


**** mapValues

How to interpret =mapValues=

#+NAME: 9AB0E13E-5525-431A-AD72-DA3D4A39EDCC
#+begin_src ein-python :results output
  print(test_entries.map(lambda x: (len(x["choices"]), x["choices"])).groupByKey().map(lambda x : (x[0], len(list(x[1])))).collect())

  print(test_entries.map(lambda x: (len(x["choices"]), x["choices"])).groupByKey().mapValues(len).collect())
#+end_src

#+RESULTS: 9AB0E13E-5525-431A-AD72-DA3D4A39EDCC
: [(4, 19690), (2, 36730), (3, 29057), (5, 8893), (6, 3579), (7, 1332), (8, 455), (9, 163), (10, 58), (11, 43)]
: 
: [(4, 19690), (2, 36730), (3, 29057), (5, 8893), (6, 3579), (7, 1332), (8, 455), (9, 163), (10, 58), (11, 43)]


** Actions
:properties:
:header-args:ein-python: :session http://127.0.0.1:8888/Spark.ipynb 
:end:
    
*** Aggregate
    [[https://stackoverflow.com/questions/28240706/explain-the-aggregate-functionality-in-spark/38949457][Explaination of Aggregate]]

    #+NAME: 7DA476A4-FDA8-44A0-B1BF-2FEF00050509
    #+begin_src ein-python :results output
    seqOp = (lambda local_result, list_element: (local_result[0] + list_element, local_result[1] + 1) )
    combOp = (lambda some_local_result, another_local_result: (some_local_result[0] + another_local_result[0], some_local_result[1] + another_local_result[1]) )

    sc.parallelize([1, 2, 1, 2]).aggregate((0, 0), seqOp, combOp)
    #+end_src

    #+RESULTS: 7DA476A4-FDA8-44A0-B1BF-2FEF00050509
    : (6, 4)





** Literature

