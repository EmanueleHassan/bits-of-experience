#+BEGIN_COMMENT
.. title: Code Quality Management and Monitoring
.. slug: code-quality-management-and-monitoring
.. date: 2022-09-19 17:32:48 UTC+02:00
.. tags: software-engineering
.. category: 
.. link: 
.. description: 
.. type: text

#+END_COMMENT

#+begin_export html
<style>
img {
display: block;
margin-top: 60px;
margin-bottom: 60px;
margin-left: auto;
margin-right: auto;
width: 70%;
height: 100%;
class: center;
}

.container {
  position: relative;
  left: 15%;
  margin-top: 60px;
  margin-bottom: 60px;
  width: 70%;
  overflow: hidden;
  padding-top: 56.25%; /* 16:9 Aspect Ratio */
  display:block;
  overflow-y: hidden;
}

.responsive-iframe {
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  right: 0;
  width: 100%;
  height: 100%;
  border: none;
  display:block;
  overflow-y: hidden;
}
</style>
#+end_export

So basically this post contains a way to statically analyze your code
in order to see the quality of it.

This is important in order find blind spots in your code. Moreover, it
is important to keep track of the history of the quality of the
software in time. So it is a very important piece of the puzzle.

These notes are based on the gradle in action book.

{{{TEASER_END}}}

   Understand the following:

   #+begin_quote
In the Java space, you can choose from a wide range of open source and
commercial solutions, such as Checkstyle, PMD, Cobertura, FindBugs,
and Sonar.  Many of these tools are already available in the form of
Gradle core or third-party plugins and can be seamlessly integrated
into your build.
   #+end_quote
  
   So a good idea to this point is to include these metrics and
   include them in git. Just to keep track about the quality of your
   code as a time series.
   
   In general such tools produce metrics along the following
   categories:

   1. Code Coverage

   2. Adherence to coding standards

   3. Bad coding practices and design problems

   4. Overly complex, duplicated, and strongly coupled code

*** Important practical aspects

    It is important to understand that you should group different code
    quality metrics in different gradle tasks.

    The idea is that you do not want to execute all of them together
    and across the entire codebase. It will take too much time.

    In this sense, you should rather decouple the tasks and also
    understand the opportunity to run them for smaller portions of
    your codebase.

    #+begin_quote
For example, during development you may want to know whether you’ve
improved the code coverage of the class you’re currently refactoring
without having to run other lengthy code quality processes.
    #+end_quote
    
    
** Code Coverage

   Code coverage analysis (also called test coverage analysis) is the
   process of finding the areas in your code that are not exercised by
   test cases.

   Empirical studies show that reasonable code coverage has an
   indirect impact on the quality of your code.

   In any case understand the following:

   #+begin_quote
But despite all these benefits, code analysis *doesn’t replace a code
review* by an experienced peer; rather, it complements it.
   #+end_quote

   A further important question in this space is the type of coverage
   metrics that you want to use.

   
*** Types of Coverage

    There is in fact more than an option to consider and especially the
    following:

    - /Branch coverage/: Measures which of the possible execution paths
      (for example, in if/else branching logic) is executed by tests.

    - /Statement coverage/: Measures which statements in a code block
      have been executed. (The default in pytest-coverage).

    - /Method coverage/: Measures which of the methods were entered
      during test execution

    - /Complexity metrics/: Measures cyclomatic complexity (the number
      of independent paths through a block of code) of packages,
      classes, and methods.

    Especially interesting to me is the /complexity metric/ as it is a
    metric that I want to explore on the codebase I work on as being
    quite indicative.

*** Types of Instrumentation

    Essentially instrumentation is the way your programs actually
    check the coverage.

    I.e. as per gradle in action:

    #+begin_quote
The job of instrumentation is to inject instructions that are used to
detect whether a particular code line or block is hit during test
execution.
    #+end_quote
    
    Understand that there are now three types of instrumentation
    possibilities in Java:

    1. *Source code* instrumentation adds instructions to the source
       code before compiling it to trace which part of the code has
       been executed. 

    2. *Offline bytecode* instrumentation applies these instructions
       directly to the compiled bytecode. 

    3. *On-the-fly* instrumentation adds these same instructions to the
       bytecode, but does this when it’s loaded by the JVM’s class
       loader.

    Note now, that obviously the last one is the better option, due to
    the following straightforward reasoning:

    #+begin_quote
In the context of a continuous delivery pipeline, where bundling the
deliverable is done after executing the code analysis phase, you want
to make sure that the source or bytecode isn’t modified after the
compilation process to avoid unexpected behavior in the target
environment.  Therefore, on-the-fly instrumentation should be
preferred.
    #+end_quote

    Note that in other cases there are ways to avoid instrumentation
    problems. However, this requires more thought and I am not
    treating it here as for the moment I am going with an on the fly
    implementation software.

*** Software Decision
    
    I will use /JaCoCo/ for running coverage and this is on the fly
    implementation such that this problem is solved in the first
    place. Note that JaCoCo produces *line and branch coverage*
    metrics. The complexity coverage metric is missing in this sense.

    If you want that cyclomatic complexity you should go with
    enterprise solutions such as Cobertura. Keep it in the back of
    your mind. You are likely going back with the request at some
    point.

    An example of cobertura report would look as follows, you see that
    you have a lot information there:

    
    #+begin_export html
     <img src="../../images/Screenshot 2022-09-21 112958.png" class="center">
    #+end_export


** Static Code Analysis

   Recall the intro.

   Basically these software covers the other points 2-4.

   An overview of the available tools is the following:

   #+begin_export html
    <img src="../../images/Screenshot 2022-09-21 115628.png" class="center">
   #+end_export

      
*** Checkstyle

    So this is the first component of the static code analysis is
    style checking.

    It is important to adhere to coding styles. It will keep your code
    uniform and clean.

    In my python projects I have the style checks already implemented
    at lsp level giving me warnings if I am not [[https://github.com/PyCQA/pycodestyle][pycodestyle conform]].

    In this sense the first step in this dimension is to select a
    relevant coding style. Check at the [[https://checkstyle.sourceforge.io/][following webpage]] in this
    sense.

    Note that while this is the primary task of checkstyle the
    *project developed in time*:

    #+begin_quote
The project started out as a tool for finding areas in your source
code that don’t comply with your coding rules. Over time, the feature
set was expanded to *also check for design problems, duplicate code,
and common bug patterns*.
    #+end_quote

    However, note that in this sense it is better to use the PMD
    project. This is focused simply around the latter.
    
    
*** PMD

    As mentioned [[http://pmd.sourceforge.net/][PMD]]:

    #+begin_quote
PMD is similar to Checkstyle, though it exclusively focuses on coding
problems like dead or duplicated code, overly complex code, and
possible bugs.
    #+end_quote
    

*** FindBugs / SpotBugs

    [[http://findbugs.sourceforge.net/][This]] is as well for finding bugs and general poor design
    practices.

    #+begin_quote
The bugs identified include problems like equals/hashCode
implementations, redundant null checks, and even performance
issues. Unlike the other analyzers presented earlier, FindBugs
operates on the Java bytecode, rather than the source code. You’ll
find that operating on bytecode makes the analysis slower than source
code analysis. For bigger projects, be prepared for it to take
minutes.
    #+end_quote

    Note that according to the official gradle documentation in the
    migration between 5.X -> 6.X the findbugs plugin was
    deprecated. It was replaced by [[https://github.com/spotbugs/spotbugs-gradle-plugin][the following]].

    Note that this seems to be quite well done as a check. The issue
    is that it is not properly integrated and does not generate html
    or xml reports via the gradle plugin.

    You should maybe use something [[https://spotbugs.readthedocs.io/en/latest/running.html][along these lines]].

    I leave it for another moment though.

    In any case this is most likely the go to software - see for
    instance this thread:

    You can understand it as well by the fact that there are a lot of
    adds-on to it, such as:

    - https://find-sec-bugs.github.io/
    - https://github.com/KengoTODA/findbugs-slf4j    


*** JDepend

    [[http://clarkware.com/software/JDepend.html][This]] is also a very cool one and differes in general from the
    above static code analysis.

    #+begin_quote
The static code analysis tool JDepend produces metrics that measure
the design quality of your code.

It scans all packages of your Java code, counts the number of classes
and interfaces, and determines their dependencies.

This information will help you identify hot spots of unwanted or
strong coupling.
    #+end_quote


*** Other options

    Note that there a ton of options on top of the above.

    You can navigate the world wide web and try to make sense of it.

    Some that might be worth exploring at some point are:

    - [[https://niels.nu/blog/2018/testing-the-architecture][archUnit]]

   
** Aggregation of the different Code Quality Reports

   Understand the following:

   #+begin_quote
You’ve seen how to generate code metrics for your project using
different code analysis tools. Each of these tools produces reports
that need to be checked individually.  With every build, the existing
report is potentially deleted and a new one is created, so you have no
idea whether the code quality has improved or decayed over time. What
you need is a centralized tool that monitors, visualizes, and
aggregates your metrics. A tool that provides this functionality is
[[http://www.sonarsource.org/][Sonar]].
   #+end_quote

   Note that you can easily understand that the way the person
   develops in the book is quite outdated. Some of the plugins do not
   exists anymore in newer software versions.

   Moreover, even the sonar component is not as it used to be. It is
   now split across two projects.

   1. =sonarlint=:

      this is cool cause it has a flavour of the static analysis tools
      you have seen above but it is on the fly.

      See the [[https://www.sonarqube.org/][official webpage]] for more.

      Note that there is an emacs plug in for it. It works in tandem
      with lsp so you are [[https://github.com/emacs-lsp/lsp-sonarlint][good to go]].

   2. =sonarqube=:

      This is actually the point mentioned in the gradle in action
      book.

      It is quite short there. You shall read more into detail over
      [[https://docs.sonarqube.org/latest/setup/install-server/][here]].

      
   
** IN-PROGRESS test sonarlint and integrate it with lsp. write notes where necessary

   I installed everything and verified everything.

   Should work correctly.


   

   

   
      
   
   
