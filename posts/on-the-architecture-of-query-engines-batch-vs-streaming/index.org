#+BEGIN_COMMENT
.. title: On the Architecture of Query Engines - Batch vs Streaming
.. slug: on-the-architecture-of-query-engines-batch-vs-streaming
.. date: 2020-06-27 11:46:43 UTC+02:00
.. tags: Big Data
.. category: 
.. link: 
.. description: 
.. type: text

#+END_COMMENT


In the previous post we addressed the Jsoniq declarative and
functional language for querying data in tree shape. It was mentioned
that due to data independence it is possible to run such Java Library
in combination to many Big Data frameworks and in general in
combination with the Hadoop framework - MapReduce, Spark, HDFS etc -.

This post addresses the importance of understanding how a query engine
operates despite the data independence. 

I.e. you might well be able to run and retrieve your desired data
without understanding how different engines work under the hood;
however you might then not write meaningful queries or run into issue
if your hardware infrastructure is not suited to fit your desired data
size and query engine operations.

{{{TEASER_END}}}

#+BEGIN_EXPORT html
<br>
<br>
#+END_EXPORT

* On the Architecture of Query Engines

We will look at what happens under the hood when you pass a Jsoniq
query to the query engine. A quick lesson on the imnportance of data
streaming solutions vs. batch processing, i.e. their respective merits and
their weakness will emerge.

Consider the following query:

#+begin_src python
%%rumble
count (
  for $i in json-lines ("file.json")
  return $i.field
)
#+end_src


Then the query is first translated into an =abstract syntax tree=,
that parse the jsoniq commands and represents the =Jsoniq= *FLWOR*
operations, i.e. the =for= (iterator), =let= (declarator), =where=
(selector), =order By=, =return= operators.

Such =abstract syntax tree= is then translated into an =expression tree=
that is optimized and rearranged for the operations it has to
execute. 

Finally, the tree is transformed into an =iterator tree= that executes
the necessary operations.

#+begin_export html
<style>
 {
  box-sizing: border-box;
  margin-top: 60px;
  margin-bottom: 60px;
}

.column {
  float: left;
  width: 33.2%;
  padding: 0px;
}

/* Clearfix (clear floats) */
.row::after {
  content: "";
  clear: both;
  display: table;
}
</style>

<div class="row">
  <div class="column">
    <img style="width:100%" src="../../images/Bildschirmfoto_2020-06-27_um_11.12.26.png">
  </div>
  <div class="column">
    <img style="width:100%" src="../../images/Bildschirmfoto_2020-06-27_um_11.12.33.png">
  </div>
  <div class="column">
    <img style="width:100%" src="../../images/Bildschirmfoto_2020-06-27_um_11.12.40.png">
  </div>
</div>
#+End_export


It is now clear that given an iterator tree as the one above there is
a huge difference in terms of the required hardware infrastructure and
performance if deciding to go for a materialized =batched execution=
or in the case of going through a =streamed execution=.

In this sense consider that the above query runs through a spark
engine on a very large object. If the data are materialized,
i.e. computed before being passed to the return iterator it is clear
that for the case where each object results in a data point you might
quickly run into memory issues as materializing all of the data in
memory before returning them might *not be feasible*.

In contrast a *streaming* execution or /real-time/ execution
circumvents such an issue. Its logic is to process, materialize *and
return* a single data point at the time. In such a way it is clear to
understand that you would not as easily run into memory management
issues.

Notice that the time-bottleneck of the two execution plans is
different. While in batch operations you might perform heavy lifting
computations and the focus is on throughput rather than latency, in
streaming execution the focus is *on latency*. Here if passing the
data from the =for iterator= to the =return iterator= to the user
takes a lot of time the entire process will take up ages.

Finally, notice that the extent to which operations you can perform on
a streaming execution engine is different compared to the one of a
batch execution engine. I.e. in streaming execution engines data must
be independent so that you can process them one by one without
affecting the end result. It is therefore clear that standard
operations such as =group by= or =order by= cannot be performed on
them. In such a case for dealing with the issues of materializing your
data you rely on distributed processing via spark and MapReduce for
performing the batch operations of choice. There each machine would
return in the final stage of the computation therefore tackling the
memory issue. 

It is clear that in the tutorial above and the =group by= and =order
by= operations Jsoniq leverages batch execution and the spark
engine. It is however important that due to this fact depending on the
query you write you might need a *very large amount of machines*.

I could experience the issue first hand when running a job on my local
machine with a spark cluster running over 10 virtual cores of my local
machine. I run into memory issues and could not perform the desired
query. I had eventually to work on the cloud with multiple workers
with around 8 virtual cores each.


* Literature

[[https://www.systems.ethz.ch/courses/spring2020/bigdataforeng/material][Big Data for Engineers - ETH 2020]]

