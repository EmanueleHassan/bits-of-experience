#+BEGIN_COMMENT
.. title: PySpark Set-Up
.. slug: pyspark-set-up
.. date: 2019-08-05 23:51:11 UTC+02:00
.. tags: Big Data, Spark
.. category: 
.. link: 
.. description: 
.. type: text

#+END_COMMENT


This post aims at providing an overview of the necessary steps required in order to leverage Apache Spark through the Python API and the PySpark module.

Despite the information is vastly reported over the internet my usual /procedere/ when I deal with new tools and software is to write a short piece of note when learning new software that is intended as an overview over the tool and serve as a beginner cheat sheet.

{{{TEASER_END}}}

** Disclaimer

Ok, before starting let me underline that this post falls under the category /learn by posting/.
As such it may contain a flawed theoretical understanding about the subject and some inaccuracies.

You are can read more about this kind of posts on the home-page under [[https://marcohassan.github.io/bits-of-experience/pages/bits-of-experience-a-readable-view-on-my-study-adventures/][this link]].


** PySpark - Why?

During my Internship as a Data Analyst at Expedia Group I had the opportunity to start working with big, messy data. 
The amount of it is impressive and I recently was told that Expedia collects as much as 13 Terabyte of data each single day (don't quote me on this though as the claim was not verified).

This enormous amount of data becomes /Big Data/, which is simply a huge amount of data that requires special technology and software to be treated as it can poorly be handled through standard hardware on local machines.

Here /PySpark/ comes to play. This is nothing else than a module that allows the user to connect to distributed files systems and leverage the Apache Spark infrastructure. The idea of this post is now not to give you an overview about the exact infrastructure architecture as, being a beginner in the field, this is still obscure to me.
In contrast, the idea is to document the PySpark software installation and the basics data manipulation operations. In fact, after taking a brief look at the syntax I came to the realization that PySpark shares the most of the standard /Pandas/ and /SQL/ operations and hence I thought it might be good practice to start to use PySpark syntax universally even when working on small dataset locally to get use to its syntax and open up the doors of in memory big data operations at a later time point.

** Software Download

lsfjslfjD


