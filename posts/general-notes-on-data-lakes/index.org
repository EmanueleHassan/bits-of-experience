#+BEGIN_COMMENT
.. title: General Notes on Data Lakes
.. slug: general-notes-on-data-lakes
.. date: 2021-05-27 16:57:36 UTC+02:00
.. tags: Data
.. category: 
.. link: 
.. description: 
.. type: text

#+END_COMMENT


Some general notes on data lakes. Sum up a bit the general decisions
in the data world.

More interesting is the Zaloni approach discussed later in the notes.

{{{TEASER_END}}}

* Important points

Perhaps more important, most data warehouses require that business
users rely on IT to do any manipulation or enrichment of data, largely
because of the inflexible design, system complexity, and intolerance
for human error in data warehouses. This slows down business
innovation.

*Schema-on-read allows users to build custom schemas into their queries
upon query execution.*

Unlike a monolithic view of a single enterprise-wide data model, the
data lake allows you to put off modeling until you actually use the
data, which creates opportunities for better operational insights and
data discovery.

Data warehouses are notorious for not being able to scale beyond a
certain volume due to restrictions of the architecture. Data
processing takes so long that organizations are prevented from
exploiting all their data to its fullest extent.

Because data can be unstructured as well as structured, you can store
everything from blog postings to product reviews. And the data doesn’t
need to be consistent to be stored in a data lake.

This would be problematic in a data warehouse; in a data lake,
however, you can put all sorts of data into a single repository
without worrying about schemas that define the integration points
between different data sets.

Unlike data warehouses, data lakes don’t come with governance built
in. This augments the risk of creating a proper data swamp.

Metadata is not automatically applied when data is ingested into the
data lake.  Without the technical, operational, and business metadata
that gives you information about the data you have, it is impossible
to organize your data lake and apply governance policies. Metadata is
what allows you to track data lineage, monitor and understand data
quality, enforce data privacy and role-based security, and manage data
life cycle policies. This is particularly critical for organizations
in tightly regulated industries.

* On-premise vs. Cloud

Whether to use on-premises or cloud storage and processing is a complicated
and important decision point for any organization.

Generally speaking, on-premises storage and processing offers tighter
control over data security and data privacy, whereas public cloud
systems offer highly scalable and elastic storage and computing
resources to meet enterprises’ need for large scale processing and
data storage without having the overheads of provisioning and
maintaining expensive infrastructure.

* Schema on Write vs. Schema on Read

This is the essential difference between data lakes and traditional
data warehouses.

** Schema on Write

 Schema on write is defined as creating a schema for data before
 writing into the database. If you have done any kind of development
 with a database you understand the structured nature of Relational
 Database(RDBMS) because you have used Structured Query Language (SQL)
 to read data from the database. This is the standard ETL pipeline.
 Transformation to fit the desired schema is necessary in traditional
 data warehouses.

 Remember just because the data is structured doesn’t mean it starts
 out that way. Most of the data that exist is in an unstructured
 fashion. Not only do you have to define the schema for the data but
 you must also structure it based on that schema.

** Schema on Read

 Schema on read differs from schema on write because you create the
 schema only when reading the data. Structured is applied to the data
 only when it’s read, this allows unstructured data to be stored in the
 database. Since it’s not necessary to define the schema before storing
 the data it makes it easier to bring in new data sources on the fly.

 The exploding growth of unstructured data and overhead of ETL for
 storing data in RDBMS is the main reason for shift to schema on
 read. This is why the most companies are shifting to datalakes.

[[https://www.thomashenson.com/schema-read-vs-schema-write-explained/][nice overview over databases technologies history and schema on write
vs schema on read]].

* Storage options

A sample technology stack for the storage function of a data lake may consist of
the following:

- Hadoop Distributed File System (HDFS)

A Java-based filesystem that provides scalable and reliable data
storage. It is designed to span large clusters of commodity
servers. For on-premises data lakes, HDFS seems to be the storage of
choice because it is highly reliable, fault tolerant, scalable, and
can store structured and unstructured data. This allows for faster
processing of the big data use-cases. HDFS also allows enterprises
to create storage tiers to allow for data life cycle management, using
those tiers to save costs while maintaining data retention policies
and regulatory requirements.

- Object storage

Object stores (Amazon Simple Storage Service [Amazon S3], Microsoft
Azure Blob Storage, Google Cloud Storage) provide scalable, reliable
data storage. Cloud-based storage offers a unique advantage. They are
designed to decouple storage from computing so that they can autoscale
compute power to meet the real-time processing needs.

- Apache Hive tables

An open source *data warehouse* system for querying and analyzing large
datasets stored in Hadoop files.

- HBase

An open source, nonrelational, distributed database that is modeled
after Google’s BigTable. Developed as part of Apache Software
Foundation’s Apache Hadoop project, it runs on top of HDFS, providing
BigTable-like capabilities for Hadoop.

- ElasticSearch

An open source, RESTful search engine built on top of Apache Lucene
and released under an Apache license. It is Java-based and can search
and index document files in diverse formats.

* Batch vs Streaming Data Processing

Batch processing is where the processing happens of blocks of data
that have already been stored over a period of time. For example,
processing all the transaction that have been performed by a major
financial firm in a week.

Batch processing works well in situations where you don’t need
real-time analytics results, and when it is more important to process
large volumes of data to get more detailed insights than it is to get
fast analytics results.

Stream processing is a golden key if you want analytics results in
real time. Stream processing allows us to process data in real time as
they arrive and quickly detect conditions within small time period
from the point of receiving the data. 

In Batch Processing it processes over all or most of the data but In
Stream Processing it processes over data on rolling window or most
recent record. So Batch Processing handles a large batch of data while
Stream processing handles Individual records or micro batches of few
records.

Stream processing allows you to feed data into analytics tools as soon
as they get generated and get instant analytics results. There are
multiple open source stream processing platforms such as Apache Kafka,
Apache Flink, Apache Storm, Apache Samza, etc.

Different tools are needed, based on whether your use case involves
batch or streaming. For batch use cases, organizations generally use
Pig, Hive, Spark, and MapReduce. For streaming use cases, they would
likely use different tools such as Spark-Streaming, Kafka, Flume, and
Storm.

Interesting here is especially: *Apache Beam*

Apache Beam provides an abstraction on top of the processing
cluster. It is an open source framework that allows you to use a
single programming model for both batch and streaming use cases, and
execute pipelines on multiple execution environments like Spark,
Flink, and others. By utilizing Beam, enterprises can develop their
data processing pipelines using Beam SDK and then choose a Beam Runner
to run the pipelines on a specific largescale data processing
system. The runner can be a number of things: a Direct Runner, Apex,
Flink, Spark, Dataflow, or Gearpump (incubating). This design allows
for the processing pipeline to be portable across different runners,
thereby providing flexibility to the enterprises to take advantage of
the best platform to meet their data processing requirements in a
future-proof way.  Data Lake

* Data lake management

Data lakes created with an integrated data management framework can
eliminate the cumbersome data preparation process of ETL that
traditional data warehouse requires. Data is smoothly ingested into
the data lake, where it is managed using metadata tags that help
locate and connect the information when business users need it. This
approach frees analysts for the important task of finding value in the
data without involving IT in every step of the process, thus
conserving IT resources.

Key to a solid data management and governance strategy is having the
right metadata management structure in place. With accurate and
descriptive meta-data, you can set policies and standards for
managing and using data.

* Zaloni Schema

A reference architecture is a framework that organizations can refer to in order
to

   1) understand industry best practices
   2) track a process and the steps it takes,
   3) derive a template for solutioning
   4) understand the components and technologies involved

We recommend organizing your data lake into four zones, plus a
sandbox. Throughout the zones, data is tracked, validated, cataloged,
assigned metadata, refined, and more.

** Zone 1: The Transient Landing Zone

We recommend loading data into a transient loading zone, where basic
data quality checks are performed using MapReduce or Spark processing
capabilities.

Many firms moreover require strict regulation over the saved
data. Think about companies saving personal & confidential data. This
is where the importance of the transient landing zone becomes
clear. Data will be encrypted and masked in this zone so that all of
the data requirements are met.

The transient zone is temporary; it is a landing zone for data where
security measures can be applied before it is stored or accessed. With
GDPR being enacted in the EU, this zone might become even more
important because there will be higher levels of regulation and
compliance, applicable to more industries.

** Zone 2: The Raw Zone

After the quality checks and security transformations have been performed in the
Transient Zone, the data is then loaded into in the Raw Data zone for storage.
However, in some situations, a Transient Zone is not needed, and the Raw Zone
is the beginning of the data lake journey.

Within this zone, you can mask or tokenize data as needed, add it to
catalogs, and enhance it with metadata. In the Raw Zone, data is
stored permanently and in its original form, so it is known as “the
single source of truth.”

** Zone 3: The Trusted Zone

The Trusted Zone imports data from the Raw Zone. This is where data is
altered so that it is in compliance with all government and industry
policies as well as checked for quality. Organizations perform
standard data cleansing and data validation methods here.

Often the data within this zone is known as a “single version of
truth.”

*This trusted repository can contain both master data and reference
data. Master data is a compilation of the basic datasets that have
been cleansed and validated. An organization needs to ensure that data
kept in the trusted zone is up to date using change data capture (CDC)
mechanisms.* (this is what you were operating on at Expedia).

Reference data, on the other hand, is considered the single version of
truth for more complex, blended datasets. For example, that healthcare
organization might have a reference dataset that merges information
from multiple source tables in the master data store.

** Zone 4: The Refined Zone

Within the Refined Zone, data goes through its last few steps before
being used to derive insights. Data here is integrated into a common
format for ease of use, and goes through possible detokenization,
further quality checks, and life cycle management. This ensures that
the data is in a format from which you can easily use it to create
models. Consumers of this zone are those with appropriate role-based
access.

Data is often transformed to reflect the needs of specific lines of
business in this zone.

** Sandbox

The Sandbox is integral to a data lake because it allows data
scientists and managers to create ad hoc exploratory use cases
without the need to involve the IT department.

Data can be imported into the Sandbox from any of the zones, as well
as directly from the source. This allows companies to explore how
certain variables could affect business outcomes and therefore derive
further insights to help make business management decisions. You can
send some of these insights directly back to the raw zone, allowing
derived data to act as sourced data and thus giving data scientists
and analysts more with which to work.

* Data Ingestion

Organizations have a number of options when transferring data to a
data lake.  Managed ingestion gives you control over how data is
ingested, where it comes from, when it arrives, and where it is stored
in the data lake.

With managed ingestion, data (both structured and unstructured) and
metadata are linked. There are two approaches to link data and
metadata. 

- The first approach is when metadata is known; for example, a meta
  file that describes the column name and data type that comes with
  the data file (consider for instance the case of importing data from
  an existing datawarehouse with named columns).

- The second approach is when metadata is not known ahead of time and
  the data stewards identify and tag individual cells after data is
  ingested.

The result is a giant table: "With managed ingestion, you enter all
data into a giant table organized with metadata tags. Each piece of
data—whether a customer’s name, a photograph, or a Facebook post—is
placed in an individual cell. It doesn’t matter where in the data lake
that individual cell is located, where the data came from, or its
format.  You can connect all of the data easily through the tags. You
can add or change tags as your analytic requirements evolve—one of the
key distinctions between a data warehouse and a data lake."

Using a managed ingestion process with a data lake opens up tremendous
possibilities. *You can quickly and easily ingest unstructured data
and make it available for analysis without needing to transform it in
any way*.

/Regulating Privacy:/

You can also protect sensitive information. As data is ingested into
the data lake and moves from the Transient Zone to the Raw Zone, each
cell is tagged according to how “visible” it is to different users in
the organization.

An integrated data lake management platform will perform managed
ingestion, which involves getting the data from the source systems
into the data lake, guaranteeing a repeatable process, and ensuring
that if anything fails in the ingest cycle, there will be operational
functions that take care of it.

*** The Stream Data Ingestion Case

Additionally, as we see more and more workloads moving to streaming,
whatever data management functions you applied to batch ingestion—when
data was coming in periodically—now needs to be applied to data that
is streaming in continuously. Integrated data lake management
platforms should be able to detect whether certain streams are not
being ingested based on the service-level agree‐ ments (SLAs) you set.

Additionally, as we see more and more workloads moving to streaming,
whatever data management functions you applied to batch ingestion—when
data was coming in periodically—now needs to be applied to data that
is streaming in continuously. Integrated data lake management
platforms should be able to detect whether certain streams are not
being ingested based on.


*** Top Down vs. Bottom Up Data Governance Approach

In a bottom-up approach, consumers of the data lake are probably data
scientists or data analysts. Collective input from these consumers
decides which datasets are valuable and useful and have good quality
data. The data lake then exposes those datasets to other consumers so
that they can see the ways that their peers have been successful with
the data lake. (this was the way expedia operated).

A top-down method takes best practices from organizations’ data
warehouse experiences and attempts to impose governance and management
from the moment the data is ingested into the data lake.

* Trends and Opportunities

** Logical Data Lakes & Federated Queries

 We are seeing more and more requirements for hybrid data stores, in
 which data can be stored not only in Hadoop Distributed File System
 (HDFS), but also in object data stores, such as Amazon Simple Storage
 Service (Amazon S3) or Microsoft Azure Elastic Block storage, or in
 No-SQL databases.

 To make this work, enterprises need a *unified view of data that
 exists across these multiple data stores* across the multiple
 environments in an enterprise.

 The integration of these various technologies and stores within an
 organization can lead to what is, in effect, a logical data lake.

 This goes hand in hand with the necessity of running *federated
 queries*. As data is stored in different physical and virtual
 environments, you might need to use different query tools and
 decompose a user’s query into multiple queries—sending them to
 on-premises data stores as well as cloud-based data stores, each of
 which possesses part of the answer. Then, the answers are aggregated
 and combined and sent back to the user such that they get one version
 of the truth across the entire logical data lake.

 /The above would be big revolution/. /Would be interesting to read
 about how much research is going into the area/.

** Enterprise Data Marketplace

Another trend is to make data available to consumers via rich metadata
data cata‐ logs in a shopping cart. Many enterprises are already
building these portals out of shared Hadoop environments, where users
can browse relevant parts of the data lake and have an Amazon-like
shopping cart experience in which they select data based on various
filters.

** Intelligent Data Lakes

Not far off in the future are more advanced data environments that use
automation and machine learning to create intelligent data
lakes. With machine learning, you can build advanced capabilities such
as text mining, forecast modeling, data mining, statistical model
building, and predictive analytics. The data lake becomes “responsive”
and “self-correcting,” with an automated data life cycle process and
self-service ingestion and provision. Business users have access and
insight into the data they need (for instance 360-degree views of
customer profiles), and they don’t need IT assistance to extract the
data that they want.

** IoT and streaming of Data

As the Internet of Things (IoT) continues to grow, much of the data
that used to come in via a batch mode is coming in via streaming
because the data is being generated at such high velocity. In such
cases, enterprises are beginning to keep the data in memory for
near-real-time streaming and analytics, to generate insights extremely
quickly. This adds another dimension to data lakes—that is, not just
being able to process high volumes of data at scale, but to provide
lowlatency views of that data to the enterprise so that it can react
and make better decisions on a near-real-time basis.

* Literature 

[[https://www.oreilly.com/data/free/files/architecting-data-lakes.pdf][O'Reilly: Architecting Data Lakes]]





