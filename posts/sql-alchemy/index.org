#+BEGIN_COMMENT
.. title: SQL Alchemy
.. slug: sql-alchemy
.. date: 2022-05-11 09:57:08 UTC+02:00
.. tags: Python, sql, Databases
.. category: 
.. link: 
.. description: 
.. type: text

#+END_COMMENT

So basically I am starting to leverage the correct stack in this
field.

This will help me to properly and quickly navigate the data.

{{{TEASER_END}}}

I am sorry but based on my experience there is really no game in
town.

For ETL and data analytics python is superior to Java. Maybe there
would be a discussion with Scala but to properly judge I would need to
take some time to explore.

This is left for a different moment.

That said I will keep my structure as follows:

- use java for orchestrating your applications

- use python for the actual ETL jobs

combine the two via APIs.

So now everything is ready, just waiting for the security piece in the
pipeline.

Moreover after the integration specified in this post you will be able
to move away from pure SQL. There is monster SQL that is developed in
my team.

There is little chance to explore these huge queries bit by bit.. I
mean too much manual copy and paste for my taste and too little slice
and dice options.

With this set-up you will be able to:

- fetch data from DBs

- use SQL and/or pandas native methods to transform your data

  (just think about the apply methods and the rich ways to transform
  your data in comparison to pure SQL)

- explore the data programatically at development time in Python

- insert the data into DBs

No way this is not the correct stack. Everyone that tries to talk you
out of it is clearly out of track. After all the /just do it/
mentality of this team has some pros. Nobody will be able to stop you
if you deliver.

So this post briefly touches on how to properly set up the thing and
the little things that is important to consider in order to keep
everything under control in the long run.

** SQL Alchemy

   Note that this is important - with it you essentially have a
   full-fledge toolkit to operate on SQL and have the possibility of
   performing ORM mapping. Note that it is [[https://en.wikipedia.org/wiki/Data_mapper_pattern][similar in spirit]] to
   Hibernate so you see that despite switching languages the concepts
   are very much the same across.

   For me the most interesting thing of it is however the integration
   with pandas. This will be the killer for me and most likely the way
   I will be working in.

   This rather than via ORM. Maybe I will be switch idea in time as I
   will have to go in this space with Hibernate and Java in either
   case and then I will be able to understand the benefits and think
   if it makes sense or rather stick to a pandas approach in the
   python space.

*** Connecting to DB - Microsoft SQL Set Up

    In any case, that said, the first thing to understand in this
    dimension is on how to create a SQL Alchemy engine in order to
    work in such a way.

    We are working with Microsoft SQL Servers. So here a snippet in
    order to see how to work with it. In any case there are a tons of
    dialects supported by sql-alchemy and should you ever work with
    other relational DBs you should be able to smoothly and swiftly
    change them.

    Essentially I created a python snippet in order to generate
    connection strings that will be passed to the =pyodbc= driver in
    order to properly set up the connection.

    You can see the snippet below
 
    #+BEGIN_SRC python
"""Return json with the arguments in order to open a DB connection."""

import struct
from azure.identity import DefaultAzureCredential


def connectionString(driver: str, server: str,
                     database: str, cloud: bool) -> dict:
    """Return the necessary arguments in order to open a DB connection."""
    json_dump = {}

    if not cloud:

        AUTHENTICATION = ";Trusted_Connection=yes" # using microsoft
                                                   # integrated
                                                   # identity

        connstr = "{}{}{}{}".format(driver, "SERVER=" + server,
                                    "DATABASE=" + database, AUTHENTICATION)

        json_dump["connstr"] = connstr

        return json_dump

    default_credential = DefaultAzureCredential()            # using AAD
    access_token = default_credential. \
        get_token('https://database.windows.net/.default')

    exptoken = b""

    for i in str.encode(access_token[0]):
        exptoken += bytes({i})
        exptoken += bytes(1)
        tokenstruct = struct.pack("=i", len(exptoken)) + exptoken

    connstr = "{}{}{}".format(driver, "SERVER=" + server,
                              "DATABASE=" + database)

    json_dump["connstr"] = connstr
    json_dump["tokenstruct"] = tokenstruct

    return json_dump

    #+END_SRC

   Then this will be consumed by my applications as follows:

   #+BEGIN_SRC python
    # Insert
    json_conn_info = connectionString(data["driver"],
                                      conn_par["insert"]["server"],
                                      conn_par["insert"]["database"],
                                      conn_par["insert"]["is_cloud"])

    # Work with SQL Alchemy
    params = urllib.parse.quote_plus(json_conn_info["connstr"])

    if conn_par["insert"]["is_cloud"]:

        engine = create_engine("mssql+pyodbc:///?odbc_connect=%s" % params,
                               connect_args={'attrs_before':
                                             {1256:
                                              json_conn_info["tokenstruct"]}})
    else:

        engine = create_engine("mssql+pyodbc:///?odbc_connect=%s" % params)

   #+END_SRC

   With it you will have your engines.

   You can verify that the engine is properly set up by trying to open
   a connection =engine.connect()=.
   
   You can read more about other options for setting it up [[https://docs.sqlalchemy.org/en/13/core/engines.html][here]]. There
   are as well the other dialects listed etc.

   I will skip now to the pools, the other important bit of
   configuration when setting up an engine together with the dialect.

**** Close connections

     Note that despite of working with connection pools you should
     still be careful in returning the connections to the pool.

     Check this [[https://stackoverflow.com/questions/8645250/how-to-close-sqlalchemy-connection-in-mysql][in this sense]].

     So basically this is why you should work in the following way
     when writing your code:

     #+BEGIN_SRC python
with engine.connect() as connection:
    df1.to_sql(name=conn_par["insert"]["name"], con=connection,
	       schema=conn_par["insert"]["schema"],
	       if_exists=conn_par["insert"]["if_exists"], index=False)
     #+END_SRC    

     Or alternatively by:

     #+BEGIN_SRC python
for i in range(1,2000):
    conn = db.connect()
    #some simple data operations
    conn.close()
     #+END_SRC

     When you say =conn.close()=, the connection is returned to the
     connection pool within the Engine, not actually closed.

     If you do want the connection to be actually closed, that is, not
     pooled, disable pooling via NullPool.

**** Disconnect Handling

     You can read about it [[https://docs.sqlalchemy.org/en/13/core/pooling.html#connection-pool-configuration][here]].

     It is not very well stated. I get that the default is the
     following if you do not specify anything:

     #+begin_quote
When pessimistic handling is not employed, as well as when the
database is shutdown and/or restarted in the middle of a connection’s
period of use within a transaction, the other approach to dealing with
stale / closed connections is to let SQLAlchemy handle disconnects as
they occur, at which point all connections in the pool are
invalidated, meaning they are assumed to be stale and will be
refreshed upon next checkout.
     #+end_quote

     Start with it, if you get issues at some point go back there and
     explore.

**** Multiprocessing

     Note that if you will go on multiprocessing in python, you will
     have to return [[https://docs.sqlalchemy.org/en/13/core/pooling.html#using-connection-pools-with-multiprocessing-or-os-fork][here]].

     There is explained how to make the connection available across
     processes. 

*** Logging

    This is as well a thing that you will have to properly set up if
    you want to create the thing in a proper way.

    Check at the following:

    #+begin_quote
Python’s standard logging module is used to implement informational
and debug log output with SQLAlchemy.

This allows SQLAlchemy’s logging to integrate in a standard way with
other applications and libraries.

There are also two parameters create_engine.echo and
create_engine.echo_pool present on create_engine() which allow
immediate logging to sys.stdout for the purposes of local development;
these parameters ultimately interact with the regular Python loggers
described below.
    #+end_quote

*** Inspect DB

    Here some snippets are saved that are handy in order to operate on
    the sql-alchemny engines.

**** Check at the available tables

#+BEGIN_SRC python
engine.table_names()
#+END_SRC   

*** Update
    CLOSED: [2022-05-17 Di. 13:49]

    Note that with sql-alchemy you can essentially execute all of the
    standard operations on your database.

    You can use the =engine.execute()= method for it. Then the engine
    will automatically ask for an available connection from the
    connection pool, perform the given action and return it to the
    connection pool.
    
    Note that unfortunately there is no update statement in pandas so
    that this bit is missing from the logic and the integration with
    SQL-Alchemy.

    So the workflow would resemble something to the following:

    Extract the existing table from the db, update it in the
    application logic and replace alltogether the existing table.

    You can still think in pandas terms and make your transformation -
    in a functional way - there in order to get your update
    statements.

    You can then use f-strings and similar in order to properly
    construct the relevant queries.


** Pandas Interaction with SQL Alchemy

   So basically that is it.

   SQL Alchemy is the engine through which you would ultimately
   mantain the connection to the database.

   You can then leverage such engine in order to interact with the
   data base in the various python modules.

   Think for instance at the pandas module. This is where my interest
   lies.

   There you have the following methods that have to well sit into
   your mind. 

   1. =pd.read_sql_table=

      #+BEGIN_SRC python
      DataFrame.read_sql_table(table_name, con, schema=None,
      index_col=None, coerce_float=True, parse_dates=None,
      columns=None, chunksize=None)
      #+END_SRC

      the important parameter is the =con= parameter.

      - =con=:

	SQLAlchemy connectable or str. A database URI could be
        provided as str.

      So you see that you can provide an =engine.connection()= to it.

      You then pass the table name you want to extract.

      Important might also be the following:

      - =parse_dates=:

      List of column names to parse as dates.

      Dict of ={column_name: format string}= where format string is
      strftime compatible in case of parsing string times or is one of
      (D, s, ns, ms, us) in case of parsing integer timestamps.

      - =index_col=:

	Column(s) to set as index

      - =coerce_float=:

	Attempts to convert values of non-string, non-numeric objects
        (like decimal.Decimal) to floating point. Can result in loss
        of Precision.

   2. =df.read_sql_query=

      #+BEGIN_SRC python
DataFrame.read_sql_query(sql, con, index_col=None, coerce_float=True,
                      params=None, parse_dates=None,
                      chunksize=None, dtype=None)
      #+END_SRC

      Note that the arguments are pretty much the same as above.

      Instead of passing a table and the corresponding schema you
      actually pass a sql statement.

      The other interesting argument is the following:

      - =dtype=:

	You pass the data type for data or columns. E.g. np.float64 or
        {‘a’: np.float64, ‘b’: np.int32, ‘c’: ‘Int64’}.

   3. =df.to_sql=

      #+BEGIN_SRC python
DataFrame.to_sql(name, con, schema=None, if_exists='fail', index=True,
                 index_label=None, chunksize=None, dtype=None, method=None)
      #+END_SRC

      This is interesting as it inserts the table into a table of
      interest.

      You already started using this method so you know more less the
      parameters.

      You have to understand the following:

      - =index=: set it to *false* generally. You do not want to write
        the index of your pandas dataframes to the sql table generally.

      - =dytpe=: same as in the above methods.

      - =if_exists=: you can say - ~{raplace, fail, append}~. 
   

** SQL Lite

   So will work quite a bit with this in memory database.

   This will make it quite easy to develop and has couple of
   advantages that I really like.

   So here some notes to organize my thinking around it and start
   properly to work in this dimension.

   Might refactor then into a different separate post but for the
   moment I am keeping the notes over here.

   /Side note:/

   Do not go too low level in here.

   It is not the same database that you are actually using in your
   company. I.e. the *dialect will be different*.

   So in general keep it for now as a solution while they set up the
   entire Dev space. On the top of it search for an in-memory mssql
   instance.

   So ordered as well the SQL Microsoft Express DB. You can then use
   this locally once it is ready and you would have solved this big
   drawback.

   This is obviously a big drawback.

   But I think it should not be too much of a problem. I am not a DB
   person in any case and hope to cut curnors on the lower level
   details of the DB by keeping all of the logic at the application
   layer. The high level stuff, meaning =insert=, =update=, =alter=,
   =select= statements is the same everywhere.

   I mean I will quickly see in time if this is a bad idea an revert
   in case. It is worth to experiment at the beginning in any case in
   order to find the perfect set up. This will make the difference in
   the long run. 

*** On the issue of the main database

    So basically everytime you connect to a database, its name is
    =main= regardless of the databse file name.

    You can see the attach database option in order to attach
    additional databases.

*** On the schema

    So it seems that sqllite has no schema.

    So basically you have to work without it. See [[https://www.sqlite.org/schematab.html][here]].

    You have just a single schema where everything goes.

    You can then check at the schema of all of your tables over there
    with the =.schema= option.

    This might be handy when copying the data over to your other DBs.
    

** TODO open design questions
*** For Pandas - Data types

    You also have to check with the data types how the situation
    really is. I.e. does it always infer the data types correctly?

    Can you append without big issues?

    -----

    The solution will be a trial and error case.

    -----

    Have to double check properly what data types are available in
    there and what you can start with it. 
