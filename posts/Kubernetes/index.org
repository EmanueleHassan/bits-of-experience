#+BEGIN_COMMENT
.. title: Kubernetes
.. slug: Kubernetes
.. date: 2019-09-06 18:21:43 UTC+02:00
.. tags: IT Architecture, Container Management
.. category: 
.. link: 
.. description: 
.. type: text

#+END_COMMENT

#+BEGIN_EXPORT html
<br>
<br>
#+END_EXPORT

A strong orchestrator tool that operates above the container level and
allows to manage a cluster to handle containers.

It is essentially the tool set that lets you manage containers.

Enterprises can use it to manage the life cycle of containerized apps
in a cluster of nodes, which is a collection of worker machines such
as virtual machines (VMs) or physical machines.

In general kubernetes try to leverage clusters in order to avoid
having a single point of failure.

{{{TEASER_END}}}

Note that in the world of Kubernetes the terminology of the cluster is
slightly different to the one of Big Data.

While the virtual machine controlling other VMs is called master VM,
the other are not called /workers/ but rather /nodes/.

* Theoretical Framework

** Architecture 

The kubernetes architecture is rather simple. It consists of three major components:

- *Docker running on a subnet:* this is the service used to run
  encapsulated container applications. Important is however to notice
  that the Docker service must run on a subnet /(available to each
  node server/.

- *Kubelet Service*: this communicates with the master components and
  receive commands and work.

- *Kube-Proxy Service*: this forwards requests to the correct containers,
  balances the load and makes sure that the isolated networking
  environment is predictable and accessible.

In a more structured way we can say that there are multiple /Pods/
available carrying multiple containers, managed by the /Docker
Engine/, within them. These /Points of Delivery/ are modules of
network, compute, storage bundled with the docker application
containers that work together to deliver networking services.

The Kube-proxy will then redirect network traffic into one of the
various Pods.

Kubelet is an agent responsible to communicate with the master what
happens at node level. For this reason you will find a Kubelet for each
node. If a Pod goes down than it is for instance responsibility of the
Kubelet to communicate this so the master. This will then be
responsible for restarting the Pod.

A good understanding of the general infrastructure can be gained by
looking at the following three videos

#+begin_export html
 <iframe width="800" height="400"
 src="https://www.youtube.com/embed/DZ-Wv3XNoAk">
 </iframe>

<br>
<br>

 <iframe width="800" height="400"
 src="https://www.youtube.com/embed/xhwi3zIVR-8">
 </iframe>

<br>
<br>

 <iframe width="800" height="400"
 src="https://www.youtube.com/embed/DZ-Wv3XNoAk">
 </iframe>
#+end_export


** Master

 Masters are responsible for an entire cluster, they are the one that
 makes the decision on which node to schedule an application.

 The master has multiple components, all under the umbrella of a single
 /control plane/.

 The main component is the API server. API server manages scheduling
 that goes through the REST services. 

 Three other important components are:

 - the /Scheduler/, that interacts with the API server scheduling
   piece but can as well interact directly with REST services. It
   determines where to host new Pods in the cluster. This furthermore
   coordinates with the Kubelet in order to determine the location of
   the Pods based on the load.

 - the /Replication Controllers/: these handle the replicas through the
   API services.

 - the /etcd/: manages state. It is the database of kubernetes. It
   tells kubernetes who is available and what the state of all these
   available things is.


*** Operation

The question that arise now is how do we specify the Master the tasks to be scheduled?

The answer to the question is a /YAML/ file. Here the desired state
for our application is specified and once fed to the Master the latter
will make sure the state is guaranteed.



** Helm

Helm, the Kubernetes native package management system, is used for
application management inside an IBM Cloud Private cluster. The Helm
GitHub community curates and continuously expands a set of tested and
preconfigured Kubernetes applications. Clients use the management
console to select stable applications from a catalog and add them to
their cluster.



** Image Registry

   Here is where you save your images. Registries can be either public
   or private. Once you push an image to a registry is can then be
   pulled by a kubernetes cluster needing the image or in general any
   possible client in need for the image.

   Usually the naming convention when creating a new image is as
   follows:

   #+BEGIN_SRC sh
   <hostname>/<repository>:<image tag>
   #+END_SRC

   Notice that here the hostname is the hostname of the image
   registry, for instance =docker.io=.


* Commands

#+BEGIN_EXAMPLE
## to view the cluster info
$ kubectl cluster-info  

## to look at the different available nodes and get info about their status and roles.
$ kubectl get nodes

## to get the specifications on where all of your clusters are running.
$ kubectl config get-contexts
#+END_EXAMPLE

Once you have a running Kubernetes cluster, you can deploy your
containerized applications on top of it. 

To do so, you create a Kubernetes Deployment configuration. The
Deployment instructs Kubernetes how to create and update instances of
your application. Once you've created a Deployment, the Kubernetes
master schedules mentioned application instances onto individual Nodes
in the cluster.

/Step 1: Create a Deployment/

As a first step it is necessary to create the necessary deployment by
providing a name for the deployment and pulling the docker image of
interest you want to run on top of it.

#+BEGIN_EXAMPLE
$ kubectl create deployment kubernetes-bootcamp --image=gcr.io/google-samples/kubernetes-bootcamp:v1
#+END_EXAMPLE

You can then double check the successful creating of the deployment by running:

#+BEGIN_EXAMPLE
$ kubectl get deployments
#+END_EXAMPLE

Notice the following outputs:

- The READY column shows the ratio of CURRENT to DESIRED replicas

- CURRENT is the number of replicas running now

- DESIRED is the configured number of replicas

- The UP-TO-DATE is the number of replicas that were updated to match the desired (configured) state

- The AVAILABLE state shows how many replicas are actually AVAILABLE to the users

Once the application is deployed it is possible to interact with it
through an kubernetes API endpoint.

This is necessary as pods that are running inside Kubernetes are
running on a private, /isolated network/. By default they are visible
from other pods and services within the same kubernetes cluster, but
not outside that network. The API overcomes such a barrier and allows
to communicate with the application system wide.

The kubectl command can create a proxy that will forward
communications into the cluster-wide, private network.  

We now have then /a connection between our host/ and the Kubernetes
cluster. The proxy enables /direct access/ to the API from the host
(the *terminal* in our case).  In order to do so it is possible to run

#+BEGIN_SRC 
$ kubectl proxy
#+END_SRC

Once the proxy is running it is possible to get the name of the host
that have access to the API by running a ~curl~ command as

#+BEGIN_SRC 
$ curl http://localhost:8001/version
#+END_SRC

Important is moreover to understand that the API server will then
automatically create an endpoint for each /Pod/ so that it is possible
to communicate directly through the specific /Pod/ through your proxy.

You can get a list of running Pods by running the following command

#+BEGIN_SRC 
$ kubectl get pods
#+END_SRC

To save the pod name you can run

#+BEGIN_SRC 
$ export POD_NAME=$(kubectl get pods -o go-template --template '{{range .items}}{{.metadata.name}}{{"\n"}}{{end}}')
#+END_SRC

Once the name of the pod is there you can interact with and access the
app deployed on it by leveraging your proxy.

You can then make a ~curl~ query of the form

#+BEGIN_SRC 
## where the $POD_NAME has to be previously saved as above.
curl http://localhost:8001/api/v1/namespaces/default/pods/$POD_NAME/proxy/
#+END_SRC

Recall that a a Pod models an application-specific "logical host" and
can contain different application containers which are relatively
tightly coupled. For example, a Pod might include both the container
with your Node.js app as well as a different container that feeds the
data to be published by the Node.js webserver. The containers in a Pod
share an IP Address and port space, are always co-located and
co-scheduled, and run in a shared context on the same Node.

Pods are the *atomic unit* on the Kubernetes platform. When we create a
Deployment on Kubernetes, that Deployment creates Pods with containers
inside them (as opposed to creating containers directly). Each Pod is
tied to the Node where it is scheduled, and remains there until
termination (according to restart policy) or deletion. In case of a
Node failure, identical Pods are scheduled on other available Nodes in
the cluster.

You can moreover get more specific information about each running pod by running

#+BEGIN_SRC 
$ kubectl describe pod
#+END_SRC

Notice moreover that it is possible to see standard output of an
application running within a pod by inspecting the logging record
within that Pod.  /This is possible as anything that the application would normally send to STDOUT becomes logs for the container within
the Pod. We can retrieve these logs using the kubectl logs command/

You can inspect such by running 

#+BEGIN_SRC 
$ kubectl logs
#+END_SRC

** Contexts

   As mentioned above context specify with which cluster and on which
   namespace you will work.

   I worked with multiple clusters throughout my experience. At some
   point I needed to make order. I had too many clusters and contexts
   I did not use anymore in my =config= file governing the kubectl
   specifications.

   In order to unset clusters and contexts use the following

   #+BEGIN_SRC sh
   ## to view the contexts
   kubectl config get-contexts

   ## to view the clusters 
   kubectl config get-contexts

   ## to remove contexts and clusters
   kubectl config unset contexts.<name>
   kubectl config unset clusters.<name>
   #+END_SRC

** Direct operating within a /Pod/

Notice that as a Pod is used as a running isolated environment within
a node it is possible to directly operate on it.

This is possible through the API given the name of a specific running Pod. 

For instance to run a bash session on the Pod it is possible to run:

#+BEGIN_SRC 
$ kubectl exec -ti $POD_NAME bash
#+END_SRC

This will open a proper bash session within the specified Pod.

We can execute commands directly on the container once the Pod is up
and running. For this, we use the exec command and use the name of the
Pod as a parameter. Let’s list the environment variables

** On Pod Lifecycle

Up to know we have addressed the issues of accessing Pod locally from
the kubernetes CLI.

When we want to expose an application to the /"outside world"/ we need
however to be careful. In order to see that think of the kubernetes
lifecycle.

Kubernetes Pods are mortal. Pods in fact have a lifecycle. When a
worker node dies, the Pods running on the Node are also lost. A
ReplicaSet might then dynamically drive the cluster back to desired
state via creation of new Pods to keep your application running.

This is in fact the strength of Kubernetes as an orchestration tool.

Consider now an image-processing backend with 3 replicas. Those
replicas are exchangeable; the front-end system should not care about
backend replicas or even if a Pod is lost and recreated. That said,
/each Pod in a Kubernetes cluster has a unique IP address/, even Pods on
the same Node, so there needs to be a way of automatically reconciling
changes among Pods so that your applications continue to function.

A Service in Kubernetes is an abstraction which defines a logical set
of Pods and a policy by which to access them. Services enable a loose
coupling between dependent Pods. A Service is defined using YAML
(preferred) or JSON, like all Kubernetes objects.

/Example of a YAML/:

Although each Pod has a unique IP address, those IPs are not exposed
outside the cluster without a Service. Services allow your
applications to receive traffic. Services can be exposed in different
ways by specifying a ~type~ in the ServiceSpec:


- ClusterIP (default) - Exposes the Service on an internal IP in the
  cluster. This type makes the Service only reachable from within the
  cluster.

- NodePort - Exposes the Service on the same port of each selected
  Node in the cluster using NAT. Makes a Service accessible from
  outside the cluster using =<NodeIP>:<NodePort>=. Superset of
  ClusterIP.

- LoadBalancer - Creates an external load balancer in the current
  cloud (if supported) and assigns a *fixed, external IP to the
  Service.* Superset of NodePort.

- ExternalName - Exposes the Service using an arbitrary name
  (specified by externalName in the spec) by returning a CNAME record
  with the name. No proxy is used. This type requires v1.7 or higher
  of =kube-dns=.

Moreover note that services are the abstraction that allow pods to die and
replicate in Kubernetes without impacting your application. Discovery
and routing among dependent Pods (such as the frontend and backend
components in an application) is handled by Kubernetes Services.

Services match a set of Pods using labels and selectors, a grouping
primitive that allows logical operation on objects in
Kubernetes. Labels are key/value pairs attached to objects and can be
used in any number of ways:

- Designate objects for development, test, and production
- Embed version tags
- Classify an object using tags

To list the services running on your cluster you can run

#+BEGIN_SRC 
$ kubectl get services
#+END_SRC

To create a new service and expose it to the external traffic we'll
use the expose command with NodePort as parameter. We choose this
option as the tutorial runs through minikube and this does not support
the /Loadbalancer/ option yet. Notice, however that in case this is
available it is recommended working through such an option.

#+BEGIN_SRC 
$ kubectl expose deployment/kubernetes-bootcamp --type="NodePort" --port 8080
#+END_SRC

You can verify that your Service was properly exposed by controlling
with the ~$ kubectl get services~ option above.

Once the service is deployed with the internal assigned port as
specified above an external port is assigned to the service. You can
get this by inspecting the service specific characteristics

#+BEGIN_SRC 
## kubectl describe services/<service name>
$ kubectl describe services/kubernetes-bootcamp 
#+END_SRC

And you can save it in your environment through 

#+BEGIN_SRC 
$ export NODE_PORT=$(kubectl get services/kubernetes-bootcamp -o go-template='{{(index .spec.ports 0).nodePort}}')
#+END_SRC

You can then connect to the kubernetes app by any device by running

#+BEGIN_SRC 
$ curl $(minikube ip):$NODE_PORT
#+END_SRC

Notice, moreover that the deployment created automatically a /label/
for our Pod. With the ~$ describe deployment~ command you can simply
get to it.

Given your label (/in the tutorial run=kubernetes-bootcamp/), you can then access your Pod directly thorough it.

For instance the command

#+BEGIN_SRC 
## kubectl get services -l <label>
$ kubectl get services -l run=kubernetes-bootcamp
#+END_SRC

will get you the services running on pods with the given label.

You can add a label to a Pod by running

#+BEGIN_SRC 
$ kubectl label pod $POD_NAME app=v1
#+END_SRC

This will be then added to the label of the pod and will allow you to
operate on the pod accordingly.

You can finally delete an existing service through:

#+BEGIN_SRC 
## kubectl delete service -l <label>
$ kubectl delete service -l run=kubernetes-bootcamp
#+END_SRC

You will see then that after deleting the service your app deployed on the specific pod will not be externally accessible anymore, i.e.

#+BEGIN_SRC 
$ curl $(minikube ip):$NODE_PORT
#+END_SRC

will fail. 

Nonetheless the app can always be reached internally given the Pod
name and the selected port deployment.

** App scaling

In the previous modules we created a Deployment, and then exposed it
publicly via a Service. The Deployment created only one Pod for
running our application. When traffic increases, we will need to scale
the application to keep up with user demand.

*Scaling is accomplished by changing the number of replicas in a
Deployment*

Scaling out a Deployment will ensure new Pods are created and
scheduled to Nodes with available resources. Scaling will increase the
number of Pods to the new desired state. Kubernetes also supports
autoscaling of Pods, but it is outside of the scope of this
tutorial. Scaling to zero is also possible, and it will terminate all
Pods of the specified Deployment.

Running multiple instances of an application will require a way to
distribute the traffic to all of them. Services have an integrated
load-balancer that will distribute network traffic to all Pods of an
exposed Deployment. Services will monitor continuously the running
Pods using endpoints, to ensure the traffic is sent only to available
Pods.

To scale an deployment it is then possible to scale it specifying the
desired number of replicas by running

#+BEGIN_SRC 
$ kubectl scale deployments/<deployment name> --replicas=4
#+END_SRC

You can then verify that the number of pods have been updated by
running the ~$ kubectl get pods~ command. 

Notice that different pods have different internal IP addresses. You
can read them through the ~$ kubectl describe services
services/<service name>~ command.

** New releases

Users expect applications to be available all the time and developers
are expected to deploy new versions of them several times a day. In
Kubernetes this is done with rolling updates. Rolling updates allow
Deployments' update to take place *with zero downtime* by incrementally
updating Pods instances with new ones. The new Pods will be scheduled
on Nodes with available resources.

By default, the maximum number of Pods that can be unavailable during
the update and the maximum number of new Pods that can be created, is
one. Both options can be configured to either numbers or percentages
(of Pods). In Kubernetes, updates are versioned and any Deployment
update can be reverted to previous (stable) version.

Similar to application Scaling, if a Deployment is exposed publicly,
the Service will load-balance the traffic only to available Pods
during the update. An available Pod is an instance that is available
to the users of the application.

** Cluster Exposure to Public Internet

Since there are three replicas of this application deployed in the
cluster, Kubernetes will load balance requests across these three
instances. 

Let's expose our application to the internet and see how Kubernetes
load balances requests.

In order to access the application, we have to expose it to the
internet using a Kubernetes Service.  

#+BEGIN_SRC sh
kubectl expose deployment/hello-world --type=NodePort --port=8080 --name=hello-world --target-port=8080
#+END_SRC

This command creates what is called a *NodePort Service*. This will
open up a port on the worker node to allow the application to be
accessed the application using that port and the IP address of the node.

List Services in order to see that this service was created.

#+BEGIN_SRC sh
kubectl get services
#+END_SRC

Two things are needed to access this application: a worker node IP
address and the correct port. 

To get a worker node IP, rerun the get pods command with the wide
option and note any one of the node IP addresses (from the column
entitled NODE): 

#+BEGIN_SRC sh
kubectl get pods -o wide 
#+END_SRC

Here is some sample output.

#+begin_example
NAME                          READY   STATUS    RESTARTS   AGE   IP               NODE            NOMINATED NODE   READINESS GATES
hello-world-dd6b5d745-f9xjk   1/1     Running   0          7m    172.30.104.185   10.114.85.153   <none>           <none>
hello-world-dd6b5d745-m89fc   1/1     Running   0          7m    172.30.165.182   10.114.85.151   <none>           <none>
hello-world-dd6b5d745-qvs9t   1/1     Running   0          7m    172.30.69.68     10.114.85.172   <none>           <none>
Using this sample output, you could choose 10.114.85.153, 10.114.85.151, or 10.114.85.172 for the node IP address.
#+end_example

Export the node IP address as an environment variable. 
Make sure to substitute the copied IP address into this command.

#+BEGIN_SRC sh
# export NODE_IP=<node_ip>
# Using the sample output, one correct command would be 
export NODE_IP=10.114.85.153.
#+END_SRC

To get the port number, run the following command and note the port:

#+BEGIN_SRC sh
kubectl get services
#+END_SRC

Here is some sample output. In this sample, the port number we need is 31758.

#+begin_example
NAME          TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
hello-world   NodePort   172.21.121.84   <none>        8080:31758/TCP   58s
#+end_example

Export the port as an environment variable. Make sure to substitute
the copied port into this command.

#+BEGIN_SRC sh
## export NODE_PORT=<node_port>
## Using the sample output, the correct command would be export NODE_PORT=31758
export NODE_PORT=31758
#+END_SRC


Ping the application to get a response.

#+BEGIN_SRC sh
curl $NODE_IP:$NODE_PORT
#+END_SRC

Notice that this output includes the Pod name. Run the command ten
times and note the *different Pod names* in each line of output.  

#+BEGIN_SRC sh
for i in `seq 10`; do curl $NODE_IP:$NODE_PORT; done
#+END_SRC

You should see more than one Pod name, and quite possibly all three
Pod names, in the output. This is because Kubernetes load balances the
requests across the three replicas, so each request could hit a
different instance of our application.

** App Scaling, new releases, cluster configuration - Implementation

*** Replicasets

It manages your pods ensuring the right number of pods are always up
and running. This can mean adding or deleting pods as
needed. ReplicaSets provide the ability to replicate pods and restart
or spin up new pods when existing ones fail.

A ReplicaSet can pick an existing pod to add to the deployment or
create a new one if there are no existing pods. It does so by asking
for a list of pods from the Kubernetes API and then filtering on the
labels, as defined in the descriptor.

The ReplicaSet does not own any of the pods; instead it uses the pod
labels to decide which pods to acquire when bringing a deployment to
the desired state. The template metadata inside the YAML spec defines
the labels of potential pod candidates to add or delete.

An example of a ReplicaSet YAML would look as follows.

#+BEGIN_SRC yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: frontend
  labels: ## here you specify the labels on which to match your
          ## Replica and apply the defined replica conditions
    app: guestbook
    tier: frontend
spec:     ## here you specify the number of replicas you desire
  replicas: 3
  selector:
    matchLabels:
      tier: frontend
  template:   ## here your pass your image and label it accordingls.
    metadata:
      labels:
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google_samples/gb-frontend:v3
#+END_SRC

Notice that Replicasets are automatically created when creating
*Deployments*. You can get a list of running Replicasets in your
cluster for the specific namespace you are working in by using:

#+BEGIN_SRC sh
kubectl get replicaset
kubectl get rs ## also accepted. alias.
#+END_SRC

Once a replicaset is running you can change the number of replicas in
an imperative way running

#+BEGIN_SRC sh
kubectl scale deploy <deployment-name> --replicas=4
#+END_SRC

*** Autoscaling

Horizontal Pod Autoscaler, or HPA, enables the application to increase
the number of pods based on traffic.

You can configure the desired state in HPA (for example, the CPU and
memory).  

The master node will periodically check pod metrics and
scale to meet the desired state by updating the replicas field of the
scaled resource, such as ReplicaSets or deployment.

For instance consider the following deployment

#+begin_src yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: php-apache
spec:
  selector:
    matchLabels:
      run: php-apache
  replicas: 1
  template:
    metadata:
      labels:
        run: php-apache
    spec:
      containers:
      - name: php-apache
        image: k8s.gcr.io/hpa-example
        ports:
        - containerPort: 80
        resources:
          limits:
            cpu: 500m
          requests:
            cpu: 200m
#+end_src

You can then create a Horizonatal Pod Autoscaler imperatively using

#+BEGIN_SRC sh
kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10  
#+END_SRC

The following command will create a Horizontal Pod Autoscaler that
maintains between 1 and 10 replicas of the Pods controlled by the
php-apache deployment we created in the first step of these
instructions. Roughly speaking, HPA will increase and decrease the
number of replicas (via the deployment) to maintain an average CPU
utilization across all Pods of 50%

The other way, working in a declarative way, would look as follows

#+begin_src yaml
apiVersion: autoscaling/v2beta2
kind: HorizontalPodAutoscaler
metadata:
  name: php-apache
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: php-apache
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50
#+end_src



*** Rolling Updates

Rolling updates provide a way to roll out app changes in an automated
and controlled fashion throughout your pods.  Rolling updates work
with pod templates such as deployments.  Rolling updates allow for
rollback if something goes wrong.

An example for a Rolling update would be as follows

#+BEGIN_SRC yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: linuxsys-deploy
  name: linuxsys-deploy
spec:
  replicas: 10
  selector:
    matchLabels:
      app: linuxsys-deploy
  minReadySeconds: 5 ## wait 5 seconds before moving on to the next
                     ## pod in the rollout stage.
  strategy: 
       type: RollingUpdate
       rollingUpdate:
         maxSurge: 25% ## there can just be at maximum 1.25 *
                       ## <specified number replicas> replicas while
                       ## performing the rollout.
         maxUnavailable: 25% ## 75% of the pods must always be available.
  template:
    metadata:
      labels:
        app: linuxsys-deploy
    spec:
      containers:
      - image: nginx:1.17-perl
        name: nginx
        ports:
        - containerPort: 80
#+END_SRC


*** Config Maps and Secrets

As software developers, we know it’s a good practice not to hard-code
configuration variables in code.

We keep them separate so that any changes in configuration do not
require code changes.  

Examples of these variables can include non-sensitive information like
environments (for example, dev, test, and prod) or sensitive
information such as API keys and account IDs.

ConfigMaps give us a way to provide configuration data to pods and
deployments so we don't have to hard-code that data in the application
code.

You can also reuse these ConfigMaps and Secrets for multiple
deployments, thereby decoupling the environment from the deployments
themselves!

Secrets work similarly to ConfigMaps but are meant for sensitive
information.

An example to do that is the following

#+begin_src yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hello-world
spec:
  selector:
    matchLabels:
      run: hello-world
  template:
    metadata:
      labels:
        run: hello-world
    spec:
      containers:
      - name: hello-world
        image: us.icr.io/sn-labs-marcohassan/hello-world:3
        ports:
        - containerPort: 8080
        envFrom:
        - configMapRef:
            name: app-config ## here you specify that environment
                             ## variables should be specified in a
                             ## ConfigMap named app-config
      imagePullSecrets:
        - name: icr
#+end_src

Then you can have an app with the following code

#+begin_src javascript
var express = require('express')
var os = require("os");
var hostname = os.hostname();
var app = express()

app.get('/', function(req, res) {
    res.send(process.env.MESSAGE + '\n')    
})
app.listen(8080, function() {
  console.log('Sample app is listening on port 8080.')
})
#+end_src

Notice now that the MESSAGE variable should come from the app-config
Configmap specified above.

You can then specify it with the following command

#+BEGIN_SRC sh
kubectl create configmap app-config --from-literal=MESSAGE="This message came from a ConfigMap!"
#+END_SRC

Now after applying the deployment

#+BEGIN_SRC sh
kubectl apply -f deployment-configmap-env-var.yaml
#+END_SRC

And deploying the new image with the new app code

#+BEGIN_SRC sh
docker build -t us.icr.io/$MY_NAMESPACE/hello-world:3 . && docker push us.icr.io/$MY_NAMESPACE/hello-world:3
#+END_SRC

You would get the configmap text when accessing your application
running on the cluster.

** Literature

IBM - Journey to Cloud Series.

[[https://kubernetes.io/docs/tutorials/kubernetes-basics/create-cluster/cluster-interactive/][Kubernetes tutorial]]





