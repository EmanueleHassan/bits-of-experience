#+BEGIN_COMMENT
.. title: Packaging Python Apps
.. slug: packaging-python-apps
.. date: 2022-05-13 09:49:38 UTC+02:00
.. tags: Python
.. category: 
.. link: 
.. description: 
.. type: text

#+END_COMMENT

#+begin_export html
<style>

img {
display: block;
margin-top: 60px;
margin-bottom: 60px;
margin-left: auto;
margin-right: auto;
width: 70%;
height: 100%;
class: center;
}

.container {
position: relative;
left: 15%;
margin-top: 60px;
margin-bottom: 60px;
width: 70%;
overflow: hidden;
padding-top: 56.25%; /* 16:9 Aspect Ratio */
display:block;
overflow-y: hidden;
}

.responsive-iframe {
position: absolute;
top: 0;
left: 0;
bottom: 0;
right: 0;
width: 100%;
height: 100%;
border: none;
display:block;
overflow-y: hidden;
}
</style>
#+end_export


So this post holds some information around the packaging of python
applications.

This will be handy to enter in the serious development world of the
language.

This touches a lot as well the point of going in direction DevOps and
consequently it will be foundamental to go in direction MLOps.

{{{TEASER_END}}}

So the first piece that will be important to understand in this space
is the one of the central repositories where you publish and fetch
your packages from.

It is always the same story. In docker you have the docker registry,
in emacs you have MELPA, in java you have MavenCentral, in python you
have =PyPi=.


** PyPi

   (Python Package Index)- is the official repository for third-party
   Python software packages. Every time you use e.g. pip to install a
   package that is not in the standard it will get downloaded from the
   PyPI server.

   All of the packages that are on PyPI are publicly visible. So if
   you upload your own package then /anybody can start using it/.


** DevPi

   Check into [[https://doc.devpi.net/][this]] for more. 

   This is a self hosted private Python Package server. Additionally
   you can use it for testing and releasing of your own packages.

   So everything you post over here is private and will not be visible
   across the internet.

   There are then lots of these repositories in the cloud. For
   instance in the azure devops center there are private indexes where
   you can publish your Python packages.
   
   I guess, that such private repositories are nothing else than DevPi
   instances provided by the cloud providers that you can use for
   storing your artifacts and fetching them.

   In my workflow I will use [[https://test.pypi.org/account/register/][TestPyPI]], which is the official testing
   website for PyPI.

   
** Root Modules for packaging your Python projects

   Here the understanding is not 100% complete yet. You are getting
   closer to it. Would need more time. As long as you do not
   understand the root behind it you will fail to package everything
   correctly.

   You can read about the lastest of packaging in Python over [[https://packaging.python.org/en/latest/flow/][here]].

   So you essentially see that the three major compoenents to this
   stage are:

   1. =distutils= - provides support for building and installing
      additional modules into a Python installation. The new modules
      may be either 100%-pure Python, or may be extension modules
      written in C, or may be collections of Python packages which
      include modules coded in both Python and C.

   2. =setuptools= - Setuptools is a package development process
      library designed to facilitate packaging Python projects by
      enhancing the Python standard library distutils (distribution
      utilities).

   3. =twine= - for securely publishing the relevant package.

   These are the root for publishing in python. All of the different
   packaging systems use these then. Read [[https://web.archive.org/web/20200227202149/https://zetcode.com/articles/packageinpython/][here]] in this sense, will
   help you quite much.

   [[https://hynek.me/articles/sharing-your-labor-of-love-pypi-quick-and-dirty/][This]] is a second good source. Here you will find as well a good
   example for the =setup.py= file. I rewrote it partially keeping a
   bit of the logic. Check at the =realoperations= and the
   =setup_config.py= in there. 

   In this sense, understand that what tox uses for uploading and
   publishing your modules is in fact the distutils and its most
   common command.

   #+begin_src sh

   # requires the build package.
   python -m build --sdist <source-tree-directory> # note sdist = source
						   # distribution while
						   # wheel is built
						   # distribution
						   # i.e. already in
						   # bytes. You do not
						   # need a gcc engine or
						   # similar to compile
						   # the C code etc.

   python -m build --wheel <source-tree-directory> # creates a wheel that
 					           # is ready to run in
						   # binary
   #+end_src

   So you have to understand the difference with twine when you use
   it. Is it simply the difference in the secure upload over HTTPS?

   Yes, that is correct, in fact you see that even when using twine
   you have to build the project first into an artifact locally.

   Then you use twine to upload it to the server of choice, be it a
   =PyPI= server a =DevPI= server or whatever.

   Note that through such build systems, you will actually have the
   following directories:

   - =dist=: will hold the packaged package. Say wheel that you can
     then upload to your server of choice. 

   - =build=: have to understand what is in here. From the first build
     it is not that clear.
   
   
** Twine

   Note that this is in general the recommended way for publishing
   python artifacts to either PyPi or DevPi. This due to the secure
   communication over HTTPS.

   In general you can understand twine as a CLI for:

   1. A user-facing tool for publishing on pypi.org

   2. A user-facing tool for publishing on other Python package
      indexes (e.g., devpi instances)

   3. A useful API for other programs (e.g., zest.releaser) to call
      for publishing on any Python package index

   Note that in this sense you have a =.pypirc= file specifying the
   servers where you would like to publish your artifact. You will as
   well have the credentials necessary for publishing under this
   servers.

   This is actually what you are doing in the Azure DevOps
   pipelines. You can see there that you are given the mentioned
   credentials.

   When you upload your artifacts to Azure make sure that you have the
   following dependency:

   #+begin_src sh
pip install twine keyring artifacts-keyring
   #+end_src


** Tox

   So you can embedd in your tox pipeline the project packaging as
   well.

   So you can both run all of the necessary tests and package/build
   the project.

   You can then publish via twine just upon the successful build.

   Have still to develop this exact pipeline but this is more less the
   idea and general concept.

   
*** TODO Create your CI-CD pipeline
   

** TODO Poetry

   I will follow [[https://py-pkgs.org/03-how-to-package-a-python][this tutorial]]. It has everything you need.

   

** TODO spline

   This is the bonus section. Also go deeper into [[https://spline.readthedocs.io/en/latest/example.html][this]].

   Just if you have time. 

   I think this is an interesting project.

   I would need more time to properly check into this.

   Unfortunaltely in my current position I have no time to really go
   out there and explore and push the boundaries.

   It is already a miracle keeping the boat afloat and modernizing it
   to the /current sate/ of the technologies. 
 
