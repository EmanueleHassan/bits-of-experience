#+BEGIN_COMMENT
.. title: Defining Functions in Spark
.. slug: defining-functions-in-spark
.. date: 2021-06-03 14:34:46 UTC+02:00
.. tags: Spark
.. category: 
.. link: 
.. description: 
.. type: text
.. status: published
#+END_COMMENT

One especially interesting feature that I already used before; but
unfortunately I did not make notes for, is the possibility of
specifying particular functions via the PySpark raw API and make them
available throughout your spark session when working via data-frames -
be it via SparkSQL or via pandas.

Note that this notes are based on [[https://jaceklaskowski.gitbooks.io/mastering-spark-sql/content/spark-sql-udfs.html][this source]]. Looks like a very
exhaustive source. So check at it again should this project really
start. There you might make sense of all of that. 

{{{TEASER_END}}}

** User Defined Functions
:PROPERTIES:
:header-args:ein-python: :session http://127.0.0.1:8888/Spark.ipynb
:header-args:sh: :session baubab
:END:

   So the usual way to work to specify the functions you would need to
   work with is via *user-defined-functions*.

   This is a very neat feature when working with Spark as it will
   allow you to define functions you can then apply to data-frames.

   So if you are working with data in tabular format it might well
   makes sense to work in such a way leveraging such customizable
   functions.

   The idea of udf when working with Python is now the following:

   #+begin_quote
   User-Defined Functions (aka UDF) is a feature of Spark SQL to
   define new Column-based functions that extend the vocabulary of
   Spark SQL’s DSL for transforming Datasets.
   
   Historically, user-defined functions operate one-row-at-a-time, and thus
   suffer from high serialization and invocation overhead. As a
   result, many data pipelines define UDFs in Java and Scala and then
   invoke them from Python.

   Now, with the rise of [[https://arrow.apache.org/][Apache Arrow]], it is now possible to define
   low-overhead, high-performant user-defined functions directly in
   Python.
   #+end_quote

   Specifically so far it was just possible in python to use the
   SparkSQL user-defined functions and perform row by row operations.

   You can think for instance at the following:

   #+begin_src python
##############
## Syntax 1 ##
##############

from pyspark.sql.functions import udf

# Use udf to define a row-at-a-time udf
@udf('double')
# Input/output are both a single double value
def squared(v):
      return v * v

# work with it either leveraging dataframes methods or sql

df.withColumn('v2', plus_one(df.v)) # note that df is a spark data frame

# or call via sql  - should be a separate chuck with the magic %sql call
%sql 
select v2, squaredWithPython(v2) as id_squared from df

##############
## Syntax 2 ##
##############

def squared(s):
  return s * s

spark.udf.register("squaredWithPython", squared)

# assume a test spark dataframe

# call it via sql - should be a separate chuck with the magic %sql call
%sql
select id, squaredWithPython(id) as id_squared from test

# or call it via dataframes methods 
test.withColumn('id', squaredWithPython(test.id))
   #+end_src

   Vis à vis the following new vectorized udf or stated differently
   pandas-udf:
   
   #+begin_src python
import pandas as pd

from pyspark.sql.functions import col, pandas_udf
from pyspark.sql.types import LongType

# Declare the function and create the UDF
def multiply_func(a, b):
    return a * b

multiply = pandas_udf(multiply_func, returnType=LongType())

### Applied to Pandas data objectes ###

## The function for a pandas_udf should be able to execute with local Pandas data
x = pd.Series([1, 2, 3])
print(multiply_func(x, x))
# 0    1
# 1    4
# 2    9
# dtype: int64

### Applied Spark Data Frames  ###

# Create a Spark DataFrame, 'spark' is an existing SparkSession
df = spark.createDataFrame(pd.DataFrame(x, columns=["x"]))

# Execute function as a Spark vectorized UDF
df.select(multiply(col("x"), col("x"))).show()
# +-------------------+
# |multiply_func(x, x)|
# +-------------------+
# |                  1|
# |                  4|
# |                  9|
# +-------------------+
   #+end_src

   Then you can do numerical tests yourself and see that the
   vectorized operations rather than the row by row operations has a
   computational edge. 

** Note and understand the difference between vectorized udf and toPandas

   Note that the two are quite different.

   Although it might come naturally to work with pandas and so you
   might be tempted to call the convert =toPandas ()= function and
   work with Pandas methods etc. this is not recommended as the entire
   benefit of working with distributed data would be gone.

   In this sense, when calling the spark method of udf what you would
   actually do is specifying working a function and work on vectorized
   columns stores leveraging data structures that resolves such
   operations in a computationally intense and fast way.

   
** Very Important Note

   Notice that despite the fact that /Apache Arrow/ and the vectorized
   pandas operations are able to bring quite some computational edge
   over the standard available udf functions that were so far
   available it is still recommended to work with SQL when working
   with dataframes.

   In this sense, you should not fall in the temptation of writing to
   many udf functions yourself even if you like to work in a pythonic
   way, as project Tungsten and the Catalyst optimizer leveraging the
   decades of research on relational structures and SQL will still be
   there so that you should go with it and be thoughtful in the extent
   you should apply such weapon. 


